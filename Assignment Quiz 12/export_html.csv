Heading 1,Heading 2,Heading 3,Paragraph
Artificial intelligence,Contents,"Reasoning, problem-solving","Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.[a]"
Blockchain,History,Knowledge representation,"The term ""artificial intelligence"" had previously been used to describe machines that mimic and display ""human"" cognitive skills that are associated with the human mind, such as ""learning"" and ""problem-solving"". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.[b]"
Internet of things,Goals,Planning,"AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).[2][citation needed]As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect.[3]  For instance, optical character recognition is frequently excluded from things considered to be AI,[4] having become a routine technology.[5]"
Cloud computing,Tools,Learning,"Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[6][7] followed by disappointment and the loss of funding (known as an ""AI winter""),[8][9] followed by new approaches, success and renewed funding.[7][10] AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.[11][10]"
Amazon,Applications,Natural language processing,"The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects.[c] General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.[12] To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques—including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields."
Laptop,Philosophy,Perception,"The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".[d]This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[14] Science fiction writers and futurologists have since suggested that AI may become an existential risk to humanity if its rational capacities are not overseen.[15][16]"
Mobile,Future,Motion and manipulation,"Artificial beings with intelligence appeared as storytelling devices in antiquity,[17]and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R.[18] These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.[19]"
Computer,In fiction,Social intelligence,"The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis.[20]"
Microsoft Windows,See also,General intelligence,"The Church-Turing thesis, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain.[21]The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".[22]"
Steve Jobs,Explanatory notes,Search and optimization,"By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the ""heuristic search"" approach, which likened intelligence to a problem of exploring a space of possibilities for answers. The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[23] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to it's connection to intellectual traditions of Descarte, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[24]"
,Citations,Logic,"The field of AI research was born at a workshop at Dartmouth College in 1956.[e][27]The attendees became the founders and leaders of AI research.[f]They and their students produced programs that the press described as ""astonishing"":[g]computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[h][29]By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[30]and laboratories had been established around the world.[31]"
,References,Probabilistic methods for uncertain reasoning,"Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.[32]Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"".[33]Marvin Minsky agreed, writing, ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"".[34]"
,Further reading,Classifiers and statistical learning methods,"They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[35]and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.[8]"
,External links,Artificial neural networks,"In the early 1980s, AI research was revived by the commercial success of expert systems,[36]a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research.[7]However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[9]"
,Sources,Specialized languages and hardware,"Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems.[37] Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.[i]Interest in neural networks and ""connectionism"" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.[42]Soft computing tools were developed in the 80s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization."
,Navigation menu,Defining artificial intelligence,"AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more  mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).[43]By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".[11]"
,Contents,Evaluating approaches to AI,"Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.[44]According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects.[j] He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.[10] In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"".[45] The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.[46]"
,History,"Machine consciousness, sentience and mind","Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.[12]"
,Structure,Superintelligence,The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.[c]
,Types,Risks,"Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[47]By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.[48]"
,Uses,Ethical machines,"Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a ""combinatorial explosion"": they became exponentially slower as the problems grew larger.[49]Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[50]"
,Blockchain interoperability,Regulation,Knowledge representation and knowledge engineering[51]allow AI programs to answer questions intelligently and make deductions about real-world facts.
,Energy consumption concerns,AI textbooks,"A representation of ""what exists"" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.[52]The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.[53]"
,Academic research,History of AI,"AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects;[53]situations, events, states and time;[54]causes and effects;[55]knowledge about knowledge (what we know about what other people know);.[56]default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[57]as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);[58]and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally).[50]"
,See also,Other sources,"Formal knowledge representations are used in content-based indexing and retrieval,[59]scene interpretation,[60]clinical decision support,[61]knowledge discovery (mining ""interesting"" and actionable inferences from large databases),[62]and other areas.[63]"
,References,Search,"An intelligent agent that can plan makes a representation of the state of the world, makes predictions about how their actions will change it and make choices that maximize the utility (or ""value"") of the available choices.[64]In classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.[65]However, if the agent is not the only actor, then it requires that the agent reason under uncertainty, and continuously re-assess its environment and adapt.[66]Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.[67]"
,Further reading,Blocks,"Machine learning (ML), a fundamental concept of AI research since the field's inception,[k]is the study of computer algorithms that improve automatically through experience.[l]"
,External links,Decentralization,"Unsupervised learning finds patterns in a stream of input. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in—the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as ""function approximators"" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, ""spam"" or ""not spam"".[71]In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.[72]Transfer learning is when the knowledge gained from one problem is applied to a new problem.[73]"
,Navigation menu,Openness,"Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[74]"
,Contents,Standardisation,"Natural language processing (NLP)[75]allows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.[76]"
,History[edit],Public blockchains,"Symbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic[49] and the breadth of commonsense knowledge.[58] Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), ""Keyword spotting"" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others.[77] They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.[78]"
,Applications[edit],Private blockchains,"Machine perception[79]is the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,[80]facial recognition, and object recognition.[81]"
,Trends and characteristics[edit],Hybrid blockchains,Computer vision is the ability to analyze visual input.[82]
,Enabling technologies for IoT[edit],Sidechains,"AI is heavily used in robotics.[83]Localization is how a robot knows its location and maps its environment. When given a small, static, and visible environment, this is easy; however, dynamic environments, such as (in endoscopy) the interior of a patient's breathing body, pose a greater challenge.[84]"
,Politics and civic engagement[edit],Cryptocurrencies,"Motion planning is the process of breaking down a movement task into ""primitives"" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. Robots can learn from experience how to move efficiently despite the presence of friction and gear slippage.[85]"
,Government regulation on IoT[edit],Smart contracts,"Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.[87] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.However, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.[88]"
,"Criticism, problems and controversies[edit]",Financial services,"Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.[89]"
,IoT adoption barriers[edit],Games,"A machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.[90]Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, ""master algorithm"" that could lead to AGI.[91]Others believe that anthropomorphic features like an artificial brain[92]or simulated child development[m]will someday reach a critical point where general intelligence emerges."
,See also[edit],Supply chain,"Many problems in AI can be solved theoretically by intelligently searching through many possible solutions:[93]Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.[94]Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[95]Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.[96]"
,References[edit],Domain names,"Simple exhaustive searches[97]are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use ""heuristics"" or ""rules of thumb"" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called ""pruning the search tree""). Heuristics supply the program with a ""best guess"" for the path on which the solution lies.[98]Heuristics limit the search for solutions into a smaller sample size.[99]"
,Bibliography[edit],Other uses,"A very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing.[100]Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming.[101]Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[102]"
,Navigation menu,Adoption decision,"Logic[103]is used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning[104]and inductive logic programming is a method for learning.[105]"
,Contents,Collaboration,"Several different forms of logic are used in AI research. Propositional logic[106] involves truth functions such as ""or"" and ""not"". First-order logic[107]adds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a ""degree of truth"" (between 0 and 1) to vague statements such as ""Alice is old"" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.[108]Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.[57]Several extensions of logic have been designed to handle specific domains of knowledge, such as description logics;[53]situation calculus, event calculus and fluent calculus (for representing events and time);[54]causal calculus;[55]belief calculus (belief revision); and modal logics.[56]Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.[citation needed]"
,Value proposition[edit],Blockchain and internal audit,"Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[109]Bayesian networks[110]are a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),[n][112]learning (using the expectation-maximization algorithm),[o][114]planning (using decision networks)[115] and perception (using dynamic Bayesian networks).[116]Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[116]"
,History[edit],Journals,"A key concept from the science of economics is ""utility"", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[117]and information value theory.[118] These tools include models such as Markov decision processes,[119] dynamic decision networks,[116] game theory and mechanism design.[120]"
,Similar concepts[edit],Search,"The simplest AI applications can be divided into two types: classifiers (""if shiny then diamond"") and controllers (""if diamond then pick up""). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[121]"
,Characteristics[edit],Consumer applications[edit],"A classifier can be trained in various ways; there are many statistical and machine learning approaches.The decision tree is the simplest and most widely used symbolic machine learning algorithm.[122]K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.[123]Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[124]The naive Bayes classifier is reportedly the ""most widely used learner""[125] at Google, due in part to its scalability.[126]Neural networks are also used for classification.[127]"
,Service models[edit],Organizational applications[edit],"Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as ""naive Bayes"" on most practical data sets.[128]"
,Deployment models[edit],Industrial applications[edit],"Neural networks[127]were inspired by the architecture of neurons in the human brain. A simple ""neuron"" N accepts input from other neurons, each of which, when activated (or ""fired""), casts a weighted ""vote"" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed ""fire together, wire together"") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes."
,Architecture[edit],Infrastructure applications[edit],"Modern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization — they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.[129]Other learning techniques for neural networks are Hebbian learning (""fire together, wire together""), GMDH or competitive learning.[130]"
,Security and privacy[edit],Military applications[edit],"The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.[131]"
,Limitations and disadvantages[edit],Product digitalization[edit],"Deep learning[133]uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.[134] Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification[135] and others."
,Emerging trends[edit],Intelligence[edit],"Deep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons,[136] and creates a hierarchy similar to the organization of the animal visual cortex.[137]"
,Digital forensics in the cloud[edit],Architecture[edit],"In a recurrent neural network the signal will propagate through a layer more than once;[138] thus, an RNN is an example of deep learning.[139]RNNs can be trained by gradient descent,[140]however long-term gradients which are back-propagated can ""vanish"" (that is, they can tend to zero) or ""explode"" (that is, they can tend to infinity), known as the vanishing gradient problem.[141]The long short term memory (LSTM) technique can prevent this in most cases.[142]"
,See also[edit],Complexity[edit],"Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing."
,References[edit],Size considerations[edit],"AI is relevant to any intellectual task.[143]Modern artificial intelligence techniques are pervasive and are too numerous to list here.[144]Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.[145]"
,Further reading[edit],Space considerations[edit],"In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),targeting online advertisements,[146][non-primary source needed]recommendation systems (offered by Netflix, YouTube or Amazon),driving internet traffic,[147][148]targeted advertising (AdSense, Facebook),virtual assistants (such as Siri or Alexa),[149]autonomous vehicles (including drones and self-driving cars),automatic language translation (Microsoft Translator, Google Translate),facial recognition (Apple's Face ID or Microsoft's DeepFace),image labeling (used by Facebook, Apple's iPhoto and TikTok)and spam filtering."
,External links[edit],"A solution to ""basket of remotes""[edit]","There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage,[150]deepfakes,[151]medical diagnosis, military logistics, or supply chain management."
,Navigation menu,Social Internet of things[edit],"Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[152] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[153] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.[154]Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus[p]and Cepheus.[156]DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.[157]"
,Contents,Addressability[edit],"By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.[158]DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[159]Other applications predict the result of judicial decisions,[160] create art (such as poetry or painting) and prove mathematical theorems."
,Places[edit],Application Layer[edit],"In 2019, WIPO reported that AI was the most prolific emerging technology in terms of number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G).[161] Since AI emerged in the 1950s, 340000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four.[162] The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134777 machine learning patents filed for a total of 167038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.[162]"
,People[edit],Short-range wireless[edit],"Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?""[163]He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"".[164] The only thing visible is the behavior of the machine, so it does not matter if the machine is conscious, or has a mind, or whether the intelligence is merely a ""simulation"" and not ""the real thing"".  He noted that we also don't know these things about other people, but that we extend a ""polite convention"" that they are actually ""thinking"". This idea forms the basis of the Turing test.[165][q]"
,Art and entertainment[edit],Medium-range wireless[edit],"AI founder John McCarthy said: ""Artificial intelligence is not, by definition, simulation of human intelligence"".[167] Russell and Norvig agree and criticize the Turing test. They wrote: ""Aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.'""[166] Other researchers and analysts disagree and have argued that AI should simulate natural intelligence by studying psychology or neurobiology.[r]"
,Military units[edit],Long-range wireless[edit],"The intelligent agent paradigm[169]defines intelligent behavior in general, without reference to human beings. An intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. Any system that has goal-directed behavior can be analyzed as an intelligent agent: something as simple as a thermostat, as complex as a human being, as well as large systems such as firms, biomes or nations. The intelligent agent paradigm became widely accepted during the 1990s, and currently serves as the definition of the field.[a]"
,Organizations[edit],Wired[edit],"The paradigm has other advantages for AI. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given ""goal function"".  It also gives them a common language to communicate with other fields — such as mathematical optimization (which is defined in terms of ""goals"") or economics (which uses the same definition of a ""rational agent"").[170]"
,Transportation[edit],Standards and standards organizations[edit],"No established unifying theory or paradigm has guided AI research for most of its history.[s] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers."
,Other uses[edit],Platform fragmentation[edit],"Symbolic AI (or ""GOFAI"")[172] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""[173]"
,See also[edit],"Privacy, autonomy, and control[edit]","However, the symbolic approach failed dismally on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult.[174]Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge.[175]Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.[t][50]"
,Navigation menu,Data storage[edit],"The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as  Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[177][178] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision."
,Contents,Security[edit],"""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. This issue was actively discussed in the 70s and 80s,[179]but in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed ""the victory of the neats"".[180]"
,History[edit],Safety[edit],"Finding a provably correct or optimal solution is intractable for many important problems.[49] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks."
,Etymology[edit],Design[edit],"AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals[181][182]General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focussing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively."
,Types[edit],Environmental sustainability impact[edit],"The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers ""don't care about the [philosophy of AI] — as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence.""[183] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction."
,Hardware[edit],Intentional obsolescence of devices[edit],"David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness.[184] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[185]"
,Comparison with desktops[edit],Confusing terminology[edit],Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[186]
,Sales[edit],Lack of interoperability and unclear value propositions[edit],"Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""[u]Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.[189]"
,Disposal[edit],Privacy and security concerns[edit],"If a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.[190]Any hypothetical robot rights would lie on a spectrum with animal rights and human rights.[191]This issue has been considered in fiction for centuries,[192]and is now being considered by, for example, California's Institute for the Future, however, critics argue that the discussion is premature.[193]"
,Extreme use[edit],Traditional governance structure[edit],"A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.[182]"
,See also[edit],Business planning and project management[edit],"If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.[194]Its intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the ""singularity"".[195]Because it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.[196]"
,Notes[edit],Search,"Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.[197]"
,References[edit],Early history[edit],"Edward Fredkin argues that ""artificial intelligence is the next stage in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.[198]"
,Navigation menu,2000s[edit],"In the past technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI.[199]A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[200]Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classifies only 9% of U.S. jobs as ""high risk"".[v][202]"
,Contents,2010s[edit],"Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"".[203]Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[204]"
,Places[edit],Infrastructure as a service (IaaS)[edit],"AI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.[205]"
,"Arts, entertainment, and media[edit]",Platform as a service (PaaS)[edit],"Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.[206]"
,Military and law enforcement[edit],Software as a service (SaaS)[edit],Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.[207]
,Science[edit],"Mobile ""backend"" as a service (MBaaS)[edit]","AI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.[208]Bias can be inadvertently introduced by the way training data is selected.[209]It can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.[210]An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.[211] Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring."
,Technology[edit],Serverless computing or Function-as-a-Service (FaaS)[edit],"Superintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, ""spell the end of the human race"".[212] Philosopher Nick Bostrom argues that sufficiently intelligent AI if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or ""friendly"" its stated goals might be.[213]Political scientist Charles T. Rubin argues that ""any sufficiently advanced benevolence may be indistinguishable from malevolence."" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.[214]"
,See also[edit],Private cloud[edit],"The opinion of experts and industry insiders is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.[215]Stephen Hawking, Microsoft founder Bill Gates, history professor Yuval Noah Harari, and SpaceX founder Elon Musk have all expressed serious misgivings about the future of AI.[216]Prominent tech titans including Peter Thiel (Amazon Web Services) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as OpenAI and the Future of Life Institute.[217]Mark Zuckerberg (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.[218]Other experts argue is that the risks are far enough in the future to not be worth researching,or that humans will be valuable from the perspective of a superintelligent machine.[219]Rodney Brooks, in particular, has said that ""malevolent"" AI is still centuries away.[w]"
,Navigation menu,Public cloud[edit],"Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[221]"
,Contents,Hybrid cloud[edit],"Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[222]Machine ethics is also called machine morality, computational ethics or computational morality,[222]and was founded at an AAAI symposium in 2005.[223]"
,Etymology,Others[edit],"Other approaches include Wendell Wallach's ""artificial moral agents""[224]and Stuart J. Russell's three principles for developing provably beneficial machines.[225]"
,History,Cloud engineering[edit],"The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.[226]The regulatory and  policy landscape for AI is an emerging issue in jurisdictions globally.[227]Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[46]Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, USA and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[46]The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[46] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[228]"
,Types,Search,"Thought-capable artificial beings have appeared as storytelling devices since antiquity,[17]and have been a persistent theme in science fiction.[19]"
,Hardware,South America[edit],"A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[229]"
,Software,Elsewhere[edit],"Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;[230]while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[231]"
,Networking and the Internet,Fictional characters[edit],Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.
,Unconventional computers,Film and television[edit],"Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[232]"
,Future,Games[edit],These were the four the most widely used AI textbooks in 2008.
,Professions and organizations,Literature[edit],Later editions.
,See also,Music[edit],The two most widely used textbooks in 2021.Open Syllabus: Explorer
,Notes,Land vehicles[edit],See also: Logic machines in fiction and List of fictional computers
,References,Ships[edit],"A blockchain is a growing list of records, called blocks, that are securely linked together using cryptography.[1][2][3][4] Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree, where data nodes are represented by leafs). The timestamp proves that the transaction data existed when the block was published to get into its hash. As blocks each contain information about the block previous to it, they form a chain, with each additional block reinforcing the ones before it. Therefore, blockchains are resistant to modification of their data because once recorded, the data in any given block cannot be altered retroactively without altering all subsequent blocks."
,Sources,Search,"Blockchains are typically managed by a peer-to-peer network for use as a publicly distributed ledger, where nodes collectively adhere to a protocol to communicate and validate new blocks. Although blockchain records are not unalterable as forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.[5]"
,External links,Smaller and larger laptops[edit],"The blockchain was popularized by a person (or group of people) using the name Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin, based on work by Stuart Haber, W. Scott Stornetta, and Dave Bayer.[3][6] The identity of Satoshi Nakamoto remains unknown to date. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications[3][2] and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain is considered a type of payment rail.[7]"
,Navigation menu,"Convertible, hybrid, 2-in-1[edit]","Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model ""snake oil"";[8] however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.[4][9]"
,Contents,Rugged laptop[edit],"Cryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation ""Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups.""[10] Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta.[4][11] They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees into the design, which improved its efficiency by allowing several document certificates to be collected into one block.[4][12] Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995.[6]"
,Genealogy,Display[edit],"The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain.[4] The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.[3]"
,Version history,Central processing unit[edit],"In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes).[13] In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size.  The ledger size had exceeded 200 GB by early 2020.[14]"
,Version control system,Graphical processing unit[edit],"The words block and chain were used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word, blockchain, by 2016.[citation needed]"
,Timeline of releases,Memory[edit],"According to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters' phase.[15] Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce."
,Usage share and device sales,Internal storage[edit],"In May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ""planning or [looking at] active experimentation with blockchain"".[16] For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.[17]"
,Security,Removable media drive[edit],"A blockchain is a decentralized, distributed, and oftentimes public, digital ledger consisting of records called blocks that are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks.[3][18] This allows the participants to verify and audit transactions independently and relatively inexpensively.[19] A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests.[20] Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double-spending. A blockchain has been described as a value-exchange protocol.[21] A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance."
,Alternative implementations,Inputs[edit],"Logically, a blockchain can be seen as consisting of several layers:[22]"
,See also,Input/output (I/O) ports[edit],"Blocks hold batches of valid transactions that are hashed and encoded into a Merkle tree.[3] Each block includes the cryptographic hash of the prior block in the blockchain, linking the two. The linked blocks form a chain.[3] This iterative process confirms the integrity of the previous block, all the way back to the initial block, which is known as the genesis block.[24] To assure the integrity of a block and the data contained in it, the block is usually digitally signed.[25]"
,References,Expansion cards[edit],"Sometimes separate blocks can be produced concurrently, creating a temporary fork. In addition to a secure hash-based history, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks.[24] Peers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially[26] as more blocks are built on top of it, eventually becoming very low.[3][27]: ch. 08 [28] For example, bitcoin uses a proof-of-work system, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of computation. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and parallel manner.[29]"
,External links,Battery and power supply[edit],"The block time is the average time it takes for the network to generate one extra block in the blockchain. Some blockchains create a new block as frequently as every five seconds.[30] By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for Ethereum is set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes.[31]"
,Navigation menu,Power connectors[edit],"A hard fork is a rule change such that the software validating according to the old rules will see the blocks produced according to the new rules as invalid. In case of a hard fork, all nodes meant to work in accordance with the new rules need to upgrade their software. If one group of nodes continues to use the old software while the other nodes use the new software, a permanent split can occur. "
,Contents,Cooling[edit],"For example, Ethereum was hard-forked in 2016 to ""make whole"" the investors in The DAO, which had been hacked by exploiting a vulnerability in its code. In this case, the fork resulted in a split creating Ethereum and Ethereum Classic chains. In 2014 the Nxt community was asked to consider a hard fork that would have led to a rollback of the blockchain records to mitigate the effects of a theft of 50 million NXT from a major cryptocurrency exchange. The hard fork proposal was rejected, and some of the funds were recovered after negotiations and ransom payment. Alternatively, to prevent a permanent split, a majority of nodes using the new software may return to the old rules, as was the case of bitcoin split on 12 March 2013.[32]"
,Background,Docking station[edit],"A more recent hard-fork example is of Bitcoin in 2017, which resulted in a split creating Bitcoin Cash.[33] The network split was mainly due to a disagreement in how to increase the transactions per second to accommodate for demand.[34]"
,Childhood,Charging trolleys[edit],"By storing data across its peer-to-peer network, the blockchain eliminates a number of risks that come with data being held centrally.[3] The decentralized blockchain may use ad hoc message passing and distributed networking. One risk of a lack of decentralization is a so-called ""51% attack"" where a central entity can gain control of more than half of a network and can manipulate that specific blockchain record at will, allowing double-spending.[36]"
,Homestead High,Solar panels[edit],"Peer-to-peer blockchain networks lack centralized points of vulnerability that computer crackers can exploit; likewise, it has no central point of failure. Blockchain security methods include the use of public-key cryptography.[37]: 5  A public key (a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A private key is like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.[3]"
,Reed College,Accessories[edit],"Every node in a decentralized system has a copy of the blockchain. Data quality is maintained by massive database replication[38] and computational trust. No centralized ""official"" copy exists and no user is ""trusted"" more than any other.[37] Transactions are broadcast to the network using the software. Messages are delivered on a best-effort basis. Mining nodes validate transactions,[24] add them to the block they are building, and then broadcast the completed block to other nodes.[27]: ch. 08  Blockchains use various time-stamping schemes, such as proof-of-work, to serialize changes.[39] Alternative consensus methods include proof-of-stake.[24] The growth of a decentralized blockchain is accompanied by the risk of centralization because the computer resources required to process larger amounts of data become more expensive.[40]"
,1972–1985,Modularity[edit],"Open blockchains are more user-friendly than some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain.[41][42][43][44][45] Proponents of permissioned or private chains argue that the term ""blockchain"" may be applied to any data structure that batches data into time-stamped blocks. These blockchains serve as a distributed version of multiversion concurrency control (MVCC) in databases.[46] Just as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain.[47]: 30–31  Opponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision.[41][43] Nikolai Hampton of Computerworld said that ""many in-house blockchain solutions will be nothing more than cumbersome databases,"" and ""without a clear security model, proprietary blockchains should be eyed with suspicion.""[8][48]"
,1985–1997,Obsolete features[edit],"An advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no access control is needed.[26] This means that applications can be added to the network without the approval or trust of others, using the blockchain as a transport layer.[26]"
,1997–2011,Advantages[edit],"Bitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include proof of work. To prolong the blockchain, bitcoin uses Hashcash puzzles. While Hashcash was designed in 1997 by Adam Back, the original idea was first proposed by Cynthia Dwork and Moni Naor and Eli Ponyatovski in their 1992 paper ""Pricing via Processing or Combatting Junk Mail""."
,Health problems,Disadvantages[edit],"In 2016, venture capital investment for blockchain-related projects was weakening in the USA but increasing in China.[49] Bitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018[update], bitcoin has the highest market capitalization."
,Innovations and designs,Manufacturers[edit],"Permissioned blockchains use an access control layer to govern who has access to the network.[50] In contrast to public blockchain networks, validators on private blockchain networks are vetted by the network owner. They do not rely on anonymous nodes to validate transactions nor do they benefit from the network effect.[citation needed] Permissioned blockchains can also go by the name of 'consortium' blockchains.[citation needed] It has been argued that permissioned blockchains can guarantee a certain level of decentralization, if carefully designed, as opposed to permissionless blockchains, which are often centralized in practice.[9]"
,Personal life,Market share[edit],"Nikolai Hampton pointed out in Computerworld that ""There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.""[8] This has a set of particularly profound adverse implications during a financial crisis or debt crisis like the financial crisis of 2007–08, where politically powerful actors may make decisions that favor some groups at the expense of others,[51] and ""the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using gigawatts of computing power — it's time-consuming and expensive.""[8] He also said, ""Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.""[8]"
,Honors and awards,Search,"The analysis of public blockchains has become increasingly important with the popularity of bitcoin, Ethereum, litecoin and other cryptocurrencies.[52] A blockchain, if it is public, provides anyone who wants access to observe and analyse the chain data, given one has the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks.[53][54] The reason for this is accusations of blockchain-enabled cryptocurrencies enabling illicit dark market trade of drugs, weapons, money laundering, etc.[55] A common belief has been that cryptocurrency is private and untraceable, thus leading many actors to use it for illegal purposes. This is changing and now specialised tech companies provide blockchain tracking services, making crypto exchanges, law-enforcement and banks more aware of what is happening with crypto funds and fiat-crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as Monero.[56][57][58] The question is about the public accessibility of blockchain data and the personal privacy of the very same data. It is a key debate in cryptocurrency and ultimately in the blockchain.[59]"
,In popular culture,Music[edit],"In April 2016, Standards Australia submitted a proposal to the International Organization for Standardization to consider developing standards to support blockchain technology. This proposal resulted in the creation of ISO Technical Committee 307, Blockchain and Distributed Ledger Technologies.[60] The technical committee has working groups relating to blockchain terminology, reference architecture, security and privacy, identity, smart contracts, governance and interoperability for blockchain and DLT, as well as standards specific to industry sectors and generic government requirements.[61][non-primary source needed] More than 50 countries are participating in the standardization process together with external liaisons such as the Society for Worldwide Interbank Financial Telecommunication (SWIFT), the European Commission, the International Federation of Surveyors, the International Telecommunication Union (ITU) and the United Nations Economic Commission for Europe (UNECE).[61]"
,See also,"Other uses in arts, entertainment, and media[edit]","Many other national standards bodies and open standards bodies are also working on blockchain standards.[62]  These include the National Institute of Standards and Technology[63] (NIST), the European Committee for Electrotechnical Standardization[64] (CENELEC), the Institute of Electrical and Electronics Engineers[65] (IEEE), the Organization for the Advancement of Structured Information Standards (OASIS), and some individual participants in the  Internet Engineering Task Force[66] (IETF)."
,References,Search,"Currently, there are at least four types of blockchain networks — public blockchains, private blockchains, consortium blockchains and hybrid blockchains."
,External links,Pre-20th century,"A public blockchain has absolutely no access restrictions. Anyone with an Internet connection can send transactions to it as well as become a validator (i.e., participate in the execution of a consensus protocol).[67][self-published source?] Usually, such networks offer economic incentives for those who secure them and utilize some type of a Proof of Stake or Proof of Work algorithm."
,Navigation menu,First computer,"Some of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain."
,,Analog computers,"A private blockchain is permissioned.[50] One cannot join it unless invited by the network administrators. Participant and validator access is restricted. To distinguish between open blockchains and other peer-to-peer decentralized database applications that are not open ad-hoc compute clusters, the terminology Distributed Ledger (DLT) is normally used for private blockchains."
,,Digital computers,A hybrid blockchain has a combination of centralized and decentralized features.[68] The exact workings of the chain can vary based on which portions of centralization and decentralization are used.
,,Modern computers,"A sidechain is a designation for a blockchain ledger that runs in parallel to a primary blockchain.[69][70] Entries from the primary blockchain (where said entries typically represent digital assets) can be linked to and from the sidechain; this allows the sidechain to otherwise operate independently of the primary blockchain (e.g., by using an alternate means of record keeping, alternate consensus algorithm, etc.).[71][better source needed]"
,,Mobile computers,"Blockchain technology can be integrated into multiple areas. The primary use of blockchains is as a distributed ledger for cryptocurrencies such as bitcoin; there were also a few other operational products that had matured from proof of concept by late 2016.[49] As of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their back office.[72]"
,,By architecture,"In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp has estimated that corporate investment into blockchain technology will reach $12.4 billion by 2022.[73] Furthermore, According to PricewaterhouseCoopers (PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicts a significant demand and interest in blockchain technology.[74]"
,,"By size, form-factor and purpose","Individual use of blockchain technology has also greatly increased since 2016. According to statistics in 2020, there were more than 40 million blockchain wallets in 2020 in comparison to around 10 million blockchain wallets in 2016.[75]"
,,History of computing hardware,"Most cryptocurrencies use blockchain technology to record transactions. For example, the bitcoin network and Ethereum network are both based on blockchain. On 8 May 2018 Facebook confirmed that it would open a new blockchain group[76] which would be headed by David Marcus, who previously was in charge of Messenger. Facebook's planned cryptocurrency platform, Libra (now known as Diem), was formally announced on June 18, 2019.[77][78]"
,,Other hardware topics,"The criminal enterprise Silk Road, which operated on Tor, utilized cryptocurrency for payments, some of which the US federal government has seized through research on the blockchain and forfeiture.[79]"
,,Input devices,"Governments have mixed policies on the legality of their citizens or banksowning cryptocurrencies. China implements blockchain technology in several industries including a national digital currency which launched in 2020.[80] To strengthen their respective currencies, Western governments including the European Union and the United States have initiated similar projects.[81]"
,,Output devices,"Blockchain-based smart contracts are proposed contracts that can be partially or fully executed or enforced without human interaction.[82] One of the main objectives of a smart contract is automated escrow. A key feature of smart contracts is that they do not need a trusted third party (such as a trustee) to act as an intermediary between contracting entities -the blockchain network executes the contract on its own. This may reduce friction between entities when transferring value and could subsequently open the door to a higher level of transaction automation.[83] An IMF staff discussion from 2018 reported that smart contracts based on blockchain technology might reduce moral hazards and optimize the use of contracts in general. But ""no viable smart contract systems have yet emerged."" Due to the lack of widespread use their legal status was unclear.[84][85]"
,,Control unit,"According to Reason, many banks have expressed interest in implementing distributed ledgers for use in banking and are cooperating with companies creating private blockchains,[86][87][88] and according to a September 2016 IBM study, this is occurring faster than expected.[89]"
,,Central processing unit (CPU),"Banks are interested in this technology not least because it has the potential to speed up back office settlement systems.[90] Moreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails.[91]"
,,Arithmetic logic unit (ALU),Banks such as UBS are opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.[92][93]
,,Memory,"Berenberg, a German bank, believes that blockchain is an ""overhyped technology"" that has had a large number of ""proofs of concept"", but still has major challenges, and very few success stories.[94]"
,,Input/output (I/O),"The blockchain has also given rise to initial coin offerings (ICOs) as well as a new category of digital asset called security token offerings (STOs), also sometimes referred to as digital security offerings (DSOs).[95] STO/DSOs may be conducted privately or on public, regulated stock exchange and are used to tokenize traditional assets such as company shares as well as more innovative ones like intellectual property, real estate,[96] art, or individual products. A number of companies are active in this space providing services for compliant tokenization, private STOs, and public STOs."
,,Multitasking,"Blockchain technology, such as cryptocurrencies and non-fungible tokens (NFTs), has been used in video games for monetization. Many live-service games offer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling, and has led to gray market issues such as skin gambling, and thus publishers typically have shied away from allowing players to earn real-world funds from games.[97] Blockchain games typically allow players to trade these in-game items for cryptocurrency, which can then be exchanged for money.[98]"
,,Multiprocessing,"The first known game to use blockchain technologies was CryptoKitties, launched in November 2017, where the player would purchase NFTs with Ethereum cryptocurrency, each NFT consisting of a virtual pet that the player could breed with others to create offspring with combined traits as new NFTs.[99][98] The game made headlines in December 2017 when one virtual pet sold for more than US$100,000.[100] CryptoKitties also illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network in early 2018 with approximately 30% of all Ethereum transactions[clarification needed] being for the game.[101][102]"
,,Languages,"By the early 2020s, there had not been a breakout success in video games using blockchain, as these games tend to focus on using blockchain for speculation instead of more traditional forms of gameplay, which offers limited appeal to most players. Such games also represent a high risk to investors as their revenues can be difficult to predict.[98] However, limited successes of some games, such as Axie Infinity during the COVID-19 pandemic, and corporate plans towards metaverse content, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021.[103] Several major publishers, including Ubisoft, Electronic Arts, and Take Two Interactive, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future.[104]"
,,Programs,"In October 2021, Valve Corporation banned blockchain games, including those using cryptocurrency and NFTs, from being hosted on its Steam digital storefront service, which is widely used for personal computer gaming, claiming that this was an extension of their policy banning games that offered in-game items with real-world value. Valve's prior history with gambling, specifically skin gambling, was speculated to be a factor in the decision to ban blockchain games.[105] Journalists and players responded positively to Valve's decision as blockchain and NFT games have a reputation for scams and fraud among most PC gamers,[97][105] Epic Games, which runs the Epic Games Store in competition to Steam, said that they would be open to accepted blockchain games, in the wake of Valve's refusal.[106]"
,,Computer architecture paradigms,There have been several different efforts to employ blockchains in supply chain management.
,,Artificial intelligence,"There are several different efforts to offer domain name services via the blockchain. These domain names can be controlled by the use of a private key, which purports to allow for uncensorable websites. This would also bypass a registrar's ability to suppress domains used for fraud, abuse, or illegal content.[115]"
,,Search,"Namecoin is a cryptocurrency that supports the "".bit"" top-level domain (TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by ICANN, instead requiring an alternative DNS root.[115] As of 2015, it was used by 28 websites, out of 120,000 registered names.[116] Namecoin was dropped by OpenNIC in 2019, due to malware and potential other legal issues.[117] Other blockchain alternatives to ICANN include The Handshake Network,[116] EmerDNS, and Unstoppable Domains.[115]"
,,By marketing role,"Specific TLDs include "".eth"", "".luxe"", and "".kred"", which are associated with the Ethereum blockchain through the Ethereum Name Service (ENS). The .kred TLD also acts as an alternative to conventional cryptocurrency wallet addresses, as a convenience for transferring cryptocurrency.[118]"
,,Early versions,"Blockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users[119] or musicians.[120] The Gartner 2019 CIO Survey reported 2% of higher education respondents had launched blockchain projects and another 18% were planning academic projects in the next 24 months.[121] In 2017, IBM partnered with ASCAP and PRS for Music to adopt blockchain technology in music distribution.[122] Imogen Heap's Mycelia service has also been proposed as a blockchain-based alternative ""that gives artists more control over how their songs and associated data circulate among fans and other musicians.""[123][124]"
,,Windows 3.x,"New distribution methods are available for the insurance industry such as peer-to-peer insurance, parametric insurance and microinsurance following the adoption of blockchain.[125][126] The sharing economy and IoT are also set to benefit from blockchains because they involve many collaborating peers.[127] The use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services.[128]"
,,Windows 9x,"Other blockchain designs include Hyperledger, a collaborative effort from the Linux Foundation to support blockchain-based distributed ledgers, with projects under this initiative including Hyperledger Burrow (by Monax) and Hyperledger Fabric (spearheaded by IBM).[129][130][131] Another is Quorum, a permissionable private blockchain by JPMorgan Chase with private storage, used for contract applications.[132]"
,,Windows NT,Blockchain is also being used in peer-to-peer energy trading.[133][134][135]
,,Windows CE,"Blockchain could be used in detecting counterfeits by associating unique identifiers to products, documents and shipments, and storing records associated with transactions that cannot be forged or altered.[136][137] It is however argued that blockchain technology needs to be supplemented with technologies that provide a strong binding between physical objects and blockchain systems.[138] The EUIPO established an Anti-Counterfeiting Blockathon Forum, with the objective of ""defining, piloting and implementing"" an anti-counterfeiting infrastructure at the European level.[139][140] The Dutch Standardisation organisation NEN uses blockchain together with QR Codes to authenticate certificates.[141]"
,,Xbox OS,"With the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner[142] stated that ""interoperability is the ability of two or more software components to cooperate despite differences in language, interface, and execution platform"". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences."
,,VFSForGit,"There are already several blockchain interoperability solutions available.[143] They can be classified into three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors."
,,File permissions,Several individual IETF participants produced the draft of a blockchain interoperability architecture.[144]
,,Search,"Blockchain mining — the peer-to-peer computer computations by which transactions are validated and verified — requires a significant amount of energy. In June 2018 the Bank for International Settlements criticized the use of public proof-of-work blockchains for their high energy consumption.[145][146][147] In 2021, a study by Cambridge University determined that Bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh).[148] According to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in 24 days.[149]"
,,Biological and adoptive families,"In February 2021, U.S. Treasury secretary Janet Yellen called Bitcoin ""an extremely inefficient way to conduct transactions"", saying ""the amount of energy consumed in processing those transactions is staggering.""[150] In March 2021, Bill Gates stated that ""Bitcoin uses more electricity per transaction than any other method known to mankind"", adding ""It's not a great climate thing.""[151]"
,,Birth and early life,"Nicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate.[152][153] The 31TWh–45TWh of electricity used for bitcoin in 2018 produced 17–22.9 million tonnes of CO2.[154][155] By 2022, the University of Cambridge and Digiconomist estimated that the two largest proof-of-work blockchains, Bitcoin and Ethereum, together used twice as much electricity in one year as the whole of Sweden, leading to the release of up to 120 million tonnes of CO2 each year.[156]"
,,Pre-Apple,"Inside the cryptocurrency industry, concern about high energy consumption has led some companies to consider moving from the proof of work blockchain model to the less energy-intensive proof of stake model.[157] Academics and researchers have estimated that Bitcoin consumes 100,000 times as much energy as proof-of-stake networks.[158][159]"
,,Apple (1976–1985),"In October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.[160] Many universities have founded departments focusing on crypto and blockchain, including MIT, in 2017. In the same year, Edinburgh became ""one of the first big European universities to launch a blockchain course"", according to the Financial Times.[161]"
,,NeXT computer,"Motivations for adopting blockchain technology (an aspect of innovation adoptation) have been investigated by researchers. For example, Janssen, et al. provided a framework for analysis,[162] and Koens & Poll pointed out that adoption could be heavily driven by non-technical factors.[163] Based on behavioral models, Li[164] has discussed the differences between adoption at the individual level and organizational levels."
,,Pixar and Disney,"Scholars in business and management have started studying the role of blockchains to support collaboration.[165][166] It has been argued that blockchains can foster both cooperation (i.e., prevention of opportunistic behavior) and coordination (i.e., communication and information sharing). Thanks to reliability, transparency, traceability of records, and information immutability, blockchains facilitate collaboration in a way that differs both from the traditional use of contracts and from relational norms. Contrary to contracts, blockchains do not directly rely on the legal system to enforce agreements.[167] In addition, contrary to the use of relational norms, blockchains do not require a trust or direct connections between collaborators."
,,Return to Apple,"The need for internal audits to provide effective oversight of organizational efficiency will require a change in the way that  information  is accessed in new formats.[169] Blockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The Institute of Internal Auditors has identified the need for internal auditors to address this transformational technology.  New methods are required to develop audit plans that identify threats and risks. The Internal Audit Foundation study, Blockchain and Internal Audit, assesses these factors.[170] The American Institute of Certified Public Accountants has outlined new roles for auditors as a result of blockchain.[171]"
,,Resignation,"In September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research, Ledger, was announced. The inaugural issue was published in December 2016.[172] The journal covers aspects of mathematics, computer science, engineering, law, economics and philosophy that relate to cryptocurrencies such as bitcoin.[173][174]"
,,Death,"The journal encourages authors to digitally sign a file hash of submitted papers, which are then timestamped into the bitcoin blockchain. Authors are also asked to include a personal bitcoin address on the first page of their papers for non-repudiation purposes.[175]"
,,Apple I,"The Internet of things (IoT) describes physical objects (or groups of such objects)  with sensors, processing ability, software, and other technologies that connect and exchange data with other devices and systems over the Internet or other communications networks.[1][2][3][4] Internet of things has been considered a misnomer because devices do not need to be connected to the public internet, they only need to be connected to a network and be individually addressable.[5][6]"
,,Apple II,"The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning.[7]  Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), independently and collectively enable the Internet of things.[8]  In the consumer market, IoT technology is most synonymous with products pertaining to the concept of the ""smart home"", including devices and appliances (such as lighting fixtures, thermostats, home security systems, cameras, and other home appliances) that support one or more common ecosystems, and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers. IoT is also used in healthcare systems.[9]"
,,Apple Lisa,"There are a number of concerns about the risks in the growth of IoT technologies and products, especially in the areas of privacy and security, and consequently, industry and governmental moves to address these concerns have begun, including the development of international and local standards, guidelines, and regulatory frameworks.[10]"
,,Macintosh,"The main concept of a network of smart devices was discussed as early as 1982, with a modified Coca-Cola vending machine at Carnegie Mellon University becoming the first ARPANET-connected appliance,[11] able to report its inventory and whether newly loaded drinks were cold or not.[12] Mark Weiser's 1991 paper on ubiquitous computing, ""The Computer of the 21st Century"", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IOT.[13][14] In 1994, Reza Raji described the concept in IEEE Spectrum as ""[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories"".[15] Between 1993 and 1997, several companies proposed solutions like Microsoft's at Work or Novell's NEST. The field gained momentum when Bill Joy envisioned device-to-device communication as a part of his ""Six Webs"" framework, presented at the World Economic Forum at Davos in 1999.[16]"
,,NeXT Computer,"The concept of the ""Internet of things"" and the term itself, first appeared in a speech by Peter T. Lewis, to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C, published in September 1985.[17] According to Lewis, ""The Internet of Things, or IoT, is the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices."""
,,iMac,"The term ""Internet of things"" was coined independently by Kevin Ashton of Procter & Gamble, later MIT's Auto-ID Center, in 1999,[18] though he prefers the phrase ""Internet for things"".[19] At that point, he viewed radio-frequency identification (RFID) as essential to the Internet of things,[20] which would allow computers to manage all individual things.[21][22][23] The main theme of the Internet of things is to embed short-range mobile transceivers in various gadgets and daily necessities to enable new forms of communication between people and things, and between things themselves.[24]"
,,iTunes,"Defining the Internet of things as ""simply the point in time when more 'things or objects' were connected to the Internet than people"", Cisco Systems estimated that the IoT was ""born"" between 2008 and 2009, with the things/people ratio growing from 0.08 in 2003 to 1.84 in 2010.[25]"
,,iPod,"The extensive set of applications for IoT devices[26] is often divided into consumer, commercial, industrial, and infrastructure spaces.[27][28]"
,,iPhone,"A growing portion of IoT devices are created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities.[29]"
,,iPad,"IoT devices are a part of the larger concept of home automation, which can include lighting, heating and air conditioning, media and security systems and camera systems.[30][31] Long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage.[32]"
,,Marriage,"A smart home or automated home could be based on a platform or hubs that control smart devices and appliances.[33] For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch.[34][35] This could be a dedicated app or iOS native applications such as Siri.[36] This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge.[36] There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products and these include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub.[37] In addition to the commercial systems, there are many non-proprietary, open source ecosystems; including Home Assistant, OpenHAB and Domoticz.[38][39]"
,,Family,"One key application of a smart home is to provide assistance to elderly individuals and to those with disabilities. These home systems use assistive technology to accommodate an owner's specific disabilities.[40] Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by hearing-impaired users.[41] They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or seizures.[42] Smart home technology applied in this way can provide users with more freedom and a higher quality of life.[40]"
,,Philanthropy,"The term ""Enterprise IoT"" refers to devices used in business and corporate settings. By 2019, it is estimated that the EIoT will account for 9.1 billion devices.[27]"
,,Search,"The Internet of Medical Things (IoMT) is an application of the IoT for medical and health related purposes, data collection and analysis for research, and monitoring.[43][44][45][46][47] The IoMT has been referenced as ""Smart Healthcare"",[48] as the technology for creating a digitized healthcare system, connecting available medical resources and healthcare services.[49][50]"
,,,"IoT devices can be used to enable remote health monitoring and emergency notification systems. These health monitoring devices can range from blood pressure and heart rate monitors to advanced devices capable of monitoring specialized implants, such as pacemakers, Fitbit electronic wristbands, or advanced hearing aids.[51] Some hospitals have begun implementing ""smart beds"" that can detect when they are occupied and when a patient is attempting to get up. It can also adjust itself to ensure appropriate pressure and support is applied to the patient without the manual interaction of nurses.[43] A 2015 Goldman Sachs report indicated that healthcare IoT devices ""can save the United States more than $300 billion in annual healthcare expenditures by increasing revenue and decreasing cost.""[52] Moreover, the use of mobile devices to support medical follow-up led to the creation of 'm-health', used analyzed health statistics.""[53]"
,,,"Specialized sensors can also be equipped within living spaces to monitor the health and general well-being of senior citizens, while also ensuring that proper treatment is being administered and assisting people to regain lost mobility via therapy as well.[54] These sensors create a network of intelligent sensors that are able to collect, process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems.[48] Other consumer devices to encourage healthy living, such as connected scales or wearable heart monitors, are also a possibility with the IoT.[55] End-to-end health monitoring IoT platforms are also available for antenatal and chronic patients, helping one manage health vitals and recurring medication requirements.[56]"
,,,"Advances in plastic and fabric electronics fabrication methods have enabled ultra-low cost, use-and-throw IoMT sensors. These sensors, along with the required RFID electronics, can be fabricated on paper or e-textiles for wireless powered disposable sensing devices.[57] Applications have been established for point-of-care medical diagnostics, where portability and low system-complexity is essential.[58]"
,,,"As of 2018[update] IoMT was not only being applied in the clinical laboratory industry,[45] but also in the healthcare and health insurance industries. IoMT in the healthcare industry is now permitting doctors, patients, and others, such as guardians of patients, nurses, families, and similar, to be part of a system, where patient records are saved in a database, allowing doctors and the rest of the medical staff to have access to patient information.[59] Moreover, IoT-based systems are patient-centered, which involves being flexible to the patient's medical conditions.[citation needed] IoMT in the insurance industry provides access to better and new types of dynamic information. This includes sensor-based solutions such as biosensors, wearables, connected health devices, and mobile apps to track customer behavior. This can lead to more accurate underwriting and new pricing models.[60]"
,,,The application of the IoT in healthcare plays a fundamental role in managing chronic diseases and in disease prevention and control. Remote monitoring is made possible through the connection of powerful wireless solutions. The connectivity enables health practitioners to capture patient's data and applying complex algorithms in health data analysis.[61]
,,,"The IoT can assist in the integration of communications, control, and information processing across various transportation systems. Application of the IoT extends to all aspects of transportation systems (i.e. the vehicle,[62] the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication,[63] smart traffic control, smart parking, electronic toll collection systems, logistics and fleet management, vehicle control, safety, and road assistance.[51][64]"
,,,"In vehicular communication systems, vehicle-to-everything communication (V2X), consists of three main components: vehicle to vehicle communication (V2V), vehicle to infrastructure communication (V2I) and vehicle to pedestrian communications (V2P). V2X is the first step to autonomous driving and connected road infrastructure.[citation needed]"
,,,"IoT devices can be used to monitor and control the mechanical, electrical and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential)[51] in home automation and building automation systems. In this context, three main areas are being covered in literature:[65]"
,,,"Also known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, operational technology (OT), locations, and people. Combined with operational technology (OT) monitoring devices, IIoT helps regulate and monitor industrial systems. Also, the same implementation can be carried out for automated record updates of asset placement in industrial storage units as the size of the assets can vary from a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money."
,,,"The IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities.[66] Network control and management of manufacturing equipment, asset and situation management, or manufacturing process control allow IoT to be used for industrial applications and smart manufacturing.[67] IoT intelligent systems enable rapid manufacturing and optimization of new products, and rapid response to product demands.[51]"
,,,"Digital control systems to automate process controls, operator tools and service information systems to optimize plant safety and security are within the purview of the IIoT.[68] IoT can also be applied to asset management via predictive maintenance, statistical evaluation, and measurements to maximize reliability.[69] Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors.[51]"
,,,"In addition to general manufacturing, IoT is also used for processes in the industrialization of construction.[70]"
,,,"There are numerous IoT applications in farming[71] such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, take informed decisions to improve quality and quantity, minimize risk and waste, and reduce the effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar, and even apply IoT-acquired data to precision fertilization programs.[72] The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs."
,,,"In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide.[73] The FarmBeats project[74] from Microsoft Research that uses TV white space to connect farms is also a part of the Azure Marketplace now.[75]"
,,,"IoT devices are in use monitoring the environments and systems of boats and yachts.[76] Many pleasure boats are left unattended for days in summer, and months in winter so such devices provide valuable early alerts of boat flooding, fire, and deep discharge of batteries. The use of global internet data networks such as Sigfox, combined with long-life batteries, and microelectronics allows the engine rooms, bilge, and batteries to be constantly monitored and reported to a connected Android & Apple applications for example."
,,,"Monitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind-farms is a key application of the IoT.[68] The IoT infrastructure can be used for monitoring any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost-saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and save money with Real-Time Data Analytics. It can also be used for scheduling repair and maintenance activities in an efficient manner, by coordinating tasks between different service providers and users of these facilities.[51] IoT devices can also be used to control critical infrastructure like bridges to provide access to ships. Usage of IoT devices for monitoring and operating infrastructure is likely to improve incident management and emergency response coordination, and quality of service, up-times and reduce costs of operation in all infrastructure related areas.[77] Even areas such as waste management can benefit[78] from automation and optimization that could be brought in by the IoT.[citation needed]"
,,,"There are several planned or ongoing large-scale deployments of the IoT, to enable better management of cities and systems. For example, Songdo, South Korea, the first of its kind fully equipped and wired smart city, is gradually being built, with approximately 70 percent of the business district completed as of June 2018[update]. Much of the city is planned to be wired and automated, with little or no human intervention.[79]"
,,,"Another application is currently undergoing a project in Santander, Spain. For this deployment, two approaches have been adopted. This city of 180,000 inhabitants has already seen 18,000 downloads of its city smartphone app. The app is connected to 10,000 sensors that enable services like parking search, environmental monitoring, digital city agenda, and more. City context information is used in this deployment so as to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification.[80]"
,,,"Other examples of large-scale deployments underway include the Sino-Singapore Guangzhou Knowledge City;[81] work on improving air and water quality, reducing noise pollution, and increasing transportation efficiency in San Jose, California;[82] and smart traffic management in western Singapore.[83] Using its RPMA (Random Phase Multiple Access) technology, San Diego-based Ingenu has built a nationwide public network[84] for low-bandwidth data transmissions using the same unlicensed 2.4 gigahertz spectrum as Wi-Fi. Ingenu's ""Machine Network"" covers more than a third of the US population across 35 major cities including San Diego and Dallas.[85] French company, Sigfox, commenced building an Ultra Narrowband wireless data network in the San Francisco Bay Area in 2014, the first business to achieve such a deployment in the U.S.[86][87] It subsequently announced it would set up a total of 4000 base stations to cover a total of 30 cities in the U.S. by the end of 2016, making it the largest IoT network coverage provider in the country thus far.[88][89] Cisco also participates in smart cities projects. Cisco has started deploying technologies for Smart Wi-Fi, Smart Safety & Security, Smart Lighting, Smart Parking, Smart Transports, Smart Bus Stops, Smart Kiosks, Remote Expert for Government Services (REGS) and Smart Education in the five km area in the city of Vijaywada, India.[90]"
,,,"Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live 24/7. The network was designed and engineered by Fluidmesh Networks, a Chicago-based company developing wireless networks for critical applications. The NYWW network is currently providing coverage on the Hudson River, East River, and Upper New York Bay. With the wireless network in place, NY Waterway is able to take control of its fleet and passengers in a way that was not previously possible. New applications can include security, energy and fleet management, digital signage, public Wi-Fi, paperless ticketing and others.[91]"
,,,"Significant numbers of energy-consuming devices (e.g. lamps, household appliances, motors, pumps, etc.) already integrate Internet connectivity, which can allow them to communicate with utilities not only to balance power generation but also helps optimize the energy consumption as a whole.[51] These devices allow for remote control by users, or central management via a cloud-based interface, and enable functions like scheduling (e.g., remotely powering on or off heating systems, controlling ovens, changing lighting conditions etc.).[51] The smart grid is a utility-side IoT application; systems gather and act on energy and power-related information to improve the efficiency of the production and distribution of electricity.[92] Using advanced metering infrastructure (AMI) Internet-connected devices, electric utilities not only collect data from end-users, but also manage distribution automation devices like transformers.[51]"
,,,"Environmental monitoring applications of the IoT typically use sensors to assist in environmental protection[93] by monitoring air or water quality,[94] atmospheric or soil conditions,[95] and can even include areas like monitoring the movements of wildlife and their habitats.[96] Development of resource-constrained devices connected to the Internet also means that other applications like earthquake or tsunami early-warning systems can also be used by emergency services to provide more effective aid. IoT devices in this application typically span a large geographic area and can also be mobile.[51] It has been argued that the standardization that IoT brings to wireless sensing will revolutionize this area.[97]"
,,,Living Lab
,,,"Another example of integrating the IoT is Living Lab which integrates and combines research and innovation processes, establishing within a public-private-people-partnership.[98] There are currently 320 Living Labs that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. For companies to implement and develop IoT services for smart cities, they need to have incentives. The governments play key roles in smart city projects as changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the government provides tax incentives and cheap rent, improves public transports, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production process, and transaction costs.[98] The relationship between the technology developers and governments who manage the city's assets, is key to provide open access to resources to users in an efficient way."
,,,"The Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield.[99]"
,,,"The Internet of Battlefield Things (IoBT) is a project initiated and executed by the U.S. Army Research Laboratory (ARL) that focuses on the basic science related to the IoT that enhance the capabilities of Army soldiers.[100] In 2017, ARL launched the Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA), establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations.[101][102]"
,,,"The Ocean of Things project is a DARPA-led program designed to establish an Internet of things across large ocean areas for the purposes of collecting, monitoring, and analyzing environmental and vessel activity data. The project entails the deployment of about 50,000 floats that house a passive sensor suite that autonomously detect and track military and commercial vessels as part of a cloud-based network.[103]"
,,,"There are several applications of smart or active packaging in which a QR code or NFC tag is affixed on a product or its packaging. The tag itself is passive, however, it contains a unique identifier (typically a URL) which enables a user to access digital content about the product via a smartphone.[104] Strictly speaking, such passive items are not part of the Internet of things, but they can be seen as enablers of digital interactions.[105] The term ""Internet of Packaging"" has been coined to describe applications in which unique identifiers are used, to automate supply chains, and are scanned on large scale by consumers to access digital content.[106] Authentication of the unique identifiers, and thereby of the product itself, is possible via a copy-sensitive digital watermark or copy detection pattern for scanning when scanning a QR code,[107] while NFC tags can encrypt communication.[108]"
,,,The IoT's major significant trend in recent years is the explosive growth of devices connected and controlled by the Internet.[109] The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.
,,,"The IoT creates opportunities for more direct integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions.[110][111][112][113]"
,,,The number of IoT devices increased 31% year-over-year to 8.4 billion in the year 2017[114] and it is estimated that there will be 30 billion devices by 2020.[109] The global market value of the IoT is projected to reach $7.1 trillion by 2020.[115]
,,,"Ambient intelligence and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as Intel) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT.[116] A promising approach in this context is deep reinforcement learning where most of IoT systems provide a dynamic and interactive environment.[117] Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as supervised learning. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn HVAC on or off) and learn through the maximizing accumulated rewards it receives in long term."
,,,"IoT intelligence can be offered at three levels: IoT devices, Edge/Fog nodes, and Cloud computing.[118] The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time obstacle detection to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder.[119][118]"
,,,"In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend,[120] clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments.[121]"
,,,"IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud.[122] Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as Modbus, Bluetooth, Zigbee, or proprietary protocols, to connect to an Edge Gateway.[122] The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing.[122] Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various database systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, PostgreSQL).[122] The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers.[123] Some experts classified the three-tiers in the IoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively.[124]"
,,,"Building on the Internet of things, the web of things is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices.[citation needed]"
,,,"The Internet of things requires huge scalability in the network space to handle the surge of devices.[125] IETF 6LoWPAN would be used to connect devices to IP networks. With billions of devices[126] being added to the Internet space, IPv6 will play a major role in handling the network layer scalability. IETF's Constrained Application Protocol, ZeroMQ, and MQTT would provide lightweight data transport."
,,,Fog computing is a viable alternative to prevent such a large burst of data flow through the Internet.[127] The edge devices' computation power to analyse and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the internet to a server with sufficient processing power.[128]
,,,"Decentralized Internet of things, or decentralized IoT, is a modified IoT. It utilizes Fog Computing to handle and balance requests of connected IoT devices in order to reduce loading on the cloud servers, and improve responsiveness for latency-sensitive IoT applications like vital signs monitoring of patients, vehicle-to-vehicle communication of autonomous driving, and critical failure detection of industrial devices.[129]"
,,,"Conventional IoT is connected via a mesh network and led by a major head node (centralized controller).[130] The head node decides how a data is created, stored, and transmitted.[131] In contrast, decentralized IoT attempts to divide IoT systems into smaller divisions.[132] The head node authorizes partial decision making power to lower level sub-nodes under mutual agreed policy.[133] Performance is improved, especially for huge IoT systems with millions of nodes.[134]"
,,,Decentralized IoT attempts to address the limited bandwidth and hashing capacity of battery-powered or wireless IoT devices via lightweight blockchain.[135][136][137]
,,,Cyberattack identification can be done through early detection and mitigation at the edge nodes with traffic monitoring and evaluation.[138]
,,,"In semi-open or closed loops (i.e. value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a complex system[139] due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality).As a practical approach, not all elements in the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability. For example, domestic robotics (domotics) running inside a smart home might only share data within and be available via a local network.[140] Managing and controlling a high dynamic ad hoc IoT things/devices network is a tough task with the traditional networks architecture, Software Defined Networking (SDN) provides the agile dynamic solution that can cope with the special requirements of the diversity of innovative IoT applications.[141][142]"
,,,"The Internet of things would encode 50 to 100 trillion objects, and be able to follow the movement of those objects. Human beings in surveyed urban environments are each surrounded by 1000 to 5000 trackable objects.[143] In 2015 there were 83 million smart devices in people's homes. This number is expected to grow to 193 million devices by 2020.[31][144]"
,,,The figure of online capable devices grew 31% from 2016 to 2017 to reach 8.4 billion.[114]
,,,"In the Internet of things, the precise geographic location of a thing—and also the precise geographic dimensions of a thing—will be critical.[145] Therefore, facts about a thing, such as its location in time and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things in the Internet of things will be sensors, and sensor location is usually important.[146]) The GeoWeb and Digital Earth are promising applications that become possible when things can become organized and connected by location. However, the challenges that remain include the constraints of variable spatial scales, the need to handle massive amounts of data, and an indexing for fast search and neighbour operations. In the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role in the Internet and the Web, geo-spatial standards will play a key role in the Internet of things.[147][148]"
,,,"Many IoT devices have the potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note,[149] where he predicts that the most likely problem will be what he calls the ""basket of remotes"" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another.[149] For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, ""where collected data is used to predict and trigger actions on the specific devices"" while making them work together.[150]"
,,,"Social Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices.[151] SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services,[152] and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering.[153]"
,,,"IoT defines a device with an identity like a citizen in a community, and connect them to the internet to provide services to its users.[154] SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human.[155]"
,,,"SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users.[156]"
,,,"IoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, navigates and groups with other IoT devices in the same or nearby network for useful service compositions in order to help its users proactively in every day's life especially during emergency.[157]"
,,,"There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:[164][165][166]"
,,,"The original idea of the Auto-ID Center is based on RFID-tags and distinct identification through the Electronic Product Code. This has evolved into objects having an IP address or URI.[167] An alternative view, from the world of the Semantic Web[168] focuses instead on making all things (not just those electronic, smart, or RFID-enabled) addressable by the existing naming protocols, such as URI. The objects themselves do not converse, but they may now be referred to by other agents, such as powerful centralised servers acting for their human owners.[169] Integration with the Internet implies that devices will use an IP address as a distinct identifier. Due to the limited address space of IPv4 (which allows for 4.3 billion different addresses), objects in the IoT will have to use the next generation of the Internet protocol (IPv6) to scale to the extremely large address space required.[170][171][172]Internet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6,[173] as it reduces the configuration overhead on the hosts,[171] and the IETF 6LoWPAN header compression. To a large extent, the future of the Internet of things will not be possible without the support of IPv6; and consequently, the global adoption of IPv6 in the coming years will be critical for the successful development of the IoT in the future.[172]"
,,,"This is a list of technical standards for the IoT, most of which are open standards, and the standards organizations that aspire to successfully setting them.[176][177]"
,,,"The GS1 digital link standard,[181] first released in August 2018, allows the use QR Codes, GS1 Datamatrix, RFID and NFC to enable various types of business-to-business, as well as business-to-consumers interactions."
,,,"Some scholars and activists argue that the IoT can be used to create new models of civic engagement if device networks can be open to user control and inter-operable platforms. Philip N. Howard, a professor and author, writes that political life in both democracies and authoritarian regimes will be shaped by the way the IoT will be used for civic engagement. For that to happen, he argues that any connected device should be able to divulge a list of the ""ultimate beneficiaries"" of its sensor data and that individual citizens should be able to add new organisations to the beneficiary list. In addition, he argues that civil society groups need to start developing their IoT strategy for making use of data and engaging with the public.[183]"
,,,"One of the key drivers of the IoT is data. The success of the idea of connecting devices to make them more efficient is dependent upon access to and storage & processing of data. For this purpose, companies working on the IoT collect data from multiple sources and store it in their cloud network for further processing. This leaves the door wide open for privacy and security dangers and single point vulnerability of multiple systems.[184] The other issues pertain to consumer choice and ownership of data[185] and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop.[186][187][188] IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995.[189]"
,,,Current regulatory environment:
,,,A report published by the Federal Trade Commission (FTC) in January 2015 made the following three recommendations:[190]
,,,"However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights.[192]"
,,,"A resolution passed by the Senate in March 2015, is already being considered by the Congress.[193] This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the Federal Communications Commission to assess the need for more spectrum to connect IoT devices."
,,,"Approved on 28 September 2018, California Senate Bill No. 327[194] goes into effect on 1 January 2020. The bill requires ""a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure,"""
,,,"Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the National Highway Traffic Safety Administration (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure.[195]"
,,,A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT.[196] These include –
,,,"In early December 2021, the U.K. government introduced the Product Security and Telecommunications Infrastructure bill (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards. The bill also seeks to improve the security credentials of consumer IoT devices.[197]"
,,,"The IoT suffers from platform fragmentation, lack of interoperability and common technical standards[198][199][200][201][202][203][204][excessive citations] a situation where the variety of IoT devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently between different inconsistent technology ecosystems hard.[1] For example, wireless connectivity for IoT devices can be done using Bluetooth, Zigbee, Z-Wave, LoRa, NB-IoT, Cat M1 as well as completely custom proprietary radios – each with its own advantages and disadvantages; and unique support ecosystem.[205]"
,,,"The IoT's amorphous computing nature is also a problem for security, since patches to bugs found in the core operating system often do not reach users of older and lower-price devices.[206][207][208] One set of researchers say that the failure of vendors to support older devices with patches and updates leaves more than 87% of active Android devices vulnerable.[209][210]"
,,,"Philip N. Howard, a professor and author, writes that the Internet of things offers immense potential for empowering citizens, making government transparent, and broadening information access. Howard cautions, however, that privacy threats are enormous, as is the potential for social control and political manipulation.[211]"
,,,"Concerns about privacy have led many to consider the possibility that big data infrastructures such as the Internet of things and data mining are inherently incompatible with privacy.[212] Key challenges of increased digitalization in the water, transport or energy sector are related to privacy and cybersecurity which necessitate an adequate response from research and policymakers alike.[213]"
,,,"Writer Adam Greenfield claims that IoT technologies are not only an invasion of public space but are also being used to perpetuate normative behavior, citing an instance of billboards with hidden cameras that tracked the demographics of passersby who stopped to read the advertisement."
,,,"The Internet of Things Council compared the increased prevalence of digital surveillance due to the Internet of things to the conceptual panopticon described by Jeremy Bentham in the 18th Century.[214] The assertion was defended by the works of French philosophers Michel Foucault and Gilles Deleuze. In Discipline and Punish: The Birth of the Prison Foucault asserts that the panopticon was a central element of the discipline society developed during the Industrial Era.[215] Foucault also argued that the discipline systems established in factories and school reflected Bentham's vision of panopticism.[215] In his 1992 paper ""Postscripts on the Societies of Control,"" Deleuze wrote that the discipline society had transitioned into a control society, with the computer replacing the panopticon as an instrument of discipline and control while still maintaining the qualities similar to that of panopticism.[216]"
,,,"Peter-Paul Verbeek, a professor of philosophy of technology at the University of Twente, Netherlands, writes that technology already influences our moral decision making, which in turn affects human agency, privacy and autonomy. He cautions against viewing technology merely as a human tool and advocates instead to consider it as an active agent.[217]"
,,,"Justin Brookman, of the Center for Democracy and Technology, expressed concern regarding the impact of the IoT on consumer privacy, saying that ""There are some people in the commercial space who say, 'Oh, big data – well, let's collect everything, keep it around forever, we'll pay for somebody to think about security later.' The question is whether we want to have some sort of policy framework in place to limit that.""[218]"
,,,"Tim O'Reilly believes that the way companies sell the IoT devices on consumers are misplaced, disputing the notion that the IoT is about gaining efficiency from putting all kinds of devices online and postulating that the ""IoT is really about human augmentation. The applications are profoundly different when you have sensors and data driving the decision-making.""[219]"
,,,"Editorials at WIRED have also expressed concern, one stating ""What you're about to lose is your privacy. Actually, it's worse than that. You aren't just going to lose your privacy, you're going to have to watch the very concept of privacy be rewritten under your nose.""[220]"
,,,"The American Civil Liberties Union (ACLU) expressed concern regarding the ability of IoT to erode people's control over their own lives. The ACLU wrote that ""There's simply no way to forecast how these immense powers – disproportionately accumulating in the hands of corporations seeking financial advantage and governments craving ever more control – will be used. Chances are big data and the Internet of Things will make it harder for us to control our own lives, as we grow increasingly transparent to powerful corporations and government institutions that are becoming more opaque to us.""[221]"
,,,"In response to rising concerns about privacy and smart technology, in 2007 the British Government stated it would follow formal Privacy by Design principles when implementing their smart metering program. The program would lead to replacement of traditional power meters with smart power meters, which could track and manage energy usage more accurately.[222] However the British Computer Society is doubtful these principles were ever actually implemented.[223] In 2009 the Dutch Parliament rejected a similar smart metering program, basing their decision on privacy concerns. The Dutch program later revised and passed in 2011.[223]"
,,,"A challenge for producers of IoT applications is to clean, process and interpret the vast amount of data which is gathered by the sensors. There is a solution proposed for the analytics of the information referred to as Wireless Sensor Networks.[224] These networks share data among sensor nodes that are sent to a distributed system for the analytics of the sensory data.[225]"
,,,"Another challenge is the storage of this bulk data. Depending on the application, there could be high data acquisition requirements, which in turn lead to high storage requirements. Currently the Internet is already responsible for 5% of the total energy generated,[224] and a ""daunting challenge to power"" IoT devices to collect and even store data still remains.[226]"
,,,"Security is the biggest concern in adopting Internet of things technology,[227] with concerns that rapid development is happening without appropriate consideration of the profound security challenges involved[228] and the regulatory changes that might be necessary.[229][230]"
,,,"Most of the technical security concerns are similar to those of conventional servers, workstations and smartphones.[231] These concerns include using weak authentication, forgetting to change default credentials, unencrypted messages sent between devices, SQL injections, Man-in-the-middle attacks, and poor handling of security updates.[232][233] However, many IoT devices have severe operational limitations on the computational power available to them. These constraints often make them unable to directly use basic security measures such as implementing firewalls or using strong cryptosystems to encrypt their communications with other devices[234] - and the low price and consumer focus of many devices makes a robust security patching system uncommon.[235]"
,,,"Rather than conventional security vulnerabilities, fault injection attacks are on the rise and targeting IoT devices. A fault injection attack is a physical attack on a device to purposefully introduce faults in the system to change the intended behavior. Faults might happen unintentionally by environmental noises and electromagnetic fields. There are ideas stemmed from control-flow integrity (CFI) to prevent fault injection attacks and system recovery to a healthy state before the fault.[236]"
,,,"Internet of things devices also have access to new areas of data, and can often control physical devices,[237] so that even by 2014 it was possible to say that many Internet-connected appliances could already ""spy on people in their own homes"" including televisions, kitchen appliances,[238] cameras, and thermostats.[239] Computer-controlled devices in automobiles such as brakes, engine, locks, hood and trunk releases, horn, heat, and dashboard have been shown to be vulnerable to attackers who have access to the on-board network. In some cases, vehicle computer systems are Internet-connected, allowing them to be exploited remotely.[240] By 2008 security researchers had shown the ability to remotely control pacemakers without authority. Later hackers demonstrated remote control of insulin pumps[241] and implantable cardioverter defibrillators.[242]"
,,,"Poorly secured Internet-accessible IoT devices can also be subverted to attack others. In 2016, a distributed denial of service attack powered by Internet of things devices running the Mirai malware took down a DNS provider and major web sites.[243] The Mirai Botnet had infected roughly 65,000 IoT devices within the first 20 hours.[244] Eventually the infections increased to around 200,000 to 300,000 infections.[244] Brazil, Colombia and Vietnam made up of 41.5% of the infections.[244] The Mirai Botnet had singled out specific IoT devices that consisted of DVRs, IP cameras, routers and printers.[244] Top vendors that contained the most infected devices were identified as Dahua, Huawei, ZTE, Cisco, ZyXEL and MikroTik.[244] In May 2017, Junade Ali, a Computer Scientist at Cloudflare noted that native DDoS vulnerabilities exist in IoT devices due to a poor implementation of the Publish–subscribe pattern.[245][246] These sorts of attacks have caused security experts to view IoT as a real threat to Internet services.[247]"
,,,"The U.S. National Intelligence Council in an unclassified report maintains that it would be hard to deny ""access to networks of sensors and remotely-controlled objects by enemies of the United States, criminals, and mischief makers... An open market for aggregated sensor data could serve the interests of commerce and security no less than it helps criminals and spies identify vulnerable targets. Thus, massively parallel sensor fusion may undermine social cohesion, if it proves to be fundamentally incompatible with Fourth-Amendment guarantees against unreasonable search.""[248] In general, the intelligence community views the Internet of things as a rich source of data.[249]"
,,,"On 31 January 2019, the Washington Post wrote an article regarding the security and ethical challenges that can occur with IoT doorbells and cameras: ""Last month, Ring got caught allowing its team in Ukraine to view and annotate certain user videos; the company says it only looks at publicly shared videos and those from Ring owners who provide consent. Just last week, a California family's Nest camera let a hacker take over and broadcast fake audio warnings about a missile attack, not to mention peer in on them, when they used a weak password""[250]"
,,,"There have been a range of responses to concerns over security. The Internet of Things Security Foundation (IoTSF) was launched on 23 September 2015 with a mission to secure the Internet of things by promoting knowledge and best practice. Its founding board is made from technology providers and telecommunications companies. In addition, large IT companies are continually developing innovative solutions to ensure the security of IoT devices. In 2017, Mozilla launched Project Things, which allows to route IoT devices through a safe Web of Things gateway.[251] As per the estimates from KBV Research,[252] the overall IoT security market[253] would grow at 27.9% rate during 2016–2022 as a result of growing infrastructural concerns and diversified usage of Internet of things.[254][255]"
,,,"Governmental regulation is argued by some to be necessary to secure IoT devices and the wider Internet – as market incentives to secure IoT devices is insufficient.[256][229][230] It was found that due to the nature of most of the IoT development boards, they generate predictable and weak keys which make it easy to be utilized by Man-in-the-middle attack. However, various hardening approaches were proposed by many researchers to resolve the issue of SSH weak implementation and weak keys.[257]"
,,,"IoT systems are typically controlled by event-driven smart apps that take as input either sensed data, user inputs, or other external triggers (from the Internet) and command one or more actuators towards providing different forms of automation.[258] Examples of sensors include smoke detectors, motion sensors, and contact sensors. Examples of actuators include smart locks, smart power outlets, and door controls. Popular control platforms on which third-party developers can build smart apps that interact wirelessly with these sensors and actuators include Samsung's SmartThings,[259] Apple's HomeKit,[260] and Amazon's Alexa,[261] among others."
,,,"A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states, e.g., ""unlock the entrance door when no one is at home"" or ""turn off the heater when the temperature is below 0 degrees Celsius and people are sleeping at night"".[258] Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. Recently, researchers from the University of California Riverside have proposed IotSan, a novel practical system that uses model checking as a building block to reveal ""interaction-level"" flaws by identifying events that can lead the system to unsafe states.[258] They have evaluated IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities (i.e., violations of safe physical states/properties)."
,,,"Given widespread recognition of the evolving nature of the design and management of the Internet of things, sustainable and secure deployment of IoT solutions must design for ""anarchic scalability.""[262] Application of the concept of anarchic scalability can be extended to physical systems (i.e. controlled real-world objects), by virtue of those systems being designed to account for uncertain management futures. This hard anarchic scalability thus provides a pathway forward to fully realize the potential of Internet-of-things solutions by selectively constraining physical systems to allow for all management regimes without risking physical failure.[262]"
,,,"Brown University computer scientist Michael Littman has argued that successful execution of the Internet of things requires consideration of the interface's usability as well as the technology itself. These interfaces need to be not only more user-friendly but also better integrated: ""If users need to learn different interfaces for their vacuums, their locks, their sprinklers, their lights, and their coffeemakers, it's tough to say that their lives have been made any easier.""[263]"
,,,"A concern regarding Internet-of-things technologies pertains to the environmental impacts of the manufacture, use, and eventual disposal of all these semiconductor-rich devices.[264] Modern electronics are replete with a wide variety of heavy metals and rare-earth metals, as well as highly toxic synthetic chemicals. This makes them extremely difficult to properly recycle. Electronic components are often incinerated or placed in regular landfills. Furthermore, the human and environmental cost of mining the rare-earth metals that are integral to modern electronic components continues to grow. This leads to societal questions concerning the environmental impacts of IoT devices over their lifetime.[265]"
,,,"The Electronic Frontier Foundation has raised concerns that companies can use the technologies necessary to support connected devices to intentionally disable or ""brick"" their customers' devices via a remote software update or by disabling a service necessary to the operation of the device. In one example, home automation devices sold with the promise of a ""Lifetime Subscription"" were rendered useless after Nest Labs acquired Revolv and made the decision to shut down the central servers the Revolv devices had used to operate.[266] As Nest is a company owned by Alphabet (Google's parent company), the EFF argues this sets a ""terrible precedent for a company with ambitions to sell self-driving cars, medical devices, and other high-end gadgets that may be essential to a person's livelihood or physical safety.""[267]"
,,,"Owners should be free to point their devices to a different server or collaborate on improved software. But such action violates the United States DMCA section 1201, which only has an exemption for ""local use"". This forces tinkerers who want to keep using their own equipment into a legal grey area. EFF thinks buyers should refuse electronics and software that prioritize the manufacturer's wishes above their own.[267]"
,,,"Examples of post-sale manipulations include Google Nest Revolv, disabled privacy settings on Android, Sony disabling Linux on PlayStation 3, enforced EULA on Wii U.[267]"
,,,"Kevin Lonergan at Information Age, a business technology magazine, has referred to the terms surrounding the IoT as a ""terminology zoo"".[268] The lack of clear terminology is not ""useful from a practical point of view"" and a ""source of confusion for the end user"".[268] A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics.[268] According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we know them today existed, and there is a long list of terms with varying degrees of overlap and technological convergence: Internet of things, Internet of everything (IoE), Internet of goods (supply chain), industrial Internet, pervasive computing, pervasive sensing, ubiquitous computing, cyber-physical systems (CPS), wireless sensor networks (WSN), smart objects, digital twin, cyberobjects or avatars,[139] cooperating objects, machine to machine (M2M), ambient intelligence (AmI), Operational technology (OT), and information technology (IT).[268] Regarding IIoT, an industrial sub-field of IoT, the Industrial Internet Consortium's Vocabulary Task Group has created a ""common and reusable vocabulary of terms""[269] to ensure ""consistent terminology""[269][270] across publications issued by the Industrial Internet Consortium. IoT One has created an IoT Terms Database including a New Term Alert[271] to be notified when a new term is published. As of March 2020[update], this database aggregates 807 IoT-related terms, while keeping material ""transparent and comprehensive.""[272][273]"
,,,"Despite a shared belief in the potential of the IoT, industry leaders and consumers are facing barriers to adopt IoT technology more widely. Mike Farley argued in Forbes that while IoT solutions appeal to early adopters, they either lack interoperability or a clear use case for end-users.[274] A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle ""to pinpoint exactly where the value of IoT lies for them"".[275]"
,,,"As for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the “things” around the user can cooperate to provide better services that fulfill personal preference.[276] When the collected information which describes a user in detail travels through multiple hops in a network, due to a diverse integration of services, devices and network, the information stored on a device is vulnerable to privacy violation by compromising nodes existing in an IoT network.[277]"
,,,"For example, on 21 October 2016, a multiple distributed denial of service (DDoS) attacks systems operated by domain name system provider Dyn, which caused the inaccessibility of several websites, such as GitHub, Twitter, and others. This attack is executed through a botnet consisting of a large number of IoT devices including IP cameras, gateways, and even baby monitors.[278]"
,,,Fundamentally there are 4 security objectives that the IoT system requires: (1) data confidentiality: unauthorized parties cannot have access to the transmitted and stored data; (2) data integrity: intentional and unintentional corruption of transmitted and stored data must be detected; (3) non-repudiation: the sender cannot deny having sent a given message; (4) data availability: the transmitted and stored data should be available to authorized parties even with the denial-of-service (DOS) attacks.[279]
,,,"Information privacy regulations also require organizations to practice ""reasonable security"". California's SB-327 Information privacy: connected devices ""would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure, as specified.""[280] As each organization's environment is unique, it can prove challenging to demonstrate what ""reasonable security"" is and what potential risks could be involved for the business. Oregon's HB 2395 also ""requires [a] person that manufactures, sells or offers to sell connected device] manufacturer to equip connected device with reasonable security features that protect connected device and information that connected device collects, contains, stores or transmits] stores from access, destruction, modification, use or disclosure that consumer does not authorize.""[281]"
,,,"According to antivirus provider Kaspersky, there were 639 million data breaches of IoT devices in 2020 and 1.5 billion breaches in the first six months of 2021.[197]"
,,,"A study issued by Ericsson regarding the adoption of Internet of things among Danish companies identified a ""clash between IoT and companies' traditional governance structures, as IoT still presents both uncertainties and a lack of historical precedence.""[275] Among the respondents interviewed, 60 percent stated that they ""do not believe they have the organizational capabilities, and three of four do not believe they have the processes needed, to capture the IoT opportunity.""[275] This has led to a need to understand organizational culture in order to facilitate organizational design processes and to test new innovation management practices. A lack of digital leadership in the age of digital transformation has also stifled innovation and IoT adoption to a degree that many companies, in the face of uncertainty, ""were waiting for the market dynamics to play out"",[275] or further action in regards to IoT ""was pending competitor moves, customer pull, or regulatory requirements.""[275] Some of these companies risk being ""kodaked"" – ""Kodak was a market leader until digital disruption eclipsed film photography with digital photos"" – failing to ""see the disruptive forces affecting their industry""[282] and ""to truly embrace the new business models the disruptive change opens up.""[282] Scott Anthony has written in Harvard Business Review that Kodak ""created a digital camera, invested in the technology, and even understood that photos would be shared online""[282] but ultimately failed to realize that ""online photo sharing was the new business, not just a way to expand the printing business.""[282]"
,,,"According to 2018 study, 70–75% of IoT deployments were stuck in the pilot or prototype stage, unable to reach scale due in part to a lack of business planning.[283][page needed][284]"
,,,"Even though scientists, engineers, and managers across the world are continuously working to create and exploit the benefits of IoT products, there are some flaws in the governance, management and implementation of such projects. Despite tremendous forward momentum in the field of information and other underlying technologies, IoT still remains a complex area and the problem of how IoT projects are managed still needs to be addressed. IoT projects must be run differently than simple and traditional IT, manufacturing or construction projects. Because IoT projects have longer project timelines, a lack of skilled resources and several security/legal issues, there is a need for new and specifically designed project processes. The following management techniques should improve the success rate of IoT projects:[285]"
,,,"Cloud computing[1] is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user.[2] Large clouds often have functions distributed over multiple locations, each location being a data center. Cloud computing relies on sharing of resources to achieve coherence and typically using a ""pay-as-you-go"" model which can help in reducing capital expenses but may also lead to unexpected operating expenses for unaware users.[3]"
,,,"Advocates of public and hybrid clouds claim that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand,[4][5][6] providing burst computing capability: high computing power at certain periods of peak demand.[7]"
,,,"The term cloud was used to refer to platforms for distributed computing as early as 1993, when Apple spin-off General Magic and AT&T used it in describing their (paired) Telescript and PersonaLink technologies.[8]  In Wired's April 1994 feature ""Bill and Andy's Excellent Adventure II"", Andy Hertzfeld commented on Telescript, General Magic's distributed programming language:"
,,,"""The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service. No one had conceived that before. The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties.""[9]"
,,,"During the 1960s, the initial concepts of time-sharing became popularized via RJE (Remote Job Entry);[10] this terminology was mostly associated with large vendors such as IBM and DEC.  Full-time-sharing solutions were available by the early 1970s on such platforms as Multics (on GE hardware), Cambridge CTSS, and the earliest UNIX ports (on DEC hardware). Yet, the ""data center"" model where users submitted jobs to operators to run on IBM's mainframes was overwhelmingly predominant."
,,,"In the 1990s, telecommunications companies, who previously offered primarily dedicated point-to-point data circuits, began offering virtual private network (VPN) services with comparable quality of service, but at a lower cost. By switching traffic as they saw fit to balance server use, they could use overall network bandwidth more effectively.[citation needed]  They began to use the cloud symbol to denote the demarcation point between what the provider was responsible for and what users were responsible for. Cloud computing extended this boundary to cover all servers as well as the network infrastructure.[11] As computers became more diffused, scientists and technologists explored ways to make large-scale computing power available to more users through time-sharing.[citation needed] They experimented with algorithms to optimize the infrastructure, platform, and applications to prioritize CPUs and increase efficiency for end users.[12]"
,,,"The use of the cloud metaphor for virtualized services dates at least to General Magic in 1994, where it was used to describe the universe of ""places"" that mobile agents in the Telescript environment could go. As described by Andy Hertzfeld:"
,,,"""The beauty of Telescript,"" says Andy, ""is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service.""[13]"
,,,"The use of the cloud metaphor is credited to General Magic communications employee David Hoffman, based on long-standing use in networking and telecom. In addition to use by General Magic itself, it was also used in promoting AT&T's associated PersonaLink Services.[14]"
,,,"In July 2002, Amazon created subsidiary Amazon Web Services, with the goal to ""enable developers to build innovative and entrepreneurial applications on their own."" In March 2006 Amazon introduced its Simple Storage Service (S3), followed by Elastic Compute Cloud (EC2) in August of the same year.[15][16] These products pioneered the usage of server virtualization to deliver IaaS at a cheaper and on-demand pricing basis."
,,,"In April 2008, Google released the beta version of Google App Engine.[17] The App Engine was a PaaS (one of the first of its kind) which provided fully maintained infrastructure and a deployment platform for users to create web applications using common languages/technologies such as Python, Node.js and PHP. The goal was to eliminate the need for some administrative tasks typical of an IaaS model, while creating a platform where users could easily deploy such applications and scale them to demand.[18]"
,,,"In early 2008, NASA's Nebula,[19] enhanced in the RESERVOIR European Commission-funded project, became the first open-source software for deploying private and hybrid clouds, and for the federation of clouds.[20]"
,,,"By mid-2008, Gartner saw an opportunity for cloud computing ""to shape the relationship among consumers of IT services, those who use IT services and those who sell them""[21] and observed that ""organizations are switching from company-owned hardware and software assets to per-use service-based models"" so that the ""projected shift to computing ... will result in dramatic growth in IT products in some areas and significant reductions in other areas.""[22]"
,,,"In 2008, the U.S. National Science Foundation began the Cluster Exploratory program to fund academic research using Google-IBM cluster technology to analyze massive amounts of data.[23]"
,,,"In 2009, the government of France announced Project Andromède to create a ""sovereign cloud"" or national cloud computing, with the government to spend €285 million.[24][25]  The initiative failed badly and Cloudwatt was shut down on 1 February 2020.[26][27]"
,,,"In February 2010, Microsoft released Microsoft Azure, which was announced in October 2008.[28]"
,,,"In July 2010, Rackspace Hosting and NASA jointly launched an open-source cloud-software initiative known as OpenStack. The OpenStack project intended to help organizations offering cloud-computing services running on standard hardware. The early code came from NASA's Nebula platform as well as from Rackspace's Cloud Files platform. As an open-source offering and along with other open-source solutions such as CloudStack, Ganeti, and OpenNebula, it has attracted attention by several key communities. Several studies aim at comparing these open source offerings based on a set of criteria.[29][30][31][32][33][34][35]"
,,,"On March 1, 2011, IBM announced the IBM SmartCloud framework to support Smarter Planet.[36] Among the various components of the Smarter Computing foundation, cloud computing is a critical part. On June 7, 2012, Oracle announced the Oracle Cloud.[37] This cloud offering is poised to be the first to provide users with access to an integrated set of IT solutions, including the Applications (SaaS), Platform (PaaS), and Infrastructure (IaaS) layers.[38][39][40]"
,,,"In May 2012, Google Compute Engine was released in preview, before being rolled out into General Availability in December 2013.[41]"
,,,"In 2019, Linux was the most common OS used on Microsoft Azure.[42] In December 2019, Amazon announced AWS Outposts, which is a fully managed service that extends AWS infrastructure, AWS services, APIs, and tools to virtually any customer datacenter, co-location space, or on-premises facility for a truly consistent hybrid experience[43]"
,,,"The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles.[44] The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more ""virtual"" devices, each of which can be easily used and managed to perform computing tasks. With operating system–level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.[44]"
,,,Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.[44]
,,,Cloud computing shares characteristics with:
,,,Cloud computing exhibits the following key characteristics:
,,,"The National Institute of Standards and Technology's definition of cloud computing identifies ""five essential characteristics"":"
,,,"On-demand self-service. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider."
,,,"Broad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations)."
,,,"Resource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. "
,,,"Rapid elasticity. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time."
,,,"Measured service. Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service."
,,,"Though service-oriented architecture advocates ""Everything as a service"" (with the acronyms EaaS or XaaS,[67] or simply aas), cloud-computing providers offer their ""services"" according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).[66] These models offer increasing abstraction; they are thus often portrayed as layers in a stack: infrastructure-, platform- and software-as-a-service, but these need not be related. For example, one can provide SaaS implemented on physical machines (bare metal), without using underlying PaaS or IaaS layers, and conversely one can run a program on IaaS and access it directly, without wrapping it as SaaS."
,,,"""Infrastructure as a service"" (IaaS) refers to online services that provide high-level APIs used to abstract various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A hypervisor runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. Containerisation offers higher performance than virtualization because there is no hypervisor overhead.  IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.[68]"
,,,"The NIST's definition of cloud computing describes IaaS as ""where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).""[66]"
,,,"IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the amount of resources allocated and consumed.[citation needed]"
,,,The NIST's definition of cloud computing defines Platform as a Service as:[66]
,,,"The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment."
,,,"PaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including operating system, programming-language execution environment, database, and web server. Application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers. With some PaaS, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually.[69][need quotation to verify]"
,,,"Some integration and data management providers also use specialized applications of PaaS as delivery models for data. Examples include iPaaS (Integration Platform as a Service) and dPaaS (Data Platform as a Service). iPaaS enables customers to develop, execute and govern integration flows.[70] Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.[71] dPaaS delivers integration—and data-management—products as a fully managed service.[72] Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of programs by building data applications for the customer. dPaaS users access data through data-visualization tools.[73]"
,,,The NIST's definition of cloud computing defines Software as a Service as:[66]
,,,"The capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings."
,,,"In the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as ""on-demand software"" and is usually priced on a pay-per-use basis or using a subscription fee.[74] In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability—which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.[75] Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization."
,,,"The pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[76] so prices become scalable and adjustable if users are added or removed at any point. It may also be free.[77] Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result,[citation needed] there could be unauthorized access to the data.[78] Examples of applications offered as SaaS are games and productivity software like Google Docs and Word Online. SaaS applications may be integrated with cloud storage or File hosting services, which is the case with Google Docs being integrated with Google Drive and Word Online being integrated with Onedrive.[citation needed]"
,,,"In the mobile ""backend"" as a service (m) model, also known as backend as a service (BaaS), web app and mobile app developers are provided with a way to link their applications to cloud storage and cloud computing services with application programming interfaces (APIs) exposed to their applications and custom software development kits (SDKs). Services include user management, push notifications, integration with social networking services[79] and more. This is a relatively recent model in cloud computing,[80] with most BaaS startups dating from 2011 or later[81][82][83] but trends indicate that these services are gaining significant mainstream traction with enterprise consumers.[84]"
,,,"Serverless computing is a cloud computing code execution model in which the cloud provider fully manages starting and stopping virtual machines as necessary to serve requests, and requests are billed by an abstract measure of the resources required to satisfy the request, rather than per virtual machine, per hour.[85] Despite the name, it does not actually involve running code without servers.[85] Serverless computing is so named because the business or person that owns the system does not have to purchase, rent or provide servers or virtual machines for the back-end code to run on."
,,,"Function as a service (FaaS) is a service-hosted remote procedure call that leverages serverless computing to enable the deployment of individual functions in the cloud that run in response to events.[86] FaaS is considered by some to come under the umbrella of serverless computing, while some others use the terms interchangeably.[87]"
,,,"Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally.[66] Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers[88] are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users ""still have to buy, build, and manage them"" and thus do not benefit from less hands-on management,[89] essentially ""[lacking] the economic model that makes cloud computing such an intriguing concept"".[90][91]"
,,,"Cloud services are considered ""public"" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge.[92] Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications.[49][93]"
,,,"Several factors like the functionality of the solutions, cost, integrational and organizational aspects as well as safety & security are influencing the decision of enterprises and organizations to choose a public cloud or on-premises solution.[94]"
,,,"Hybrid cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources,[95][96] that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.[66] Gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.[97] A hybrid cloud service crosses isolation and provider boundaries so that it can't be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service."
,,,"Varied use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.[98] This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.[99]"
,,,"Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.[100] This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.[66] Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and ""bursts"" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed.[101] Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.[102] The specialized model of hybrid cloud, which is built atop heterogeneous hardware, is called ""Cross-platform Hybrid Cloud"". A cross-platform hybrid cloud is usually powered by different CPU architectures, for example, x86-64 and ARM, underneath. Users can transparently deploy and scale applications without knowledge of the cloud's hardware diversity.[103] This kind of cloud emerges from the rise of ARM-based system-on-chip for server-class computing."
,,,Hybrid cloud infrastructure essentially serves to eliminate limitations inherent to the multi-access relay characteristics of private cloud networking. The advantages include enhanced runtime flexibility and adaptive memory processing unique to virtualized interface models.[104]
,,,"Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether managed internally or by a third-party, and either hosted internally or externally. The costs are spread over fewer users than a public cloud (but more than a private cloud), so only some of the cost savings potential of cloud computing are realized.[66]"
,,,"A cloud computing platform can be assembled from a distributed set of machines in different locations, connected to a single network or hub service. It is possible to distinguish between two types of distributed clouds: public-resource computing and volunteer cloud."
,,,"Multicloud is the use of multiple cloud computing services in a single heterogeneous architecture to reduce reliance on single vendors, increase flexibility through choice, mitigate against disasters, etc. It differs from hybrid cloud in that it refers to multiple cloud services, rather than multiple deployment modes (public, private, legacy).[106][107][108]"
,,,Poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more that could be done with a single provider.[109]
,,,"The issues of transferring large amounts of data to the cloud as well as data security once the data is in the cloud initially hampered adoption of cloud for big data, but now that much data originates in the cloud and with the advent of bare-metal servers, the cloud has become[110] a solution for use cases including business analytics and geospatial analysis.[111]"
,,,"HPC cloud refers to the use of cloud computing services and infrastructure to execute high-performance computing (HPC) applications.[112] These applications consume considerable amount of computing power and memory and are traditionally executed on clusters of computers. In 2016 a handful of companies, including R-HPC, Amazon Web Services, Univa, Silicon Graphics International, Sabalcore, Gomput, and Penguin Computing offered a high performance computing cloud. The Penguin On Demand (POD) cloud was one of the first non-virtualized remote HPC services offered on a pay-as-you-go basis.[113][114] Penguin Computing launched its HPC cloud in 2016 as alternative to Amazon's EC2 Elastic Compute Cloud, which uses virtualized computing nodes.[115][116]"
,,,"Cloud architecture,[117] the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others."
,,,"Cloud engineering is the application of engineering disciplines of cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization and governance in conceiving, developing, operating and maintaining cloud computing systems. It is a multidisciplinary method encompassing contributions from diverse areas such as systems, software, web, performance, information technology engineering, security, platform, risk, and quality engineering."
,,,"Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information.[118] Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored.[118] Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.[119][118] Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity.[120] The systems work by creating and describing identities, recording activities, and getting rid of unused identities."
,,,"According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. ""There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into"". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called ""hyperjacking"". Some examples of this include the Dropbox security breach, and iCloud 2014 leak.[121] Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[121]"
,,,"There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.[122] Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.[123] Some small businesses that don't have expertise in IT security could find that it's more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click ""Accept"" without reading). This is important now that cloud computing is becoming popular and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Now). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[124]"
,,,"According to Bruce Schneier, ""The downside is that you will have limited customization options. Cloud computing is cheaper because of economics of scale, and—like any outsourced task—you tend to get what you want. A restaurant with a limited menu is cheaper than a personal chef who can cook anything you want. Fewer options at a much cheaper price: it's a feature, not a bug."" He also suggests that ""the cloud provider might not meet your legal needs"" and that businesses need to weigh the benefits of cloud computing against the risks.[125]In cloud computing, the control of the back end infrastructure is limited to the cloud vendor only. Cloud providers often decide on the management policies, which moderates what the cloud users are able to do with their deployment.[126] Cloud users are also limited to the control and management of their applications, data and services.[127] This includes data caps, which are placed on cloud users by the cloud vendor allocating a certain amount of bandwidth for each customer and are often shared among other cloud users.[127]"
,,,"Privacy and confidentiality are big concerns in some activities. For instance, sworn translators working under the stipulations of an NDA, might face problems regarding sensitive data that are not encrypted.[128] Due to the use of the internet, confidential information such as employee data and user data can be easily available to third-party organisations and people in Cloud Computing.[129]"
,,,"Cloud computing has some limitations for smaller business operations, particularly regarding security and downtime. Technical outages are inevitable and occur sometimes when cloud service providers (CSPs) become overwhelmed in the process of serving their clients. This may result in temporary business suspension. Since this technology's systems rely on the Internet, an individual cannot access their applications, server, or data from the cloud during an outage.[130]"
,,,"Cloud computing is still a subject of research.[131] A driving factor in the evolution of cloud computing has been chief technology officers seeking to minimize risk of internal outages and mitigate the complexity of housing network and computing hardware in-house.[132] They are also looking to share information to workers located in diverse areas in near and real-time, to enable teams to work seamlessly, no matter where they are located. Since the global pandemic of 2020, cloud technology jumped ahead in popularity due to the level of security of data and the flexibility of working options for all employees, notably remote workers. For example, Zoom grew over 160% in 2020 alone.[133]"
,,,The issue of carrying out investigations where the cloud storage devices cannot be physically accessed has generated a number of changes to the way that digital evidence is located and collected.[134] New process models have been developed to formalize collection.[135]
,,,In some scenarios existing digital forensics tools can be employed to access cloud storage as networked drives (although this is a slow process generating a large amount of internet traffic).[citation needed]
,,,An alternative approach is to deploy a tool that processes in the cloud itself.[136]
,,,"For organizations using Office 365 with an 'E5' subscription, there is the option to use Microsoft's built-in e-discovery resources, although these do not provide all the functionality that is typically required for a forensic process.[137]"
,,,Amazon most often refers to:
,,,Amazon or Amazone may also refer to:
,,,"A laptop, laptop computer, or notebook computer is a small, portable personal computer (PC) with a screen and alphanumeric keyboard. Laptops typically have a clam shell form factor with the screen mounted on the inside of the upper lid and the keyboard on the inside of the lower lid, although 2-in-1 PCs with a detachable keyboard are often marketed as laptops or as having a laptop mode. Laptops are folded shut for transportation, and thus are suitable for mobile use.[1] Its name comes from lap, as it was deemed practical to be placed on a person's lap when being used. Today, laptops are used in a variety of settings, such as at work, in education, for playing games, web browsing, for personal multimedia, and for general home computer use."
,,,"As of 2021, in American English, the terms laptop computer and notebook computer are used interchangeably;[2] in other dialects of English, one or the other may be preferred. Although the terms notebook computers or notebooks originally referred to a specific size of laptop (originally smaller and lighter than mainstream laptops of the time),[3] the terms have come to mean the same thing and notebook no longer refers to any specific size."
,,,"Laptops combine all the input/output components and capabilities of a desktop computer, including the display screen, small speakers, a keyboard, data storage device, sometimes an optical disc drive, pointing devices (such as a touch pad or pointing stick), with an operating system, a processor and memory into a single unit. Most modern laptops feature integrated webcams and built-in microphones, while many also have touchscreens. Laptops can be powered either from an internal battery or by an external power supply from an AC adapter. Hardware specifications, such as the processor speed and memory capacity, significantly vary between different types, models and price points."
,,,"Design elements, form factor and construction can also vary significantly between models depending on the intended use. Examples of specialized models of laptops include rugged notebooks for use in construction or military applications, as well as low production cost laptops such as those from the One Laptop per Child (OLPC) organization, which incorporate features like solar charging and semi-flexible components not found on most laptop computers. Portable computers, which later developed into modern laptops, were originally considered to be a small niche market, mostly for specialized field applications, such as in the military, for accountants, or traveling sales representatives. As portable computers evolved into modern laptops, they became widely used for a variety of purposes.[4]"
,,,"As the personal computer (PC) became feasible in 1971, the idea of a portable personal computer soon followed. A ""personal, portable information manipulator"" was imagined by Alan Kay at Xerox PARC in 1968,[5] and described in his 1972 paper as the ""Dynabook"".[6] The IBM Special Computer APL Machine Portable (SCAMP) was demonstrated in 1973.[7] This prototype was based on the IBM PALM processor.[8] The IBM 5100, the first commercially available portable computer, appeared in September 1975, and was based on the SCAMP prototype.[9]"
,,,"As 8-bit CPU machines became widely accepted, the number of portables increased rapidly. The first ""laptop-sized notebook computer"" was the Epson HX-20,[10][11] invented (patented) by Suwa Seikosha's Yukio Yokozawa in July 1980,[12] introduced at the COMDEX computer show in Las Vegas by Japanese company Seiko Epson in 1981,[13][11] and released in July 1982.[11][14] It had an LCD screen, a rechargeable battery, and a calculator-size printer, in a 1.6 kg (3.5 lb) chassis, the size of an A4 notebook.[11] It was described as a ""laptop"" and ""notebook"" computer in its patent.[12]"
,,,"The portable micro computer Portal of the French company R2E Micral CCMC officially appeared in September 1980 at the Sicob show in Paris. It was a portable microcomputer designed and marketed by the studies and developments department of R2E Micral at the request of the company CCMC specializing in payroll and accounting. It was based on an Intel 8085 processor, 8-bit, clocked at 2 MHz. It was equipped with a central 64 KB RAM, a keyboard with 58 alphanumeric keys and 11 numeric keys (separate blocks), a 32-character screen, a floppy disk: capacity = 140,000 characters, of a thermal printer: speed = 28 characters / second, an asynchronous channel, asynchronous channel, a 220 V power supply. It weighed 12 kg and its dimensions were 45 × 45 × 15 cm. It provided total mobility. Its operating system was aptly named Prologue."
,,,"The Osborne 1, released in 1981, was a luggable computer that used the Zilog Z80 and weighed 24.5 pounds (11.1 kg).[15] It had no battery, a 5 in (13 cm) cathode-ray tube (CRT) screen, and dual 5.25 in (13.3 cm) single-density floppy drives. Both Tandy/RadioShack and Hewlett Packard (HP) also produced portable computers of varying designs during this period.[16][17] The first laptops using the flip form factor appeared in the early 1980s. The Dulmont Magnum was released in Australia in 1981–82, but was not marketed internationally until 1984–85. The US$8,150 (equivalent to $22,880 in 2021) GRiD Compass 1101, released in 1982, was used at NASA and by the military, among others. The Sharp PC-5000,[18] Ampere[19] and Gavilan SC released in 1983. The Gavilan SC was described as a ""laptop"" by its manufacturer,[20] while the Ampere had a modern clamshell design.[19][21] The Toshiba T1100 won acceptance not only among PC experts but the mass market as a way to have PC portability.[22]"
,,,"From 1983 onward, several new input techniques were developed and included in laptops, including the touch pad (Gavilan SC, 1983), the pointing stick (IBM ThinkPad 700, 1992), and handwriting recognition (Linus Write-Top,[23] 1987). Some CPUs, such as the 1990 Intel i386SL, were designed to use minimum power to increase battery life of portable computers and were supported by dynamic power management features such as Intel SpeedStep and AMD PowerNow! in some designs."
,,,"Displays reached 640x480 (VGA) resolution by 1988 (Compaq SLT/286), and color screens started becoming a common upgrade in 1991,[24] with increases in resolution and screen size occurring frequently until the introduction of 17"" screen laptops in 2003. Hard drives started to be used in portables, encouraged by the introduction of 3.5"" drives in the late 1980s, and became common in laptops starting with the introduction of 2.5"" and smaller drives around 1990; capacities have typically lagged behind physically larger desktop drives."
,,,"Common resolutions of laptop webcams are 720p (HD), and in lower-end laptops 480p.[25] The earliest known laptops with 1080p (Full HD) webcams like the Samsung 700G7C were released in the early 2010s.[26]"
,,,"Optical disc drives became common in full-size laptops around 1997; this initially consisted of CD-ROM drives, which were supplanted by CD-R, DVD, and Blu-ray drives with writing capability over time. Starting around 2011, the trend shifted against internal optical drives, and as of 2021, they have largely disappeared; they are still readily available as external peripherals."
,,,"While the terms laptop and notebook are used interchangeably today, there is some question as to the original etymology and specificity of either term. The term laptop appears to have been coined in the early 1980s to describe a mobile computer which could be used on one's lap and to distinguish these devices from earlier and much heavier portable computers (informally called ""luggables""). The term notebook appears to have gained currency somewhat later as manufacturers started producing even smaller portable devices, further reducing their weight and size and incorporating a display roughly the size of A4 paper;[3] these were marketed as notebooks to distinguish them from bulkier mainstream or desktop replacement laptops."
,,,"Since the introduction of portable computers during the late 1970s, their form has changed significantly, spawning a variety of visually and technologically differing subclasses. Except where there is a distinct legal trademark around a term (notably, Ultrabook), there are rarely hard distinctions between these classes and their usage has varied over time and between different sources. Since the late 2010s, the use of more specific terms has become less common, with sizes distinguished largely by the size of the screen."
,,,"There were in the past a number of marketing categories for smaller and larger laptop computers; these included ""subnotebook"" models, low cost ""netbooks"", and ""ultra-mobile PCs"" where the size class overlapped with devices like smartphone and handheld tablets, and ""Desktop replacement"" laptops for machines notably larger and heavier than typical to operate more powerful processors or graphics hardware.[27] All of these terms have fallen out of favor as the size of mainstream laptops has gone down and their capabilities have gone up; except for niche models, laptop sizes tend to be distinguished by the size of the screen, and for more powerful models, by any specialized purpose the machine is intended for, such as a ""gaming laptop"" or a ""mobile workstation"" for professional use."
,,,"The latest trend of technological convergence in the portable computer industry spawned a broad range of devices, which combined features of several previously separate device types. The hybrids, convertibles, and 2-in-1s emerged as crossover devices, which share traits of both tablets and laptops. All such devices have a touchscreen display designed to allow users to work in a tablet mode, using either multi-touch gestures or a stylus/digital pen."
,,,"Convertibles are devices with the ability to conceal a hardware keyboard. Keyboards on such devices can be flipped, rotated, or slid behind the back of the chassis, thus transforming from a laptop into a tablet. Hybrids have a keyboard detachment mechanism, and due to this feature, all critical components are situated in the part with the display. 2-in-1s can have a hybrid or a convertible form, often dubbed 2-in-1 detachable and 2-in-1 convertibles respectively, but are distinguished by the ability to run a desktop OS, such as Windows 10. 2-in-1s are often marketed as laptop replacement tablets.[28]"
,,,"2-in-1s are often very thin, around 10 millimetres (0.39 in), and light devices with a long battery life. 2-in-1s are distinguished from mainstream tablets as they feature an x86-architecture CPU (typically a low- or ultra-low-voltage model), such as the Intel Core i5, run a full-featured desktop OS like Windows 10, and have a number of typical laptop I/O ports, such as USB 3 and Mini DisplayPort."
,,,"2-in-1s are designed to be used not only as a media consumption device but also as valid desktop or laptop replacements, due to their ability to run desktop applications, such as Adobe Photoshop. It is possible to connect multiple peripheral devices, such as a mouse, keyboard, and several external displays to a modern 2-in-1."
,,,"Microsoft Surface Pro-series devices and Surface Book are examples of modern 2-in-1 detachable, whereas Lenovo Yoga-series computers are a variant of 2-in-1 convertibles. While the older Surface RT and Surface 2 have the same chassis design as the Surface Pro, their use of ARM processors and Windows RT do not classify them as 2-in-1s, but as hybrid tablets.[29] Similarly, a number of hybrid laptops run a mobile operating system, such as Android. These include Asus's Transformer Pad devices, examples of hybrids with a detachable keyboard design, which do not fall in the category of 2-in-1s."
,,,"A rugged laptop is designed to reliably operate in harsh usage conditions such as strong vibrations, extreme temperatures, and wet or dusty environments. Rugged laptops are bulkier, heavier, and much more expensive than regular laptops,[30] and thus are seldom seen in regular consumer use."
,,,"The basic components of laptops function identically to their desktop counterparts. Traditionally they were miniaturized and adapted to mobile use, although desktop systems increasingly use the same smaller, lower-power parts which were originally developed for mobile use. The design restrictions on power, size, and cooling of laptops limit the maximum performance of laptop parts compared to that of desktop components, although that difference has increasingly narrowed.[31]"
,,,"In general, laptop components are not intended to be replaceable or upgradable by the end-user, except for components that can be detached; in the past, batteries and optical drives were commonly exchangeable. This restriction is one of the major differences between laptops and desktop computers, because the large ""tower"" cases used in desktop computers are designed so that new motherboards, hard disks, sound cards, RAM, and other components can be added. Memory and storage can often be upgraded with some disassembly, but with the most compact laptops, there may be no upgradeable components at all.[32]"
,,,"Intel, Asus, Compal, Quanta, and some other laptop manufacturers have created the Common Building Block standard for laptop parts to address some of the inefficiencies caused by the lack of standards and inability to upgrade components.[33]"
,,,The following sections summarizes the differences and distinguishing features of laptop components in comparison to desktop personal computer parts.[34]
,,,"Internally, a display is usually an LCD panel, although occasionally OLEDs are used. These interface to the laptop using the LVDS or embedded DisplayPort protocol, while externally, it can be a glossy screen or a matte (anti-glare) screen. As of 2021, mainstream consumer laptops tend to come with either 13"" or 15""-16"" screens; 14"" models are more popular among business machines. Larger and smaller models are available, but less common – there is no clear dividing line in minimum or maximum size. Machines small enough to be handheld (screens in the 6–8"" range) can be marketed either as very small laptops or ""handheld PCs,"" while the distinction between the largest laptops and ""All-in-One"" desktops is whether they fold for travel."
,,,"In the past, there was a broader range of marketing terms (both formal and informal) to distinguish between different sizes of laptops. These included Netbooks, subnotebooks, Ultra-mobile PC, and Desktop replacement computers; these are sometimes still used informally, although they are essentially dead in terms of manufacturer marketing."
,,,"Having a higher resolution display allows more items to fit onscreen at a time, improving the user's ability to multitask, although at the higher resolutions on smaller screens, the resolution may only serve to display sharper graphics and text rather than increasing the usable area. Since the introduction of the MacBook Pro with Retina display in 2012, there have been an increase in the availability of ""HiDPI"" (or high Pixel density) displays; as of 2021, this is generally considered to be anything higher than 1920 pixels wide. This has increasingly converged around 4K (3840-pixel-wide) resolutions."
,,,"External displays can be connected to most laptops, and models with a Mini DisplayPort can handle up to three.[35]"
,,,The earliest laptops known to feature a display with doubled 120 Hz of refresh rate and active shutter 3D system were released in 2011 by Dell (M17x) and Samsung (700G7A).[36][37]
,,,"A laptop's central processing unit (CPU) has advanced power-saving features and produces less heat than one intended purely for desktop use. Mainstream laptop CPUs made after 2018 have four processor cores, although some inexpensive models still have 2-core CPUs, and 6-core and 8-core models are also available."
,,,"For the low price and mainstream performance, there is no longer a significant performance difference between laptop and desktop CPUs, but at the high end, the fastest desktop CPUs still substantially outperform the fastest laptop processors, at the expense of massively higher power consumption and heat generation; the fastest laptop processors top out at 56 watts of heat, while the fastest desktop processors top out at 150 watts."
,,,"There has been a wide range of CPUs designed for laptops available from both Intel, AMD, and other manufacturers. On non-x86 architectures, Motorola and IBM produced the chips for the former PowerPC-based Apple laptops (iBook and PowerBook). Between around 2000 to 2014, most full-size laptops had socketed, replaceable CPUs; on thinner models, the CPU was soldered on the motherboard and was not replaceable or upgradable without replacing the motherboard. Since 2015, Intel has not offered new laptop CPU models with pins to be interchangeable, preferring ball grid array chip packages which have to be soldered;[38]and as of 2021, only a few rare models using desktop parts."
,,,"In the past, some laptops have used a desktop processor instead of the laptop version and have had high-performance gains at the cost of greater weight, heat, and limited battery life; this is not unknown as of 2021, but since around 2010, the practice has been restricted to small-volume gaming models. Laptop CPUs are rarely able to be overclocked; most use locked processors. Even on gaming models where unlocked processors are available, the cooling system in most laptops is often very close to its limits and there is rarely headroom for an overclocking–related operating temperature increase."
,,,"On most laptops, a graphical processing unit (GPU) is integrated into the CPU to conserve power and space. This was introduced by Intel with the Core i-series of mobile processors in 2010, and similar accelerated processing unit (APU) processors by AMD later that year."
,,,"Before that, lower-end machines tended to use graphics processors integrated into the system chipset, while higher-end machines had a separate graphics processor. In the past, laptops lacking a separate graphics processor were limited in their utility for gaming and professional applications involving 3D graphics, but the capabilities of CPU-integrated graphics have converged with the low-end of dedicated graphics processors since the mid-2010s."
,,,"Higher-end laptops intended for gaming or professional 3D work still come with dedicated and in some cases even dual, graphics processors on the motherboard or as an internal expansion card. Since 2011, these almost always involve switchable graphics so that when there is no demand for the higher performance dedicated graphics processor, the more power-efficient integrated graphics processor will be used. Nvidia Optimus and AMD Hybrid Graphics are examples of this sort of system of switchable graphics."
,,,"Since around the year 2000, most laptops have used SO-DIMM RAM,[34] although, as of 2021, an increasing number of models use memory soldered to the motherboard. Before 2000, most laptops used proprietary memory modules if their memory was upgradable."
,,,"In the early 2010s, high end laptops such as the 2011 Samsung 700G7A have passed the 10 GB RAM barrier, featuring 16 GB of RAM.[39]"
,,,"When upgradeable, memory slots are sometimes accessible from the bottom of the laptop for ease of upgrading; in other cases, accessing them requires significant disassembly. Most laptops have two memory slots, although some will have only one, either for cost savings or because some amount of memory is soldered. Some high-end models have four slots; these are usually mobile engineering workstations, although a few high-end models intended for gaming do as well."
,,,"As of 2021, 8 GB RAM is most common, with lower-end models occasionally having 4GB. Higher-end laptops may come with 16 GB of RAM or more."
,,,"The earliest laptops most often used floppy disk for storage, although a few used either RAM disk or tape, by the late 1980s hard disk drives had become the standard form of storage."
,,,"Between 1990 and 2009, almost all laptops typically had a hard disk drive (HDD) for storage; since then, solid-state drives (SSD) have gradually come to supplant hard drives in all but some inexpensive consumer models. Solid-state drives are faster and more power-efficient, as well as eliminating the hazard of drive and data corruption caused by a laptop's physical impacts, as they use no mechanical parts such as a rotational platter.[40] In many cases, they are more compact as well. Initially, in the late 2000s, SSDs were substantially more expensive than HDDs, but as of 2021 prices on smaller capacity (under 1 terabyte) drives have converged; larger capacity drives remain more expensive than comparable-sized HDDs."
,,,"Since around 1990, where a hard drive is present it will typically be a 2.5-inch drive; some very compact laptops support even smaller 1.8-inch HDDs, and a very small number used 1"" Microdrives. Some SSDs are built to match the size/shape of a laptop hard drive, but increasingly they have been replaced with smaller mSATA or M.2 cards. SSDs using the newer and much faster NVM Express standard for connecting are only available as cards."
,,,"As of 2021, many laptops no longer contain space for a 2.5"" drive, accepting only M.2 cards; a few of the smallest have storage soldered to the motherboard. For those that can, they can typically contain a single 2.5-inch drive, but a small number of laptops with a screen wider than 15 inches can house two drives."
,,,"A variety of external HDDs or NAS data storage servers with support of RAID technology can be attached to virtually any laptop over such interfaces as USB, FireWire, eSATA, or Thunderbolt, or over a wired or wireless network to further increase space for the storage of data. Many laptops also incorporate a card reader which allows for use of memory cards, such as those used for digital cameras, which are typically SD or microSD cards. This enables users to download digital pictures from an SD card onto a laptop, thus enabling them to delete the SD card's contents to free up space for taking new pictures."
,,,"Optical disc drives capable of playing CD-ROMs, compact discs (CD), DVDs, and in some cases, Blu-ray discs (BD), were nearly universal on full-sized models between the mid-1990s and the early 2010s. As of 2021, drives are uncommon in compact or premium laptops; they remain available in some bulkier models, but the trend towards thinner and lighter machines is gradually eliminating these drives and players – when needed they can be connected via USB instead."
,,,"An alphanumeric keyboard is used to enter text, data, and other commands (e.g., function keys). A touchpad (also called a trackpad), a pointing stick, or both, are used to control the position of the cursor on the screen, and an integrated keyboard[41] is used for typing. Some touchpads have buttons separate from the touch surface, while others share the surface. A quick double-tap is typically registered as a click, and operating systems may recognize multi-finger touch gestures."
,,,"An external keyboard and mouse may be connected using a USB port or wirelessly, via Bluetooth or similar technology. Some laptops have multitouch touchscreen displays, either available as an option or standard. Most laptops have webcams and microphones, which can be used to communicate with other people with both moving images and sound, via web conferencing or video-calling software."
,,,"Laptops typically have USB ports and a combined headphone/microphone jack, for use with headphones, a combined headset, or an external mic. Many laptops have a card reader for reading digital camera SD cards."
,,,"On a typical laptop there are several USB ports; if they use only the older USB connectors instead of USB-C, they will typically have an external monitor port (VGA, DVI, HDMI or Mini DisplayPort or occasionally more than one), an audio in/out port (often in form of a single socket) is common. It is possible to connect up to three external displays to a 2014-era laptop via a single Mini DisplayPort, using multi-stream transport technology.[35]"
,,,"Apple, in a 2015 version of its MacBook, transitioned from a number of different I/O ports to a single USB-C port.[42] This port can be used both for charging and connecting a variety of devices through the use of aftermarket adapters. Google, with its updated version of Chromebook Pixel, shows a similar transition trend towards USB-C, although keeping older USB Type-A ports for a better compatibility with older devices.[43] Although being common until the end of the 2000s decade, Ethernet network port are rarely found on modern laptops, due to widespread use of wireless networking, such as Wi-Fi. Legacy ports such as a PS/2 keyboard/mouse port, serial port, parallel port, or FireWire are provided on some models, but they are increasingly rare. On Apple's systems, and on a handful of other laptops, there are also Thunderbolt ports, but Thunderbolt 3 uses USB-C. Laptops typically have a headphone jack, so that the user can connect external headphones or amplified speaker systems for listening to music or other audio."
,,,"In the past, a PC Card (formerly PCMCIA) or ExpressCard slot for expansion was often present on laptops to allow adding and removing functionality, even when the laptop is powered on; these are becoming increasingly rare since the introduction of USB 3.0. Some internal subsystems such as Ethernet, Wi-Fi, or a wireless cellular modem can be implemented as replaceable internal expansion cards, usually accessible under an access cover on the bottom of the laptop. The standard for such cards is PCI Express, which comes in both mini and even smaller M.2 sizes. In newer laptops, it is not uncommon to also see Micro SATA (mSATA) functionality on PCI Express Mini or M.2 card slots allowing the use of those slots for SATA-based solid-state drives.[44]"
,,,"Since the late 1990s, laptops have typically used lithium ion or lithium polymer batteries, These replaced the older nickel metal-hydride typically used in the 1990s, and nickel–cadmium batteries used in most of the earliest laptops. A few of the oldest laptops used non-rechargeable batteries, or lead–acid batteries."
,,,"Battery life is highly variable by model and workload and can range from one hour to nearly a day. A battery's performance gradually decreases over time; a substantial reduction in capacity is typically evident after one to three years of regular use, depending on the charging and discharging pattern and the design of the battery. Innovations in laptops and batteries have seen situations in which the battery can provide up to 24 hours of continued operation, assuming average power consumption levels. An example is the HP EliteBook 6930p when used with its ultra-capacity battery.[45]"
,,,Laptops with removable batteries may support larger replacement batteries with extended capacity.
,,,"A laptop's battery is charged using an external power supply, which is plugged into a wall outlet. The power supply outputs a DC voltage typically in the range of 7.2—24 volts. The power supply is usually external and connected to the laptop through a DC connector cable. In most cases, it can charge the battery and power the laptop simultaneously. When the battery is fully charged, the laptop continues to run on power supplied by the external power supply, avoiding battery use. If the used power supply is not strong enough to power computing components and charge the battery simultaneously, the battery may charge in a shorter period of time if the laptop is turned off or sleeping. The charger typically adds about 400 grams (0.88 lb) to the overall transporting weight of a laptop, although some models are substantially heavier or lighter. Most 2016-era laptops use a smart battery, a rechargeable battery pack with a built-in battery management system (BMS). The smart battery can internally measure voltage and current, and deduce charge level and State of Health (SoH) parameters, indicating the state of the cells.[citation needed]"
,,,"Historically, DC connectors, typically cylindrical/barrel-shaped coaxial power connectors have been used in laptops. Some vendors such as Lenovo made intermittent use of a rectangular connector."
,,,"Some connector heads feature a center pin to allow the end device to determine the power supply type by measuring the resistance between it and the connector's negative pole (outer surface). Vendors may block charging if a power supply is not recognized as original part, which could deny the legitimate use of universal third-party chargers.[46]"
,,,"With the advent of USB-C, portable electronics made increasing use of it for both power delivery and data transfer. Its support for 20 V (common laptop power supply voltage) and 5 A typically suffices for low to mid-end laptops, but some with higher power demands such as gaming laptops depend on dedicated DC connectors to handle currents beyond 5 A without risking overheating, some even above 10 A. Additionally, dedicated DC connectors are more durable and less prone to wear and tear from frequent reconnection, as their design is less delicate.[47]"
,,,"Waste heat from the operation is difficult to remove in the compact internal space of a laptop. The earliest laptops used passive cooling; this gave way to heat sinks placed directly on the components to be cooled, but when these hot components are deep inside the device, a large space-wasting air duct is needed to exhaust the heat. Modern laptops instead rely on heat pipes to rapidly move waste heat towards the edges of the device, to allow for a much smaller and compact fan and heat sink cooling system. Waste heat is usually exhausted away from the device operator towards the rear or sides of the device. Multiple air intake paths are used since some intakes can be blocked, such as when the device is placed on a soft conforming surface like a chair cushion. Secondary device temperature monitoring may reduce performance or trigger an emergency shutdown if it is unable to dissipate heat, such as if the laptop were to be left running and placed inside a carrying case. Aftermarket cooling pads with external fans can be used with laptops to reduce operating temperatures."
,,,"A docking station (sometimes referred to simply as a dock) is a laptop accessory that contains multiple ports and in some cases expansion slots or bays for fixed or removable drives. A laptop connects and disconnects to a docking station, typically through a single large proprietary connector. A docking station is an especially popular laptop accessory in a corporate computing environment, due to a possibility of a docking station transforming a laptop into a full-featured desktop replacement, yet allowing for its easy release. This ability can be advantageous to ""road warrior"" employees who have to travel frequently for work, and yet who also come into the office. If more ports are needed, or their position on a laptop is inconvenient, one can use a cheaper passive device known as a port replicator. These devices mate to the connectors on the laptop, such as through USB or FireWire."
,,,"Laptop charging trolleys, also known as laptop trolleys or laptop carts, are mobile storage containers to charge multiple laptops, netbooks, and tablet computers at the same time. The trolleys are used in schools that have replaced their traditional static computer labs[48] suites of desktop equipped with ""tower"" computers, but do not have enough plug sockets in an individual classroom to charge all of the devices. The trolleys can be wheeled between rooms and classrooms so that all students and teachers in a particular building can access fully charged IT equipment.[49]"
,,,"Laptop charging trolleys are also used to deter and protect against opportunistic and organized theft. Schools, especially those with open plan designs, are often prime targets for thieves who steal high-value items. Laptops, netbooks, and tablets are among the highest–value portable items in a school. Moreover, laptops can easily be concealed under clothing and stolen from buildings. Many types of laptop–charging trolleys are designed and constructed to protect against theft. They are generally made out of steel, and the laptops remain locked up while not in use. Although the trolleys can be moved between areas from one classroom to another, they can often be mounted or locked to the floor, support pillars, or walls to prevent thieves from stealing the laptops, especially overnight.[48]"
,,,"In some laptops, solar panels are able to generate enough solar power for the laptop to operate.[50] The One Laptop Per Child Initiative released the OLPC XO-1 laptop which was tested and successfully operated by use of solar panels.[51] Presently, they are designing an OLPC XO-3 laptop with these features. The OLPC XO-3 can operate with 2 watts of electricity because its renewable energy resources generate a total of 4 watts.[52][53] Samsung has also designed the NC215S solar–powered notebook that will be sold commercially in the U.S. market.[54]"
,,,"A common accessory for laptops is a laptop sleeve, laptop skin, or laptop case, which provides a degree of protection from scratches. Sleeves, which are distinguished by being relatively thin and flexible, are most commonly made of neoprene, with sturdier ones made of low-resilience polyurethane. Some laptop sleeves are wrapped in ballistic nylon to provide some measure of waterproofing. Bulkier and sturdier cases can be made of metal with polyurethane padding inside and may have locks for added security. Metal, padded cases also offer protection against impacts and drops. Another common accessory is a laptop cooler, a device that helps lower the internal temperature of the laptop either actively or passively. A common active method involves using electric fans to draw heat away from the laptop, while a passive method might involve propping the laptop up on some type of pad so it can receive more airflow. Some stores sell laptop pads that enable a reclining person on a bed to use a laptop."
,,,"Some of the components of earlier models of laptops can easily be replaced without opening completely its bottom part, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc."
,,,"Some of the components of recent models of laptops reside inside. Replacing most of its components, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc., requires removal of its either top or bottom part, removal of the motherboard, and returning them."
,,,"In some types, solder and glue are used to mount components such as RAM, storage, and batteries, making repairs additionally difficult.[55][56]"
,,,Features that certain early models of laptops used to have that are not available in most current laptops include:
,,,"Portability is usually the first feature mentioned in any comparison of laptops versus desktop PCs.[57] Physical portability allows a laptop to be used in many places—not only at home and the office but also during commuting and flights, in coffee shops, in lecture halls and libraries, at clients' locations or a meeting room, etc. Within a home, portability enables laptop users to move their devices from the living room to the dining room to the family room. Portability offers several distinct advantages:"
,,,Other advantages of laptops:
,,,"Compared to desktop PCs, laptops have disadvantages in the following areas:"
,,,"While the performance of mainstream desktops and laptops are comparable, and the cost of laptops has fallen less rapidly than desktops, laptops remain more expensive than desktop PCs at the same performance level.[60][needs update] The upper limits of performance of laptops remain much lower than the highest-end desktops (especially ""workstation class"" machines with two processor sockets), and ""leading-edge"" features usually appear first in desktops and only then, as the underlying technology matures, are adapted to laptops."
,,,"For Internet browsing and typical office applications, where the computer spends the majority of its time waiting for the next user input, even relatively low-end laptops (such as Netbooks) can be fast enough for some users.[61] Most higher-end laptops are sufficiently powerful for high-resolution movie playback, some 3D gaming and video editing and encoding. However, laptop processors can be disadvantaged when dealing with a higher-end database, maths, engineering, financial software, virtualization, etc. This is because laptops use the mobile versions of processors to conserve power, and these lag behind desktop chips when it comes to performance. Some manufacturers work around this performance problem by using desktop CPUs for laptops.[62]"
,,,"The upgradeability of laptops is very limited compared to thoroughly standardized desktops. In general, hard drives and memory can be upgraded easily. Optical drives and internal expansion cards may be upgraded if they follow an industry standard, but all other internal components, including the motherboard, CPU, and graphics, are not always intended to be upgradeable. Intel, Asus, Compal, Quanta and some other laptop manufacturers have created the Common Building Block standard for laptop parts to address some of the inefficiencies caused by the lack of standards. The reasons for limited upgradeability are both technical and economic. There is no industry-wide standard form factor for laptops; each major laptop manufacturer pursues its own proprietary design and construction, with the result that laptops are difficult to upgrade and have high repair costs. Moreover, starting with 2013 models, laptops have become increasingly integrated (soldered) with the motherboard for most of its components (CPU, SSD, RAM, keyboard, etc.) to reduce size and upgradeability prospects. Devices such as sound cards, network adapters, hard and optical drives, and numerous other peripherals are available, but these upgrades usually impair the laptop's portability, because they add cables and boxes to the setup and often have to be disconnected and reconnected when the laptop is on the move.[citation needed]"
,,,"Prolonged use of laptops can cause repetitive strain injury because of their small, flat keyboard and trackpad pointing devices.[63] Usage of separate, external ergonomic keyboards and pointing devices is recommended to prevent injury when working for long periods of time; they can be connected to a laptop easily by USB, Bluetooth or via a docking station. Some health standards require ergonomic keyboards at workplaces."
,,,"A laptop's integrated screen often requires users to lean over for a better view, which can cause neck or spinal injuries. A larger and higher-quality external screen can be connected to almost any laptop to alleviate this and to provide additional screen space for more productive work. Another solution is to use a computer stand."
,,,"A study by State University of New York researchers found that heat generated from laptops can increase the temperature of the lap of male users when balancing the computer on their lap, potentially putting sperm count at risk. The study, which included roughly two dozen men between the ages of 21 and 35, found that the sitting position required to balance a laptop can increase scrotum temperature by as much as 2.1 °C (4 °F). However, further research is needed to determine whether this directly affects male sterility.[64] A later 2010 study of 29 males published in Fertility and Sterility found that men who kept their laptops on their laps experienced scrotal hyperthermia (overheating) in which their scrotal temperatures increased by up to 2.0 °C (4 °F). The resulting heat increase, which could not be offset by a laptop cushion, may increase male infertility.[65][66][67][68][69]"
,,,"A common practical solution to this problem is to place the laptop on a table or desk or to use a book or pillow between the body and the laptop.[citation needed] Another solution is to obtain a cooling unit for the laptop. These are usually USB powered and consist of a hard thin plastic case housing one, two, or three cooling fans – with the entire assembly designed to sit under the laptop in question – which results in the laptop remaining cool to the touch, and greatly reduces laptop heat buildup."
,,,"Heat generated from using a laptop on the lap can also cause skin discoloration on the thighs known as ""toasted skin syndrome"".[70][71][72][73]"
,,,"Laptops are less durable than desktops/PCs. However, the durability of the laptop depends on the user if proper maintenance is done then the laptop can work longer."
,,,"Because of their portability, laptops are subject to more wear and physical damage than desktops. Components such as screen hinges, latches, power jacks, and power cords deteriorate gradually from ordinary use and may have to be replaced. A liquid spill onto the keyboard, a rather minor mishap with a desktop system (given that a basic keyboard costs about US$20), can damage the internals of a laptop and destroy the computer, result in a costly repair or entire replacement of laptops. One study found that a laptop is three times more likely to break during the first year of use than a desktop.[74] To maintain a laptop, it is recommended to clean it every three months for dirt, debris, dust, and food particles. Most cleaning kits consist of a lint-free or microfiber cloth for the LCD screen and keyboard, compressed air for getting dust out of the cooling fan, and a cleaning solution. Harsh chemicals such as bleach should not be used to clean a laptop, as they can damage it.[75]"
,,,"Laptops rely on extremely compact cooling systems involving a fan and heat sink that can fail from blockage caused by accumulated airborne dust and debris. Most laptops do not have any type of removable dust collection filter over the air intake for these cooling systems, resulting in a system that gradually conducts more heat and noise as the years pass. In some cases, the laptop starts to overheat even at idle load levels. This dust is usually stuck inside where the fan and heat sink meet, where it can not be removed by a casual cleaning and vacuuming. Most of the time, compressed air can dislodge the dust and debris but may not entirely remove it. After the device is turned on, the loose debris is reaccumulated into the cooling system by the fans. Complete disassembly is usually required to clean the laptop entirely. However, preventative maintenance such as regular cleaning of the heat sink via compressed air can prevent dust build-up on the heat sink. Many laptops are difficult to disassemble by the average user and contain components that are sensitive to electrostatic discharge (ESD)."
,,,"Battery life is limited because the capacity drops with time, eventually requiring replacement after as little as a year. A new battery typically stores enough energy to run the laptop for three to five hours, depending on usage, configuration, and power management settings. Yet, as it ages, the battery's energy storage will dissipate progressively until it lasts only a few minutes. The battery is often easily replaceable and a higher capacity model may be obtained for longer charging and discharging time. Some laptops (specifically ultrabooks) do not have the usual removable battery and have to be brought to the service center of their manufacturer or a third-party laptop service center to have their battery replaced. Replacement batteries can also be expensive."
,,,"Because they are valuable, commonly used, portable, and easy to hide in a backpack or other type of travel bag, laptops are often stolen. Every day, over 1,600 laptops go missing from U.S. airports.[76] The cost of stolen business or personal data, and of the resulting problems (identity theft, credit card fraud, breach of privacy), can be many times the value of the stolen laptop itself. Consequently, the physical protection of laptops and the safeguarding of data contained on them are both of great importance. Most laptops have a Kensington security slot, which can be used to tether them to a desk or other immovable object with a security cable and lock. In addition, modern operating systems and third-party software offer disk encryption functionality, which renders the data on the laptop's hard drive unreadable without a key or a passphrase. As of 2015, some laptops also have additional security elements added, including eye recognition software and fingerprint scanning components.[77]"
,,,"Software such as LoJack for Laptops, Laptop Cop, and GadgetTrack have been engineered to help people locate and recover their stolen laptops in the event of theft. Setting one's laptop with a password on its firmware (protection against going to firmware setup or booting), internal HDD/SSD (protection against accessing it and loading an operating system on it afterward), and every user account of the operating system are additional security measures that a user should do.[78][79] Fewer than 5% of lost or stolen laptops are recovered by the companies that own them,[80] however, that number may decrease due to a variety of companies and software solutions specializing in laptop recovery. In the 2010s, the common availability of webcams on laptops raised privacy concerns. In Robbins v. Lower Merion School District (Eastern District of Pennsylvania 2010), school-issued laptops loaded with special software enabled staff from two high schools to take secret webcam shots of students at home, via their students' laptops.[81][82][83]"
,,,"There are many laptop brands and manufacturers. Several major brands that offer notebooks in various classes are listed in the adjacent box.The major brands usually offer good service and support, including well-executed documentation and driver downloads that remain available for many years after a particular laptop model is no longer produced. Capitalizing on service, support, and brand image, laptops from major brands are more expensive than laptops by smaller brands and ODMs. Some brands specialize in a particular class of laptops, such as gaming laptops (Alienware), high-performance laptops (HP Envy), netbooks (EeePC) and laptops for children (OLPC)."
,,,"Many brands, including the major ones, do not design and do not manufacture their laptops. Instead, a small number of Original Design Manufacturers (ODMs) design new models of laptops, and the brands choose the models to be included in their lineup. In 2006, 7 major ODMs manufactured 7 of every 10 laptops in the world, with the largest one (Quanta Computer) having 30% of the world market share.[84] Therefore, identical models are available both from a major label and from a low-profile ODM in-house brand."
,,,"Battery-powered portable computers had just 2% worldwide market share in 1986.[85] However, laptops have become increasingly popular, both for business and personal use.[86] Around 109 million notebook PCs shipped worldwide in 2007, a growth of 33% compared to 2006.[87] In 2008 it was estimated that 145.9 million notebooks were sold, and that the number would grow in 2009 to 177.7 million.[88] The third quarter of 2008 was the first time when worldwide notebook PC shipments exceeded desktops, with 38.6 million units versus 38.5 million units.[86][89][90][91]"
,,,"May 2005 was the first time notebooks outsold desktops in the US over the course of a full month; at the time notebooks sold for an average of $1,131 while desktops sold for an average of $696.[92] When looking at operating systems, for Microsoft Windows laptops the average selling price (ASP) showed a decline in 2008/2009, possibly due to low-cost netbooks, drawing an average US$689 at U.S. retail stores in August 2008. In 2009, ASP had further fallen to $602 by January and to $560 in February. While Windows machines ASP fell $129 in these seven months, Apple macOS laptop ASP declined just $12 from $1,524 to $1,512.[93]"
,,,"The list of materials that go into a laptop computer is long, and many of the substances used, such as beryllium (used in beryllium-copper alloy contacts in some connectors and sockets), lead (used in lead-tin solder), chromium, and mercury (used in CCFL LCD backlights) compounds, are toxic or carcinogenic to humans. Although these toxins are relatively harmless when the laptop is in use, concerns that discarded laptops cause a serious health risk and toxic environmental damage, were so strong, that the Waste Electrical and Electronic Equipment Directive (WEEE Directive) in Europe specified that all laptop computers must be recycled by law. Similarly, the U.S. Environmental Protection Agency (EPA) has outlawed landfill dumping or the incinerating of discarded laptop computers."
,,,"Most laptop computers begin the recycling process with a method known as Demanufacturing, this involves the physical separation of the components of the laptop.[94] These components are then either grouped into materials (e.g. plastic, metal and glass) for recycling or more complex items that require more advanced materials separation (e.g.) circuit boards, hard drives and batteries."
,,,Corporate laptop recycling can require an additional process known as data destruction. The data destruction process ensures that all information or data that has been stored on a laptop hard drive can never be retrieved again. Below is an overview of some of the data protection and environmental laws and regulations applicable for laptop recycling data destruction:
,,,"The ruggedized Grid Compass computer was used since the early days of the Space Shuttle program. The first commercial laptop used in space was a Macintosh portable in 1991 aboard Space Shuttle mission STS-43.[95][96][97] Apple and other laptop computers continue to be flown aboard crewed spaceflights, though the only long-duration flight certified computer for the International Space Station is the ThinkPad.[98] As of 2011, over 100 ThinkPads were aboard the ISS. Laptops used aboard the International Space Station and other spaceflights are generally the same ones that can be purchased by the general public but needed modifications are made to allow them to be used safely and effectively in a weightless environment such as updating the cooling systems to function without relying on hot air rising and accommodation for the lower cabin air pressure.[99] Laptops operating in harsh usage environments and conditions, such as strong vibrations, extreme temperatures, and wet or dusty conditions differ from those used in space in that they are custom designed for the task and do not use commercial off-the-shelf hardware."
,,,Mobile may refer to:
,,,"A computer is a digital electronic machine that can be programmed to carry out sequences of arithmetic or logical operations (computation) automatically. Modern computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. A computer system is a ""complete"" computer that includes the hardware, operating system (main software), and peripheral equipment needed and used for ""full"" operation. This term may also refer to a group of computers that are linked and function together, such as a computer network or computer cluster."
,,,"A broad range of industrial and consumer products use computers as control systems. Simple special-purpose devices like microwave ovens and remote controls are included, as are factory devices like industrial robots and computer-aided design, as well as general-purpose devices like personal computers and mobile devices like smartphones. Computers power the Internet, which links billions of other computers and users."
,,,"Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit (IC) chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (as predicted by Moore's law), leading to the Digital Revolution during the late 20th to early 21st centuries."
,,,"Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, along with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved."
,,,"According to the Oxford English Dictionary, the first known use of computer was in a 1613 book called The Yong Mans Gleanings by the English writer Richard Brathwait: ""I haue  [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number."" This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. During the latter part of this period women were often hired as computers because they could be paid less than their male counterparts.[1] By 1943, most human computers were women.[2]"
,,,"The Online Etymology Dictionary gives the first attested use of computer in the 1640s, meaning 'one who calculates'; this is an ""agent noun from compute (v.)"". The Online Etymology Dictionary states that the use of the term to mean ""'calculating machine' (of any type) is from 1897.""  The Online Etymology Dictionary indicates that the ""modern use"" of the term, to mean 'programmable digital electronic computer' dates from ""1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine"".[3]"
,,,"Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.[a][4] The use of counting rods is one example."
,,,"The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.[5]"
,,,"The Antikythera mechanism is believed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to approximately c. 100 BC. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.[7]"
,,,"Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.[8] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[9][10] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[11] Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,[12] an early fixed-wired knowledge processing machine[13] with a gear train and gear-wheels,[14] c. 1000 AD."
,,,"The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation."
,,,The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.
,,,"The slide rule was invented around 1620–1630 by the English clergyman William Oughtred, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft."
,,,"In the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically ""programmed"" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.[15]"
,,,"In 1831–1835, mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine, which, through a system of pulleys and cylinders and over, could predict the perpetual calendar for every year from AD 0 (that is, 1 BC) to AD 4000, keeping track of leap years and varying day length. The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location."
,,,"The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[16] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers."
,,,"Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the ""father of the computer"",[17] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[18][19]"
,,,"The machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906."
,,,"During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[20] The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson (later to become Lord Kelvin) in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the elder brother of the more famous Sir William Thomson.[16]"
,,,"The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (slide rule) and aircraft (control systems)."
,,,"By 1938, the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well."
,,,"Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.[21]"
,,,"In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[22][23] The Z3 was built with 2000 relays, implementing a 22 bit word length that operated at a clock frequency of about 5–10 Hz.[24] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating-point numbers. Rather than the harder-to-implement decimal system (used in Charles Babbage's earlier design), using a binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[25] The Z3 was not itself a universal computer but could be extended to be Turing complete.[26][27]"
,,,"Zuse's next computer, the Z4, became the world's first commercial computer; after initial delay due to the Second World War, it was completed in 1950 and delivered to the ETH Zurich.[28] The computer was manufactured by Zuse's own company, Zuse KG [de], which was founded in 1941 as the first company with the sole purpose of developing computers.[28]"
,,,"Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[20] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942,[29] the first ""automatic electronic digital computer"".[30] This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[31]"
,,,"During World War II, the British code-breakers at Bletchley Park achieved a number of successes at breaking encrypted German military communications. The German encryption machine, Enigma, was first attacked with the help of the electro-mechanical bombes which were often run by women.[32][33] To crack the more sophisticated German Lorenz SZ 40/42 machine, used for high-level Army communications, Max Newman and his colleagues commissioned Flowers to build the Colossus.[31] He spent eleven months from early February 1943 designing and building the first Colossus.[34] After a functional test in December 1943, Colossus was shipped to Bletchley Park, where it was delivered on 18 January 1944[35] and attacked its first message on 5 February.[31]"
,,,"Colossus was the world's first electronic digital programmable computer.[20] It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1,500 thermionic valves (tubes), but Mark II with 2,400 valves, was both five times faster and simpler to operate than Mark I, greatly speeding the decoding process.[36][37]"
,,,"The ENIAC[38] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the U.S. Although the ENIAC was similar to the Colossus, it was much faster, more flexible, and it was Turing-complete. Like the Colossus, a ""program"" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches. The programmers of the ENIAC were six women, often known collectively as the ""ENIAC girls"".[39][40]"
,,,"It combined the high speed of electronics with the ability to be programmed for many complex problems. It could add or subtract 5000 times a second, a thousand times faster than any other machine. It also had modules to multiply, divide, and square root. High speed memory was limited to 20 words (about 80 bytes). Built under the direction of John Mauchly and J. Presper Eckert at the University of Pennsylvania, ENIAC's development and construction lasted from 1943 to full operation at the end of 1945. The machine was huge, weighing 30 tons, using 200 kilowatts of electric power and contained over 18,000 vacuum tubes, 1,500 relays, and hundreds of thousands of resistors, capacitors, and inductors.[41]"
,,,"The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper,[42] On Computable Numbers. Turing proposed a simple device that he called ""Universal Computing machine"" and that is now known as a universal Turing machine. He proved that such a machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable. The fundamental concept of Turing's design is the stored program, where all the instructions for computing are stored in memory. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[43] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine."
,,,"Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine.[31] With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945, Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report ""Proposed Electronic Calculator"" was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945.[20]"
,,,"The Manchester Baby was the world's first stored-program computer. It was built at the University of Manchester in England by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[44] It was designed as a testbed for the Williams tube, the first random-access digital storage device.[45] Although the computer was considered ""small and primitive"" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[46] As soon as the Baby had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. Grace Hopper was the first person to develop a compiler for programming language.[2]"
,,,"The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[47] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[48] In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951[49] and ran the world's first regular routine office computer job."
,,,"The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen and Walter Brattain, while working under William Shockley at Bell Labs, built the first working transistor, the point-contact transistor, in 1947, which was followed by Shockley's bipolar junction transistor in 1948.[50][51] From 1955 onwards, transistors replaced vacuum tubes in computer designs, giving rise to the ""second generation"" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space. However, early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, which limited them to a number of specialised applications.[52]"
,,,"At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[53] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125 kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[54] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[54][55]"
,,,"The metal–oxide–silicon field-effect transistor (MOSFET), also known as the MOS transistor, was invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.[56] It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[52] With its high scalability,[57] and much lower power consumption and higher density than bipolar junction transistors,[58] the MOSFET made it possible to build high-density integrated circuits.[59][60] In addition to data processing, it also enabled the practical use of MOS transistors as memory cell storage elements, leading to the development of MOS semiconductor memory, which replaced earlier magnetic-core memory in computers. The MOSFET led to the microcomputer revolution,[61] and became the driving force behind the computer revolution.[62][63] The MOSFET is the most widely used transistor in computers,[64][65] and is the fundamental building block of digital electronics.[66]"
,,,"The next great advance in computing power came with the advent of the integrated circuit (IC).The idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence, Geoffrey W.A. Dummer. Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington, D.C. on 7 May 1952.[67]"
,,,"The first working ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor.[68] Kilby recorded his initial ideas concerning the integrated circuit in July 1958, successfully demonstrating the first working integrated example on 12 September 1958.[69] In his patent application of 6 February 1959, Kilby described his new device as ""a body of semiconductor material ... wherein all the components of the electronic circuit are completely integrated"".[70][71] However, Kilby's invention was a hybrid integrated circuit (hybrid IC), rather than a monolithic integrated circuit (IC) chip.[72] Kilby's IC had external wire connections, which made it difficult to mass-produce.[73]"
,,,"Noyce also came up with his own idea of an integrated circuit half a year later than Kilby.[74] Noyce's invention was the first true monolithic IC chip.[75][73] His chip solved many practical problems that Kilby's had not. Produced at Fairchild Semiconductor, it was made of silicon, whereas Kilby's chip was made of germanium. Noyce's monolithic IC was fabricated using the planar process, developed by his colleague Jean Hoerni in early 1959. In turn, the planar process was based on Mohamed M. Atalla's work on semiconductor surface passivation by silicon dioxide in the late 1950s.[76][77][78]"
,,,"Modern monolithic ICs are predominantly MOS (metal-oxide-semiconductor) integrated circuits, built from MOSFETs (MOS transistors).[79] The earliest experimental MOS IC to be fabricated was a 16-transistor chip built by Fred Heiman and Steven Hofstein at RCA in 1962.[80] General Microelectronics later introduced the first commercial MOS IC in 1964,[81] developed by Robert Norman.[80] Following the development of the self-aligned gate (silicon-gate) MOS transistor by Robert Kerwin, Donald Klein and John Sarace at Bell Labs in 1967, the first silicon-gate MOS IC with self-aligned gates was developed by Federico Faggin at Fairchild Semiconductor in 1968.[82] The MOSFET has since become the most critical device component in modern ICs.[83]"
,,,"The development of the MOS integrated circuit led to the invention of the microprocessor,[84][85] and heralded an explosion in the commercial and personal use of computers. While the subject of exactly which device was the first microprocessor is contentious, partly due to lack of agreement on the exact definition of the term ""microprocessor"", it is largely undisputed that the first single-chip microprocessor was the Intel 4004,[86] designed and realized by Federico Faggin with his silicon-gate MOS IC technology,[84] along with Ted Hoff, Masatoshi Shima and Stanley Mazor at Intel.[b][88] In the early 1970s, MOS IC technology enabled the integration of more than 10,000 transistors on a single chip.[60]"
,,,"System on a Chip (SoCs) are complete computers on a microchip (or chip) the size of a coin.[89] They may or may not have integrated RAM and flash memory. If not integrated, the RAM is usually placed directly above (known as Package on package) or below (on the opposite side of the circuit board) the SoC, and the flash memory is usually placed right next to the SoC, this all done to improve data transfer speeds, as the data signals don't have to travel long distances. Since ENIAC in 1945, computers have advanced enormously, with modern SoCs (Such as the Snapdragon 865) being the size of a coin while also being hundreds of thousands of times more powerful than ENIAC, integrating billions of transistors, and consuming only a few watts of power."
,,,"The first mobile computers were heavy and ran from mains power. The 50 lb (23 kg) IBM 5100 was an early example. Later portables such as the Osborne 1 and Compaq Portable were considerably lighter but still needed to be plugged in. The first laptops, such as the Grid Compass, removed this requirement by incorporating batteries – and with the continued miniaturization of computing resources and advancements in portable battery life, portable computers grew in popularity in the 2000s.[90] The same developments allowed manufacturers to integrate computing resources into cellular mobile phones by the early 2000s."
,,,"These smartphones and tablets run on a variety of operating systems and recently became the dominant computing device on the market.[91] These are powered by System on a Chip (SoCs), which are complete computers on a microchip the size of a coin.[89]"
,,,"Computers can be classified in a number of different ways, including:"
,,,"The term hardware covers all of those parts of a computer that are tangible physical objects. Circuits, computer chips, graphic cards, sound cards, memory (RAM), motherboard, displays, power supplies, cables, keyboards, printers and ""mice"" input devices are all hardware."
,,,"A general-purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires. Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch. Each circuit represents a bit (binary digit) of information so that when the circuit is on it represents a ""1"", and when off it represents a ""0"" (in positive logic representation). The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits."
,,,"When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of input devices are:"
,,,The means through which computer gives output are known as output devices. Some examples of output devices are:
,,,"The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[d] Control systems in advanced computers may change the order of execution of some instructions to improve performance."
,,,"A key component common to all CPUs is the program counter, a special memory cell (a register) that keeps track of which location in memory the next instruction is to be read from.[e]"
,,,"The control system's function is as follows— this is a simplified description, and some of these steps may be performed concurrently or in a different order depending on the type of CPU:"
,,,"Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as ""jumps"" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow)."
,,,"The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen."
,,,"The control unit, ALU, and registers are collectively known as a central processing unit (CPU). Early CPUs were composed of many separate components. Since the 1970s, CPUs have typically been constructed on a single MOS integrated circuit chip called a microprocessor."
,,,"The ALU is capable of performing two classes of operations: arithmetic and logic.[92] The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can operate only on whole numbers (integers) while others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operation—although it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return Boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other (""is 64 greater than 65?""). Logic operations involve Boolean logic: AND, OR, XOR, and NOT. These can be useful for creating complicated conditional statements and processing Boolean logic."
,,,"Superscalar computers may contain multiple ALUs, allowing them to process several instructions simultaneously.[93] Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices."
,,,"A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered ""address"" and can store a single number. The computer can be instructed to ""put the number 123 into the cell numbered 1357"" or to ""add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595."" The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers."
,,,"In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or −128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory."
,,,"The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area. There are typically between two and one hundred registers depending on the type of CPU. Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed. As data is constantly being worked on, reducing the need to access main memory (which is often slow compared to the ALU and control units) greatly increases the computer's speed."
,,,Computer main memory comes in two principal varieties:
,,,"RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[f]"
,,,"In more sophisticated computers there may be one or more RAM cache memories, which are slower than registers but faster than main memory. Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically, often without the need for any intervention on the programmer's part."
,,,"I/O is the means by which a computer exchanges information with the outside world.[95] Devices that provide input or output to the computer are called peripherals.[96] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.I/O devices are often complex computers in their own right, with their own CPU and memory. A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics.[citation needed] Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O. A 2016-era flat screen display contains its own computer circuitry."
,,,"While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[97] One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running ""at the same time"". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed ""time-sharing"" since each program is allocated a ""slice"" of time in turn.[98]"
,,,"Before the era of inexpensive computers, the principal use for multitasking was to allow many people to share the same computer. Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a ""time slice"" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss."
,,,"Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration, a technique once employed in only large and powerful machines such as supercomputers, mainframe computers and servers. Multiprocessor and multi-core (multiple CPUs on a single integrated circuit) personal and laptop computers are now widely available, and are being increasingly used in lower-end markets as a result."
,,,"Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general-purpose computers.[g] They often feature thousands of CPUs, customized high-speed interconnects, and specialized computing hardware. Such designs tend to be useful for only specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once. Supercomputers usually see usage in large-scale simulation, graphics rendering, and cryptography applications, as well as with other so-called ""embarrassingly parallel"" tasks."
,,,"Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. Software is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. It is often divided into system software and application software Computer hardware and software require each other and neither can be realistically used on its own. When software is stored in hardware that cannot easily be modified, such as with BIOS ROM in an IBM PC compatible computer, it is sometimes called ""firmware""."
,,,"There are thousands of different programming languages—some intended for general purpose, others useful for only highly specialized applications."
,,,"The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language. In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors."
,,,This section applies to most common RAM machine–based computers.
,,,"In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called ""jump"" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that ""remembers"" the location it jumped from and another instruction to return to the instruction following that jump instruction."
,,,"Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention."
,,,"Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:"
,,,"Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second."
,,,"In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches."
,,,"While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[h] it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember – a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler."
,,,"Programming languages provide various ways of specifying programs for computers to run. Unlike natural languages, programming languages are designed to permit no ambiguity and to be concise. They are purely written languages and are often difficult to read aloud. They are generally either translated into machine code by a compiler or an assembler before being run, or translated directly at run time by an interpreter. Sometimes programs are executed by a hybrid method of the two techniques."
,,,"Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) are generally unique to the particular architecture of a computer's central processing unit (CPU). For instance, an ARM architecture CPU (such as may be found in a smartphone or a hand-held videogame) cannot understand the machine language of an x86 CPU that might be in a PC.[i] Historically a significant number of other cpu architectures were created and saw extensive use, notably including the MOS Technology 6502 and 6510 in addition to the Zilog Z80."
,,,"Although considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually ""compiled"" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[j] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles."
,,,"Program design of small programs is relatively simple and involves the analysis of the problem, collection of inputs, using the programming constructs within languages, devising or using established procedures and algorithms, providing data for output devices and solutions to the problem as applicable. As problems become larger and more complex, features such as subprograms, modules, formal documentation, and new paradigms such as object-oriented programming are encountered. Large programs involving thousands of line of code and more require formal software methodologies.The task of developing large software systems presents a significant intellectual challenge. Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult; the academic and professional discipline of software engineering concentrates specifically on this challenge."
,,,"Errors in computer programs are called ""bugs"". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to ""hang"", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash.[100] Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[k] Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term ""bugs"" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[101]"
,,,"Computers have been used to coordinate information between multiple locations since the 1950s. The U.S. military's SAGE system was the first large-scale example of such a system, which led to a number of special-purpose commercial systems such as Sabre.[102] In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[103] The technologies that made the Arpanet possible spread and evolved."
,,,"In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. ""Wireless"" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments."
,,,"A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word ""computer"" is synonymous with a personal electronic computer,[l] the modern definition of a computer is literally: ""A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information.""[104] Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]"
,,,"There is active research to make computers out of many promising new types of technology, such as optical computers, DNA computers, neural computers, and quantum computers. Most computers are universal, and are able to calculate any computable function, and are limited only by their memory capacity and operating speed. However different designs of computers can give very different performance for particular problems; for example quantum computers can potentially break some modern encryption algorithms (by quantum factoring) very quickly."
,,,There are many types of computer architectures:
,,,"Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[105] Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms. The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The Church–Turing thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity."
,,,"A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning. Artificial intelligence based products generally fall into two major categories: rule-based systems and pattern recognition systems. Rule-based systems attempt to represent the rules used by human experts and tend to be expensive to develop. Pattern-based systems use data about a problem to generate conclusions. Examples of pattern-based systems include voice recognition, font recognition, translation and the emerging field of on-line marketing."
,,,"As the use of computers has spread throughout society, there are an increasing number of careers involving computers."
,,,"The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations, clubs and societies of both a formal and informal nature."
,,,"Microsoft Windows, commonly referred to as Windows, is a group of several proprietary graphical operating system families, all of which are developed and marketed by Microsoft. Each family caters to a certain sector of the computing industry. Active Microsoft Windows families include Windows NT and Windows IoT; these may encompass subfamilies, (e.g. Windows Server or Windows Embedded Compact) (Windows CE). Defunct Microsoft Windows families include Windows 9x, Windows Mobile and Windows Phone."
,,,"Microsoft introduced an operating environment named Windows on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs).[4] Microsoft Windows came to dominate the world's personal computer (PC) market with over 90% market share, overtaking Mac OS, which had been introduced in 1984."
,,,"Apple came to see Windows as an unfair encroachment on their innovation in GUI development as implemented on products such as the Lisa and Macintosh (eventually settled in court in Microsoft's favor in 1993).  On PCs, Windows is still the most popular operating system in all countries.[5][6] However, in 2014, Microsoft admitted losing the majority of the overall operating system market to Android,[7] because of the massive growth in sales of Android smartphones. In 2014, the number of Windows devices sold was less than 25% that of Android devices sold. This comparison, however, may not be fully relevant, as the two operating systems traditionally target different platforms. Still, numbers for server use of Windows (that are comparable to competitors) show one third market share, similar to that for end user use. "
,,,"As of October 2021[update], the most recent version of Windows for PCs and tablets is Windows 11, version 21H2. The most recent version for embedded devices is Windows 10, version 21H1. The most recent version for server computers is Windows Server 2022, version 21H2.[8] A specialized version of Windows also runs on the Xbox One and Xbox Series X/S video game consoles.[9]"
,,,"Microsoft, the developer of Windows, has registered several trademarks, each of which denotes a family of Windows operating systems that target a specific sector of the computing industry. As of 2014, the following Windows families were being actively developed:"
,,,The following Windows families are no longer being developed:
,,,The term Windows collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:
,,,"The history of Windows dates back to 1981 when Microsoft started work on a program called ""Interface Manager"". It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name ""Windows"", but Windows 1.0 was not released until November 1985.[12] Windows 1.0 was to compete with Apple's operating system, but achieved little popularity.  Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard Viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows. Microsoft sold as included Windows Development libraries with the C development environment, which included numerous windows samples.[13]"
,,,"Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management.[14] Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights.[15][16] Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory."
,,,"Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area.[citation needed]"
,,,"In addition to full Windows-packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set."
,,,"The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and use it for file system services.[17] However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control."
,,,"Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications.[citation needed] Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 is the first Microsoft Windows version to achieve broad commercial success, selling 2 million copies in the first six months.[18][19]"
,,,"Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along with Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.[20]"
,,,"Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1.[21] The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language.[22] Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities."
,,,"The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that ""by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world.""[23] Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer.[24] Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.[25]"
,,,"Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which were also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.[26]"
,,,"On September 14, 2000, Microsoft released Windows Me (Millennium Edition), the last DOS-based version of Windows. Windows Me incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs),[27] expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools.[28] However, Windows Me was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. PC World considered Windows Me to be one of the worst operating systems Microsoft had ever released, and the 4th worst tech product of all time.[11]"
,,,"In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as ""NT OS/2"". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.[29][30]"
,,,"Windows NT was the first Windows operating system based on a hybrid kernel. The hybrid kernel was designed as a modified microkernel, influenced by the Mach microkernel developed by Richard Rashid at Carnegie Mellon University, but without meeting all of the criteria of a pure microkernel."
,,,"The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.[30]"
,,,"The next major version of Windows NT, Windows XP, was released on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a ""task-oriented"" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, a ""compatibility mode"" to help provide backwards compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.[31][32]"
,,,"At retail, Windows XP was now marketed in two main editions: the ""Home"" edition was targeted towards consumers, while the ""Professional"" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the ""Media Center"" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the ""Tablet PC"" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications).[33][34][35] Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.[36]"
,,,"After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003.[30] It was followed in December 2005, by Windows Server 2003 R2."
,,,"After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008."
,,,"On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released as RTM (release to manufacturing) while the former was released to the public 3 months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible.[37] Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar with revealable jump lists that contain shortcuts to files frequently used with specific applications and shortcuts to tasks within the application,[38] a home networking system called HomeGroup,[39] and performance improvements."
,,,"Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. The new Windows version required a minimum resolution of 1024×768 pixels,[40] effectively making it unfit for netbooks with 800×600-pixel screens."
,,,"Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own OneDrive (formerly SkyDrive) and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture, and a new keyboard shortcut for screenshots.[41][42][43][44][45][46][47] An update to Windows 8, called Windows 8.1,[48] was released on October 17, 2013, and includes features such as new live tile sizes, deeper OneDrive integration, and many other revisions. Windows 8 and Windows 8.1 have been subject to some criticism, such as removal of the Start menu."
,,,"On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes on PC include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1, Windows 8.1 and Windows Phone 8.1 devices from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).[49]"
,,,"In February 2017, Microsoft announced the migration of its Windows source code repository from Perforce to Git. This migration involved 3.5 million separate files in a 300 gigabyte repository.[50] By May 2017, 90 percent of its engineering team was using Git, in about 8500 commits and 1760 Windows builds per day.[50]"
,,,"In June 2021, shortly before Microsoft's announcement of Windows 11, Microsoft updated their lifecycle policy pages for Windows 10, revealing that support for their last release of Windows 10 will be October 14, 2025.[51][52]"
,,,"On June 24, 2021, Windows 11 was announced as the successor to Windows 10 during a livestream. The new operating system was designed to be more user-friendly and understandable. It was released on October 5, 2021.[53][54] Windows 11 is a free upgrade to some Windows 10 users as of now."
,,,"In July 2021, Microsoft announced it will start selling subscriptions to virtualized Windows desktops as part of a new Windows 365 service in the following month. It is not a standalone version of Microsoft Windows, but a web service that provides access to Windows 10 and Windows 11 built on top of Azure Virtual Desktop. The new service will allow for cross-platform usage, aiming to make the operating system available for both Apple and Android users. The subscription service will be accessible through any operating system with a web browser. The new service is an attempt at capitalizing on the growing trend, fostered during the COVID-19 pandemic, for businesses to adopt a hybrid remote work environment, in which ""employees split their time between the office and home"". As the service will be accessible through web browsers, Microsoft will be able to bypass the need to publish the service through Google Play or the Apple App Store.[55][56][57][58][59]"
,,,"Microsoft announced Windows 365 availability to business and enterprise customers on August 2, 2021.[60]"
,,,"Multilingual support has been built into Windows since Windows 3.0. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs."
,,,"Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) –  they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translates the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language, and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but available as optional updates through the Windows Update service (except Windows 8)."
,,,The interface language of installed applications is not affected by changes in the Windows interface language. The availability of languages depends on the application developers themselves.
,,,"Windows 8 and Windows Server 2012 introduces a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets."
,,,"Windows NT included support for several platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000 (although some of the platforms implement 64-bit computing, the OS treated them as 32-bit). Windows 2000 dropped support for all platforms, except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Windows NT family still runs on IA-32 but the Windows Server line ceased supporting this platform with the release of Windows Server 2008 R2."
,,,"With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition, released in 2005, is the last Windows client operating systems to support Itanium. Windows Server line continues to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture."
,,,"On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 Editions to support x86-64 (or simply x64), the 64-bit version of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. x64 is still supported."
,,,"An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated. Starting from Windows 10 Fall Creators Update (version 1709) and later includes support for PCs with ARM architecture.[61]"
,,,Windows 11 is the first version to drop support for 32-bit hardware.[62]
,,,"Windows CE (officially known as Windows Embedded Compact), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so."
,,,"Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8."
,,,"Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel."
,,,"Xbox OS is an unofficial name given to the version of Windows that runs on Xbox consoles.[63] From Xbox One onwards it is an implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.[64]Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC.[65] It was originally based on NT 6.2 (Windows 8) kernel, and the latest version runs on an NT 10.0 base. This system is sometimes referred to as ""Windows 10 on Xbox One"" or ""OneCore"".[66][67]Xbox One and Xbox Series operating systems also allow limited (due to licensing restrictions and testing resources) backward compatibility with previous generation hardware,[68] and the Xbox 360's system is backwards compatible with the original Xbox.[69]"
,,,"Up to and including every version before Windows 2000, Microsoft used an in-house version control system named Source Library Manager (SLM). Shortly after Windows 2000 was released, Microsoft switched to a fork of Perforce named Source Depot.[70] This system was used up until 2017 once the system couldn't keep up with the size of Windows. Microsoft had begun to integrate Git into Team Foundation Server in 2013, but Windows continued to rely on Source Depot.[citation needed] The Windows code was divided among 65 different repositories with a kind of virtualization layer to produce unified view of all of the code."
,,,"In 2017 Microsoft announced that it would start using Git, an open source version control system created by Linus Torvalds and in May 2017 they reported that has completed migration into the Git repository.[71][72][50]"
,,,"Because of its large, decades-long history, however, the Windows codebase is not especially well suited to the decentralized nature of Linux development that Git was originally created to manage.[citation needed] Each Git repository contains a complete history of all the files, which proved unworkable for Windows developers because cloning the whole repository takes several hours.[citation needed] Microsoft has been working on a new project called the Virtual File System for Git (VFSForGit) to address these challenges.[72]"
,,,In 2021 the VFS for Git has been superseded by Scalar.[73]
,,,"Version market shareAs a percentage of desktop and laptop systems using Windows,[76] according to StatCounter data from April 2022.[77]"
,,,Use of the latest version Windows 10 has exceeded Windows 7 globally since early 2018.[78]
,,,"For desktop and laptop computers, according to Net Applications and StatCounter, which track the use of operating systems in devices that are active on the Web, Windows was the most used operating-system family in August 2021, with around 91% usage share according to Net Applications[79] and around 76% usage share according to StatCounter.[80]"
,,,"Including personal computers of all kinds (e.g., desktops, laptops, mobile devices, and game consoles), Windows OSes accounted for 32.67% of usage share in August 2021, compared to Android (highest, at 46.03%), iOS's 13.76%, iPadOS's 2.81%, and macOS's 2.51%, according to Net Applications[81] and 30.73% of usage share in August 2021, compared to Android (highest, at 42.56%), iOS/iPadOS's 16.53%, and macOS's 6.51%, according to StatCounter.[82]"
,,,"Those statistics do not include servers (including so-called cloud computing, where Microsoft is known not to be a leader, with Linux used more than Windows), as Net Applications and StatCounter use web browsing as a proxy for all use."
,,,"Consumer versions of Windows were originally designed for ease-of-use on a single-user PC without a network connection, and did not have security features built in from the outset.[83] However, Windows NT and its successors are designed for security (including on a network) and multi-user PCs, but were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.[84]"
,,,"These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's Counterpane Internet Security reported that it had seen over 1,000 new viruses and worms in the previous six months.[85] In 2005, Kaspersky Lab found around 11,000 malicious programs –  viruses, Trojans, back-doors, and exploits written for Windows.[86]"
,,,"Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary.[87] In versions of Windows after and including Windows 2000 SP3 and Windows XP, updates can be automatically downloaded and installed if the user selects to do so. As a result, Service Pack 2 for Windows XP, as well as Service Pack 1 for Windows Server 2003, were installed by users more quickly than it otherwise might have been.[88]"
,,,"While the Windows 9x series offered the option of having profiles for multiple users, they had no concept of access privileges, and did not allow concurrent access; and so were not true multi-user operating systems. In addition, they implemented only partial memory protection. They were accordingly widely criticised for lack of security."
,,,"The Windows NT series of operating systems, by contrast, are true multi-user, and implement absolute memory protection. However, a lot of the advantages of being a true multi-user operating system were nullified by the fact that, prior to Windows Vista, the first user account created during the setup process was an administrator account, which was also the default for new accounts. Though Windows XP did have limited accounts, the majority of home users did not change to an account type with fewer rights – partially due to the number of programs which unnecessarily required administrator rights – and so most home users ran as administrator all the time."
,,,"Windows Vista changes this[89] by introducing a privilege elevation system called User Account Control. When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or ""Run as administrator"" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.[90]"
,,,"Leaked documents published by WikiLeaks, codenamed Vault 7 and dated from 2013 to 2016, detail the capabilities of the CIA to perform electronic surveillance and cyber warfare,[91] such as the ability to compromise operating systems such as Microsoft Windows.[92]"
,,,"In August 2019, computer experts reported that the BlueKeep security vulnerability, CVE-2019-0708, that potentially affects older unpatched Microsoft Windows versions via the program's Remote Desktop Protocol, allowing for the possibility of remote code execution, may now include related flaws, collectively named DejaBlue, affecting newer Windows versions (i.e., Windows 7 and all recent versions) as well.[93] In addition, experts reported a Microsoft security vulnerability, CVE-2019-1162, based on legacy code involving Microsoft CTF and ctfmon (ctfmon.exe), that affects all Windows versions from the older Windows XP version to the most recent Windows 10 versions; a patch to correct the flaw is currently available.[94]"
,,,"All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGDLP (Accounts, Global, Domain Local, Permissions) in which file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directly to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders."
,,,"Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:"
,,,"Steven Paul Jobs (February 24, 1955 – October 5, 2011) was an American entrepreneur, inventor, business magnate, media proprietor, and investor. He was the co-founder, chairman, and CEO of Apple; the chairman and majority shareholder of Pixar; a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of NeXT. He is widely recognized as a pioneer of the personal computer revolution of the 1970s and 1980s, along with his early business partner and fellow Apple co-founder Steve Wozniak."
,,,"Born in San Francisco to a Syrian father and a German-American mother, Jobs was adopted shortly after his birth. Jobs attended Reed College in 1972 before withdrawing that same year, and traveled through India in 1974 seeking enlightenment and studying Zen Buddhism. He and Wozniak co-founded Apple in 1976 to sell Wozniak's Apple I personal computer. Together, the duo gained fame and wealth a year later with the Apple II, one of the first highly successful mass-produced microcomputers. Jobs saw the commercial potential of the Xerox Alto in 1979, which was mouse-driven and had a graphical user interface (GUI). This led to the development of the unsuccessful Apple Lisa in 1983, followed by the breakthrough Macintosh in 1984, the first mass-produced computer with a GUI. The Macintosh introduced the desktop publishing industry in 1985 with the addition of the Apple LaserWriter, the first laser printer to feature vector graphics."
,,,"Jobs was forced out of Apple in 1985 after a long power struggle with the company's board and its then-CEO John Sculley. That same year, Jobs took a few Apple employees with him to found NeXT, a computer platform development company that specialized in computers for higher-education and business markets. In addition, he helped to develop the visual effects industry when he funded the computer graphics division of George Lucas's Lucasfilm in 1986. The new company was Pixar, which produced the first 3D computer animated feature film Toy Story (1995) and went on to become a major animation studio, producing over 20 films since."
,,,"Jobs became CEO of Apple in 1997, following the company's acquisition of NeXT. He was largely responsible for helping revive Apple, which had been on the verge of bankruptcy. He worked closely with English designer Jony Ive to develop a line of products that had larger cultural ramifications, beginning in 1997 with the ""Think different"" advertising campaign and leading to the Apple Store, App Store, iMac, iPad, iPod, iPhone, iTunes, and iTunes Store. In 2001, the original Mac OS was replaced with the completely new Mac OS X (now known as macOS), based on NeXT's NeXTSTEP platform, giving the OS a modern Unix-based foundation for the first time. Jobs was diagnosed with a pancreatic neuroendocrine tumor in 2003. He died of respiratory arrest related to the tumor at age 56 on October 5, 2011."
,,,"Steven Paul Jobs was born in San Francisco, California, on February 24, 1955, the son of Joanne Carole Schieble and Abdulfattah Jandali (Arabic: عبد الفتاح الجندلي). He was adopted by Clara (née Hagopian) and Paul Reinhold Jobs.[2]"
,,,"Jandali, Jobs's biological father, was Syrian and went by the name ""John"". He grew up in an Arab Muslim household in Homs, Syria.[3] While an undergraduate at the American University of Beirut in Lebanon, he was a student activist and spent time in prison for his political activities.[3] He pursued a PhD at the University of Wisconsin, where he met Schieble, an American Catholic of German and Swiss descent.[3][4] As a doctoral candidate, Jandali was a teaching assistant for a course Schieble was taking, although both were the same age.[5] Novelist Mona Simpson, Jobs's biological sister, noted that Schieble's parents were not happy that their daughter was dating a Muslim.[6] Walter Isaacson, author of the biography Steve Jobs, additionally states that Schieble's father ""threatened to cut her off completely"" if she continued the relationship.[4]"
,,,"Jobs's adoptive father was a Coast Guard mechanic.[7] After leaving the Coast Guard, he married Hagopian, an American of Armenian descent, in 1946.[8] Their attempts to start a family were halted after Hagopian had an ectopic pregnancy, leading them to consider adoption in 1955.[7][8][9] Hagopian's parents were survivors of the Armenian Genocide.[10]"
,,,"""Of all the inventions of humans, the computer is going to rank near or at the top as history unfolds and we look back. It is the most awesome tool that we have ever invented. I feel incredibly lucky to be at exactly the right place in Silicon Valley, at exactly the right time, historically, where this invention has taken form."""
,,,"—Steve Jobs, 1995. From the documentary, Steve Jobs: The Lost Interview.[11]"
,,,"Schieble became pregnant with Jobs in 1954, when she and Jandali spent the summer with his family in Homs. According to Jandali, Schieble deliberately did not involve him in the process: ""Without telling me, Joanne upped and left to move to San Francisco to have the baby without anyone knowing, including me.""[12]"
,,,"Schieble gave birth to Jobs in San Francisco on February 24, 1955, and chose an adoptive couple for him that was ""Catholic, well-educated, and wealthy"",[13][14] but the couple later changed their mind.[13] Jobs was then placed with Paul and Clara Jobs, neither of whom had a college education, and Schieble refused to sign the adoption papers.[15] She then took the matter to court in an attempt to have her baby placed with a different family,[13] and only consented to releasing the baby to Paul and Clara after the couple pledged to pay for the boy's college education.[16] Jobs's cousin, Bassma Al Jandaly, maintains that Jobs's birth name was Abdul Lateef Jandali.[17]"
,,,"In his youth, Steve's parents took him to a Lutheran church.[18] When Jobs was in high school, Clara admitted to his girlfriend, Chrisann Brennan, that she ""was too frightened to love [Steve] for the first six months of his life ... I was scared they were going to take him away from me. Even after we won the case, Steve was so difficult a child that by the time he was two I felt we had made a mistake. I wanted to return him.""[13] When Chrisann shared this comment with Steve, he stated that he was already aware,[13] and would later say he was deeply loved and indulged by Paul and Clara.[19][page needed] Many years later, Jobs's wife Laurene also noted that ""he felt he had been really blessed by having the two of them as parents.""[19][page needed] Jobs would become upset when Paul and Clara were referred to as his ""adoptive parents""; he regarded them as his parents ""1,000%"". With regard to his biological parents, Jobs referred to them as ""my sperm and egg bank. That's not harsh, it's just the way it was, a sperm bank thing, nothing more.""[7]"
,,,"""I always thought of myself as a humanities person as a kid, but I liked electronics… then I read something that one of my heroes, Edwin Land of Polaroid, said about the importance of people who could stand at the intersection of humanities and sciences, and I decided that's what I wanted to do."""
,,,From Steve Jobs[20]
,,,"Paul Jobs worked in several jobs that included a try as a machinist,[21]several other jobs,[22] and then ""back to work as a machinist."""
,,,"Paul and Clara adopted Jobs's sister Patricia in 1957[23] and by 1959 the family had moved to the Monta Loma neighborhood in Mountain View, California.[24] It was during this time that Paul built a workbench in his garage for his son in order to ""pass along his love of mechanics.""[25] Jobs, meanwhile, admired his father's craftsmanship ""because he knew how to build anything. If we needed a cabinet, he would build it. When he built our fence, he gave me a hammer so I could work with him ... I wasn't that into fixing cars ... but I was eager to hang out with my dad.""[25] By the time he was ten, Jobs was deeply involved in electronics and befriended many of the engineers who lived in the neighborhood.[26][page needed] He had difficulty making friends with children his own age, however, and was seen by his classmates as a ""loner.""[26][page needed]"
,,,"Jobs had difficulty functioning in a traditional classroom, tended to resist authority figures, frequently misbehaved, and was suspended a few times.[26][page needed] Clara had taught him to read as a toddler, and Jobs stated that he was ""pretty bored in school and [had] turned into a little terror... you should have seen us in the third grade, we basically destroyed the teacher.""[26][page needed] He frequently played pranks on others at Monta Loma Elementary School in Mountain View.[28] His father Paul (who was abused as a child) never reprimanded him, however, and instead blamed the school for not challenging his brilliant son.[28]"
,,,"Jobs would later credit his fourth grade teacher, Imogene ""Teddy"" Hill, with turning him around: ""She taught an advanced fourth grade class and it took her about a month to get hip to my situation. She bribed me into learning. She would say, 'I really want you to finish this workbook. I'll give you five bucks if you finish it.' That really kindled a passion in me for learning things! I learned more that year than I think I learned in any other year in school. They wanted me to skip the next two years in grade school and go straight to junior high to learn a foreign language but my parents very wisely wouldn't let it happen.""[26][page needed] Jobs skipped the 5th grade and transferred to the 6th grade at Crittenden Middle School in Mountain View[26][page needed] where he became a ""socially awkward loner"".[29] Jobs was often ""bullied"" at Crittenden Middle, and in the middle of 7th grade, he gave his parents an ultimatum: they had to either take him out of Crittenden or he would drop out of school.[30]"
,,,"Though the Jobs family was not well off, they used all their savings in 1967 to buy a new home, allowing Jobs to change schools.[26][page needed] The new house (a three-bedroom home on Crist Drive in Los Altos, California) was in the better Cupertino School District, Cupertino, California,[31] and was embedded in an environment that was even more heavily populated with engineering families than the Mountain View area was.[26][page needed] The house was declared a historic site in 2013, as it was the first site for Apple Computer;[27] as of 2013, it was owned by Jobs's sister, Patty, and occupied by his step-mother, Marilyn.[32]"
,,,"When he was 13 in 1968, Jobs was given a summer job by Bill Hewlett (of Hewlett-Packard) after Jobs cold-called him to ask for parts for an electronics project.[26][page needed]"
,,,"The location of the Los Altos home meant that Jobs would be able to attend nearby Homestead High School, which had strong ties to Silicon Valley.[20] He began his first year there in late 1968 along with Bill Fernandez.[26][page needed] (Fernandez introduced Jobs to Steve Wozniak, and would later be Apple's first employee.) Neither Jobs nor Fernandez (whose father was a lawyer) came from engineering households and thus decided to enroll in John McCollum's ""Electronics 1.""[26][page needed] McCollum and the rebellious Jobs (who had grown his hair long and become involved in the growing counterculture) would eventually clash and Jobs began to lose interest in the class.[26][page needed]"
,,,"He underwent a change during mid-1970: ""I got stoned for the first time; I discovered Shakespeare, Dylan Thomas, and all that classic stuff. I read Moby Dick and went back as a junior taking creative writing classes.""[26][page needed] Jobs also later noted to his official biographer that ""I started to listen to music a whole lot, and I started to read more outside of just science and technology—Shakespeare, Plato. I loved King Lear ... when I was a senior I had this phenomenal AP English class. The teacher was this guy who looked like Ernest Hemingway. He took a bunch of us snowshoeing in Yosemite.""[33] During his last two years at Homestead High, Jobs developed two different interests: electronics and literature.[33] These dual interests were particularly reflected during Jobs's senior year as his best friends were Wozniak and his first girlfriend, the artistic Homestead junior Chrisann Brennan.[citation needed]"
,,,"In 1971 after Wozniak began attending University of California, Berkeley, Jobs would visit him there a few times a week. This experience led him to study in nearby Stanford University's student union. Jobs also decided that rather than join the electronics club, he would put on light shows with a friend for Homestead's avant-garde Jazz program. He was described by a Homestead classmate as ""kind of a brain and kind of a hippie ... but he never fit into either group. He was smart enough to be a nerd, but wasn't nerdy. And he was too intellectual for the hippies, who just wanted to get wasted all the time. He was kind of an outsider. In high school everything revolved around what group you were in, and if you weren't in a carefully defined group, you weren't anybody. He was an individual, in a world where individuality was suspect."" By his senior year in late 1971, he was taking freshman English class at Stanford and working on a Homestead underground film project with Chrisann Brennan.[26][page needed]"
,,,"Around that time, Wozniak designed a low-cost digital ""blue box"" to generate the necessary tones to manipulate the telephone network, allowing free long-distance calls. Jobs decided then to sell them and split the profit with Wozniak. The clandestine sales of the illegal blue boxes went well and perhaps planted the seed in Jobs's mind that electronics could be both fun and profitable.[34] Jobs, in a 1994 interview, recalled that it took six months for him and Wozniak to figure out how to build the blue boxes.[35] Jobs later reflected that had it not been for Wozniak's blue boxes, ""there wouldn't have been an Apple"".[36] He states it showed them that they could take on large companies and beat them.[37][38]"
,,,"By his senior year of high school, Jobs began using LSD.[39] He later recalled that on one occasion he consumed it in a wheat field outside Sunnyvale, and experienced ""the most wonderful feeling of my life up to that point"".[40] In mid-1972, after graduation and before leaving for Reed College, Jobs and Brennan rented a house from their other roommate, Al.[41]"
,,,"""I was interested in Eastern mysticism which hit the shores about then. At Reed there was a constant flow of people stopping by – from Timothy Leary and Richard Alpert, to Gary Snyder. There was a constant flow of intellectual questioning about the truth of life. That was the time when every college student in the country read Be Here Now and Diet for a Small Planet."""
,,,—Steve Jobs[26][page needed]
,,,"In September 1972, Jobs enrolled at Reed College in Portland, Oregon.[42] He insisted on applying only to Reed although it was an expensive school that Paul and Clara could ill afford.[43] Jobs soon befriended Robert Friedland,[44] who was Reed's student body president at that time.[26][page needed] Brennan remained involved with Jobs while he was at Reed. He later asked her to come and live with him in a house he rented near the Reed campus, but she refused."
,,,"After just one semester, Jobs dropped out of Reed College without telling his parents.[45] Jobs later explained that he decided to drop out because he did not want to spend his parents' money on an education that seemed meaningless to him.[46] He continued to attend by auditing his classes,[46] which included a course on calligraphy that was taught by Robert Palladino. In a 2005 commencement speech at Stanford University, Jobs stated that during this period, he slept on the floor in friends' dorm rooms, returned Coke bottles for food money, and got weekly free meals at the local Hare Krishna temple. In that same speech, Jobs said: ""If I had never dropped in on that single calligraphy course in college, the Mac would have never had multiple typefaces or proportionally spaced fonts.""[47]"
,,,"I was lucky to get into computers when it was a very young and idealistic industry. There weren't many degrees offered in computer science, so people in computers were brilliant people from mathematics, physics, music, zoology, whatever. They loved it, and no one was really in it for the money [...] There are people around here who start companies just to make money, but the great companies, well, that's not what they're about."""
,,,—Steve Jobs[48]
,,,"In February 1974, Jobs returned to his parents' home in Los Altos and began looking for a job.[49] He was soon hired by Atari, Inc. in Los Gatos, California, which gave him a job as a technician.[49][50] Back in 1973, Steve Wozniak designed his own version of the classic video game Pong and gave the board to Jobs. According to Wozniak, Atari only hired Jobs because he took the board down to the company, and they thought that he had built it himself.[51] Atari's cofounder Nolan Bushnell later described him as ""difficult but valuable"", pointing out that ""he was very often the smartest guy in the room, and he would let people know that.""[52]"
,,,"During this period, Jobs and Brennan remained involved with each other while continuing to see other people. By early 1974, Jobs was living what Brennan describes as a ""simple life"" in a Los Gatos cabin, working at Atari, and saving money for his impending trip to India.[citation needed]"
,,,"Jobs traveled to India in mid-1974[53] to visit Neem Karoli Baba[54] at his Kainchi ashram with his Reed friend (and eventual Apple employee) Daniel Kottke, in search of spiritual enlightenment. When they got to the Neem Karoli ashram, it was almost deserted because Neem Karoli Baba had died in September 1973.[50] Then they made a long trek up a dry riverbed to an ashram of Haidakhan Babaji.[50]"
,,,"After seven months, Jobs left India[55] and returned to the US ahead of Daniel Kottke.[50] Jobs had changed his appearance; his head was shaved and he wore traditional Indian clothing.[56][57] During this time, Jobs experimented with psychedelics, later calling his LSD experiences ""one of the two or three most important things [he had] done in [his] life"".[58][59] He spent a period at the All One Farm, a commune in Oregon that was owned by Robert Friedland. Brennan joined him there for a period.[citation needed]"
,,,"During this time period, Jobs and Brennan both became practitioners of Zen Buddhism through the Zen master Kōbun Chino Otogawa. Jobs was living in his parents' backyard toolshed, which he had converted into a bedroom.[citation needed] Jobs engaged in lengthy meditation retreats at the Tassajara Zen Mountain Center, the oldest Sōtō Zen monastery in the US.[60] He considered taking up monastic residence at Eihei-ji in Japan, and maintained a lifelong appreciation for Zen.[61]"
,,,"In mid-1975, after returning to Atari, Jobs was assigned to create a circuit board for the arcade video game Breakout.[62] According to Bushnell, Atari offered US$100 for each TTL chip that was eliminated in the machine. Jobs had little specialized knowledge of circuit board design and made a deal with Wozniak to split the fee evenly between them if Wozniak could minimize the number of chips. Much to the amazement of Atari engineers, Wozniak reduced the TTL count to 46, a design so tight that it was impossible to reproduce on an assembly line.[63] According to Wozniak, Jobs told him that Atari gave them only $700 (instead of the $5,000 paid out), and that Wozniak's share was thus $350.[64] Wozniak did not learn about the actual bonus until ten years later, but said that if Jobs had told him about it and explained that he needed the money, Wozniak would have given it to him.[65]"
,,,"Jobs and Wozniak attended meetings of the Homebrew Computer Club in 1975, which was a stepping stone to the development and marketing of the first Apple computer.[14]"
,,,"""Basically Steve Wozniak and I invented the Apple because we wanted a personal computer. Not only couldn't we afford the computers that were on the market, those computers were impractical for us to use. We needed a Volkswagen. The Volkswagen isn't as fast or comfortable as other ways of traveling, but the VW owners can go where they want, when they want and with whom they want. The VW owners have personal control of their car."""
,,,—Steve Jobs[26][page needed]
,,,"By March 1976, Wozniak completed the basic design of the Apple I computer and showed it to Jobs, who suggested that they sell it; Wozniak was at first skeptical of the idea but later agreed.[66] In April of that same year, Jobs, Wozniak, and administrative overseer Ronald Wayne founded Apple Computer Company (now called Apple Inc.) as a business partnership in Jobs's parents' Crist Drive home on April 1, 1976.[67] The operation originally started in Jobs's bedroom and later moved to the garage.[67][68] Wayne stayed only a short time, leaving Jobs and Wozniak as the active primary cofounders of the company.[69] The two decided on the name ""Apple"" after Jobs returned from the All One Farm commune in Oregon and told Wozniak about his time spent in the farm's apple orchard.[70] Jobs originally planned to produce bare printed circuit boards of the Apple I and sell them to computer hobbyists for $50 each.[71][72] To raise the money they needed to build the first batch of the circuit boards, Wozniak sold his HP scientific calculator and Jobs sold his Volkswagen van.[71][72] Later that year, computer retailer Paul Terrell purchased 50 fully assembled units of the Apple I from them for $500 each.[73][74] Eventually about 200 Apple I computers were produced in total.[75]"
,,,"A neighbor on Crist Drive recalled Jobs as an odd individual who would greet his clients ""with his underwear hanging out, barefoot and hippie-like"".[32] Another neighbor, Larry Waterland, who had just earned his PhD in chemical engineering at Stanford, recalled dismissing Jobs's budding business: ""'You punched cards, put them in a big deck,' he said about the mainframe machines of that time. 'Steve took me over to the garage. He had a circuit board with a chip on it, a DuMont TV set, a Panasonic cassette tape deck and a keyboard. He said, 'This is an Apple computer.' I said, 'You've got to be joking.' I dismissed the whole idea.'""[32] Jobs's friend from Reed College and India, Daniel Kottke, recalled that as an early Apple employee, he ""was the only person who worked in the garage ... Woz would show up once a week with his latest code. Steve Jobs didn't get his hands dirty in that sense."" Kottke also stated that much of the early work took place in Jobs's kitchen, where he spent hours on the phone trying to find investors for the company.[32]"
,,,"They received funding from a then-semi-retired Intel product marketing manager and engineer Mike Markkula.[76] Scott McNealy, one of the cofounders of Sun Microsystems, said that Jobs broke a ""glass age ceiling"" in Silicon Valley because he'd created a very successful company at a young age.[38] Markkula brought Apple to the attention of Arthur Rock, which after looking at the crowded Apple booth at the Home Brew Computer Show, started with a $60,000 investment and went on the Apple board.[77] Jobs was not pleased when Markkula recruited Mike Scott from National Semiconductor in February 1977 to serve as the first president and CEO of Apple.[78][79]"
,,,"""For what characterizes Apple is that its scientific staff always acted and performed like artists – in a field filled with dry personalities limited by the rational and binary worlds they inhabit, Apple's engineering teams had passion. They always believed that what they were doing was important and, most of all, fun. Working at Apple was never just a job; it was also a crusade, a mission, to bring better computer power to people. At its roots that attitude came from Steve Jobs. It was ""Power to the People"", the slogan of the sixties, rewritten in technology for the eighties and called Macintosh."""
,,,"—Jeffrey S. Young, 1987. From the book, Steve Jobs: The Journey is the Reward (published 1988).[26][page needed]"
,,,"After Brennan returned from her own journey to India, she and Jobs fell in love again, as Brennan noted changes in him that she attributes to Kobun (whom she was also still following). It was also at this time that Jobs displayed a prototype Apple I computer for Brennan and his parents in their living room. Brennan notes a shift in this time period, where the two main influences on Jobs were Apple Inc. and Kobun. By early 1977, she and Jobs would spend time together at her home at Duveneck Ranch in Los Altos, which served as a hostel and environmental education center."
,,,"In April 1977, Jobs and Wozniak introduced the Apple II at the West Coast Computer Faire.[80] It is the first consumer product to have been sold by Apple Computer. Primarily designed by Wozniak, Jobs oversaw the development of its unusual case and Rod Holt developed the unique power supply.[81] During the design stage, Jobs argued that the Apple II should have two expansion slots, while Wozniak wanted eight. After a heated argument, Wozniak threatened that Jobs should ""go get himself another computer"". They later decided to go with eight slots.[82] The Apple II became one of the first highly successful mass-produced microcomputer products in the world.[83]"
,,,"As Jobs became more successful with his new company, his relationship with Brennan grew more complex. In 1977, the success of Apple was now a part of their relationship, and Brennan, Daniel Kottke, and Jobs moved into a house near the Apple office in Cupertino.[citation needed] Brennan eventually took a position in the shipping department at Apple.[84] Brennan's relationship with Jobs deteriorated as his position with Apple grew, and she began to consider ending the relationship. In October 1977, Brennan was approached by Rod Holt, who asked her to take ""a paid apprenticeship designing blueprints for the Apples"".[citation needed] Both Holt and Jobs believed that it would be a good position for her, given her artistic abilities. Holt was particularly eager that she take the position and puzzled by her ambivalence toward it. Brennan's decision, however, was overshadowed by the fact that she realized she was pregnant and that Jobs was the father. It took her a few days to tell Jobs, whose face, according to Brennan ""turned ugly"" at the news. At the same time, according to Brennan, at the beginning of her third trimester, Jobs said to her: ""I never wanted to ask that you get an abortion. I just didn't want to do that.""[citation needed] He also refused to discuss the pregnancy with her.[85] Brennan turned down the internship and decided to leave Apple. She stated that Jobs told her ""If you give up this baby for adoption, you will be sorry"" and ""I am never going to help you.""[citation needed] According to Brennan, Jobs ""started to seed people with the notion that I slept around and he was infertile, which meant that this could not be his child."" A few weeks before she was due to give birth, Brennan was invited to deliver her baby at the All One Farm. She accepted the offer.[citation needed] When Jobs was 23 (the same age as his biological parents when they had him)[85] Brennan gave birth to her baby, Lisa Brennan, on May 17, 1978.[86] Jobs went there for the birth after he was contacted by Robert Friedland, their mutual friend and the farm owner. While distant, Jobs worked with her on a name for the baby, which they discussed while sitting in the fields on a blanket. Brennan suggested the name ""Lisa"" which Jobs also liked and notes that Jobs was very attached to the name ""Lisa"" while he ""was also publicly denying paternity."" She would discover later that during this time, Jobs was preparing to unveil a new kind of computer that he wanted to give a female name (his first choice was ""Claire"" after St. Clare). She also stated that she never gave him permission to use the baby's name for a computer and he hid the plans from her. Jobs also worked with his team to come up with the phrase, ""Local Integrated Software Architecture"" as an alternative explanation for the Apple Lisa.[87] Decades later, however, Jobs admitted to his biographer Walter Isaacson that ""obviously, it was named for my daughter"".[88]"
,,,"When Jobs denied paternity, a DNA test established him as Lisa's father.[clarification needed] It required him to give Brennan $385 a month in addition to returning the welfare money she had received. Jobs gave her $500 a month at the time when Apple went public and Jobs became a millionaire. Later, Brennan agreed to give an interview with Michael Moritz for Time magazine for its Time Person of the Year special, released on January 3, 1983, in which she discussed her relationship with Jobs. Rather than name Jobs the Person of the Year, the magazine named the computer[clarification needed] the ""Machine of the Year"".[89] In the issue, Jobs questioned the reliability of the paternity test (which stated that the ""probability of paternity for Jobs, Steven... is 94.1%"").[90] Jobs responded by arguing that ""28% of the male population of the United States could be the father"".[90] Time also noted that ""the baby girl and the machine on which Apple has placed so much hope for the future share the same name: Lisa"".[90]"
,,,"Jobs was worth over $1 million in 1978, when he was just 23 years old. His net worth grew to over $250 million by the time he was 25, according to estimates.[91] He was also one of the youngest ""people ever to make the Forbes list of the nation's richest people—and one of only a handful to have done it themselves, without inherited wealth"".[26][page needed]"
,,,"In 1982, Jobs bought an apartment on the top two floors of The San Remo, a Manhattan building with a politically progressive reputation. Although he never lived there,[92] he spent years renovating it with the help of I. M. Pei. In 2003, he sold it to U2 singer Bono."
,,,"In 1983, Jobs lured John Sculley away from Pepsi-Cola to serve as Apple's CEO, asking, ""Do you want to spend the rest of your life selling sugared water, or do you want a chance to change the world?""[93]"
,,,"In 1984, Jobs bought the Jackling House and estate, and resided there for a decade. After that, he leased it out for several years until 2000 when he stopped maintaining the house, allowing exposure to the weather to degrade it. In 2004, Jobs received permission from the town of Woodside to demolish the house in order to build a smaller contemporary styled one. After a few years in court, the house was finally demolished in 2011, a few months before he died.[94]"
,,,"Jobs began directing the development of the Macintosh in 1981, when he took over the project from early Apple employee Jef Raskin, who conceived the computer (Wozniak, who with Raskin had heavy influence over the program early on in its development, was on leave during this time due to an airplane crash earlier that year[95]).[96][97] On January 22, 1984, Apple aired a Super Bowl television commercial titled ""1984"", which ended with the words: ""On January 24th, Apple Computer will introduce Macintosh. And you'll see why 1984 won't be like 1984.""[98] On January 24, 1984, an emotional Jobs introduced the Macintosh to a wildly enthusiastic audience at Apple's annual shareholders meeting held in the Flint Auditorium;[99][100] Macintosh engineer Andy Hertzfeld described the scene as ""pandemonium"".[101] The Macintosh was based on The Lisa (and Xerox PARC's mouse-driven graphical user interface),[102][103] and it was widely acclaimed by the media with strong initial sales supporting it.[104][105] However, the computer's slow processing speed and limited range of available software led to a rapid sales decline in the second half of 1984.[104][105][106]"
,,,"Sculley's and Jobs's respective visions for the company greatly differed. The former favored open architecture computers like the Apple II, sold to education, small business, and home markets less vulnerable to IBM. Jobs wanted the company to focus on the closed architecture Macintosh as a business alternative to the IBM PC. President and CEO Sculley had little control over chairman of the board Jobs's Macintosh division; it and the Apple II division operated like separate companies, duplicating services.[108] Although its products provided 85 percent of Apple's sales in early 1985, the company's January 1985 annual meeting did not mention the Apple II division or employees. Many left, including Wozniak, who stated that the company had ""been going in the wrong direction for the last five years"" and sold most of his stock.[109] Despite being frustrated with the company's (including Jobs himself) dismissal of the Apple II employees in favor of the Macintosh, Wozniak left amicably and remained an honorary employee of Apple, maintaining a friendship with Jobs until his death.[110][111][112]"
,,,"By early 1985, the Macintosh's failure to defeat the IBM PC became clear,[104][105] and it strengthened Sculley's position in the company. In May 1985, Sculley—encouraged by Arthur Rock—decided to reorganize Apple, and proposed a plan to the board that would remove Jobs from the Macintosh group and put him in charge of ""New Product Development"". This move would effectively render Jobs powerless within Apple.[26][page needed] In response, Jobs then developed a plan to get rid of Sculley and take over Apple. However, Jobs was confronted after the plan was leaked, and he said that he would leave Apple. The Board declined his resignation and asked him to reconsider. Sculley also told Jobs that he had all of the votes needed to go ahead with the reorganization. A few months later, on September 17, 1985, Jobs submitted a letter of resignation to the Apple Board. Five additional senior Apple employees also resigned and joined Jobs in his new venture, NeXT.[26][page needed]"
,,,"The Macintosh's struggle continued after Jobs left Apple. Though marketed and received in fanfare, the expensive Macintosh was a hard sell.[113]: 308–309  In 1985, Bill Gates's then-developing company, Microsoft, threatened to stop developing Mac applications unless it was granted ""a license for the Mac operating system software. Microsoft was developing its graphical user interface ... for DOS, which it was calling Windows and didn't want Apple to sue over the similarities between the Windows GUI and the Mac interface.""[113]: 321  Sculley granted Microsoft the license which later led to problems for Apple.[113]: 321  In addition, cheap IBM PC clones that ran on Microsoft software and had a graphical user interface began to appear. Although the Macintosh preceded the clones, it was far more expensive, so ""through the late 1980s, the Windows user interface was getting better and better and was thus taking increasingly more share from Apple"".[113]: 322  Windows-based IBM-PC clones also led to the development of additional GUIs such as IBM's TopView or Digital Research's GEM,[113]: 322  and thus ""the graphical user interface was beginning to be taken for granted, undermining the most apparent advantage of the Mac...it seemed clear as the 1980s wound down that Apple couldn't go it alone indefinitely against the whole IBM-clone market.""[113]: 322 "
,,,"Following his resignation from Apple in 1985, Jobs founded NeXT Inc.[114] with $7 million. A year later he was running out of money, and he sought venture capital with no product on the horizon. Eventually, Jobs attracted the attention of billionaire Ross Perot, who invested heavily in the company.[115] The NeXT computer was shown to the world in what was considered Jobs's comeback event,[116] a lavish invitation-only gala launch event[117] that was described as a multimedia extravaganza.[118] The celebration was held at the Louise M. Davies Symphony Hall, San Francisco, California, on Wednesday, October 12, 1988. Steve Wozniak said in a 2013 interview that while Jobs was at NeXT he was ""really getting his head together"".[95]"
,,,"NeXT workstations were first released in 1990 and priced at US$9,999. Like the Apple Lisa, the NeXT workstation was technologically advanced and designed for the education sector, but was largely dismissed as cost-prohibitive for educational institutions.[119] The NeXT workstation was known for its technical strengths, chief among them its object-oriented software development system. Jobs marketed NeXT products to the financial, scientific, and academic community, highlighting its innovative, experimental new technologies, such as the Mach kernel, the digital signal processor chip, and the built-in Ethernet port. Making use of a NeXT computer, English computer scientist Tim Berners-Lee invented the World Wide Web in 1990 at CERN in Switzerland.[120]"
,,,"The revised, second generation NeXTcube was released in 1990. Jobs touted it as the first ""interpersonal"" computer that would replace the personal computer. With its innovative NeXTMail multimedia email system, NeXTcube could share voice, image, graphics, and video in email for the first time. ""Interpersonal computing is going to revolutionize human communications and groupwork"", Jobs told reporters.[121] Jobs ran NeXT with an obsession for aesthetic perfection, as evidenced by the development of and attention to NeXTcube's magnesium case.[122] This put considerable strain on NeXT's hardware division, and in 1993, after having sold only 50,000 machines, NeXT transitioned fully to software development with the release of NeXTSTEP/Intel.[123] The company reported its first yearly profit of $1.03 million in 1994.[124] In 1996, NeXT Software, Inc. released WebObjects, a framework for Web application development. After NeXT was acquired by Apple Inc. in 1997, WebObjects was used to build and run the Apple Store,[123] MobileMe services, and the iTunes Store."
,,,"In 1986, Jobs funded the spinout of The Graphics Group (later renamed Pixar) from Lucasfilm's computer graphics division for the price of $10 million, $5 million of which was given to the company as capital and $5 million of which was paid to Lucasfilm for technology rights.[125]"
,,,"The first film produced by Pixar with its Disney partnership, Toy Story (1995), with Jobs credited as executive producer,[citation needed] brought financial success and critical acclaim to the studio when it was released. Over the course of Jobs's life, under Pixar's creative chief John Lasseter, the company produced box-office hits A Bug's Life (1998); Toy Story 2 (1999); Monsters, Inc. (2001); Finding Nemo (2003); The Incredibles (2004); Cars (2006); Ratatouille (2007); WALL-E (2008); Up (2009); Toy Story 3 (2010); and Cars 2 (2011). Brave (2012), Pixar's first film to be produced since Jobs's death, honored him with a tribute for his contributions to the studio.[126] Finding Nemo, The Incredibles, Ratatouille, WALL-E, Up, Toy Story 3 and Brave each received the Academy Award for Best Animated Feature, an award introduced in 2001.[127][128]"
,,,"In 2003 and 2004, as Pixar's contract with Disney was running out, Jobs and Disney chief executive Michael Eisner tried but failed to negotiate a new partnership,[129] and in January 2004, Jobs announced that he would never deal with Disney again.[130] Pixar would seek a new partner to distribute its films after its contract expired."
,,,"In October 2005, Bob Iger replaced Eisner at Disney, and Iger quickly worked to mend relations with Jobs and Pixar. On January 24, 2006, Jobs and Iger announced that Disney had agreed to purchase Pixar in an all-stock transaction worth $7.4 billion. When the deal closed, Jobs became The Walt Disney Company's largest single shareholder with approximately seven percent of the company's stock.[131] Jobs's holdings in Disney far exceeded those of Eisner, who holds 1.7%, and of Disney family member Roy E. Disney, who until his 2009 death held about 1% of the company's stock and whose criticisms of Eisner—especially that he soured Disney's relationship with Pixar—accelerated Eisner's ousting. Upon completion of the merger, Jobs received 7% of Disney shares, and joined the board of directors as the largest individual shareholder.[131][132][133] Upon Jobs's death his shares in Disney were transferred to the Steven P. Jobs Trust led by Laurene Jobs.[134]"
,,,"After Jobs's death Iger recalled in 2019 that many warned him about Jobs, ""that he would bully me and everyone else"". Iger wrote, ""Who wouldn't want Steve Jobs to have influence over how a company is run?"", and that as an active Disney board member ""he rarely created trouble for me. Not never but rarely"". He speculated that they would have seriously considered merging Disney and Apple had Jobs lived.[130] Floyd Norman, of Pixar, described Jobs as a ""mature, mellow individual"" who never interfered with the creative process of the filmmakers.[135] In early June 2014, Pixar cofounder and Walt Disney Animation Studios President Ed Catmull revealed that Jobs once advised him to ""just explain it to them until they understand"" in disagreements. Catmull released the book Creativity, Inc. in 2014, in which he recounts numerous experiences of working with Jobs. Regarding his own manner of dealing with Jobs, Catmull writes:[136][page needed]"
,,,"In all the 26 years with Steve, Steve and I never had one of these loud verbal arguments and it's not my nature to do that. ... but we did disagree fairly frequently about things. ... I would say something to him and he would immediately shoot it down because he could think faster than I could. ... I would then wait a week ... I'd call him up and I give my counter argument to what he had said and he'd immediately shoot it down. So I had to wait another week, and sometimes this went on for months. But in the end one of three things happened. About a third of the time he said, 'Oh, I get it, you're right.' And that was the end of it. And it was another third of the time in which [I'd] say, 'Actually I think he is right.' The other third of the time, where we didn't reach consensus, he just let me do it my way, never said anything more about it.[137]"
,,,"In 1996, Apple announced that it would buy NeXT for $427 million. The deal was finalized in February 1997,[138] bringing Jobs back to the company he had cofounded. Jobs became de facto chief after then-CEO Gil Amelio was ousted in July 1997. He was formally named interim chief executive on September 16.[139] In March 1998, to concentrate Apple's efforts on returning to profitability, Jobs terminated a number of projects, such as Newton, Cyberdog, and OpenDoc. In the coming months, many employees developed a fear of encountering Jobs while riding in the elevator, ""afraid that they might not have a job when the doors opened. The reality was that Jobs's summary executions were rare, but a handful of victims was enough to terrorize a whole company.""[140] Jobs changed the licensing program for Macintosh clones, making it too costly for the manufacturers to continue making machines."
,,,"With the purchase of NeXT, much of the company's technology found its way into Apple products, most notably NeXTSTEP, which evolved into Mac OS X. Under Jobs's guidance, the company increased sales significantly with the introduction of the iMac and other new products; since then, appealing designs and powerful branding have worked well for Apple. At the 2000 Macworld Expo, Jobs officially dropped the ""interim"" modifier from his title at Apple and became permanent CEO.[141] Jobs quipped at the time that he would be using the title ""iCEO"".[142]"
,,,"The company subsequently branched out, introducing and improving upon other digital appliances. With the introduction of the iPod portable music player, iTunes digital music software, and the iTunes Store, the company made forays into consumer electronics and music distribution. On June 29, 2007, Apple entered the cellular phone business with the introduction of the iPhone, a multi-touch display cell phone, which also included the features of an iPod and, with its own mobile browser, revolutionized the mobile browsing scene. While nurturing open-ended innovation, Jobs also reminded his employees that ""real artists ship"".[143]"
,,,"Jobs had a public war of words with Dell Computer CEO Michael Dell, starting in 1987, when Jobs first criticized Dell for making ""un-innovative beige boxes"".[144] On October 6, 1997, at a Gartner Symposium, when Dell was asked what he would do if he ran the then-troubled Apple Computer company, he said: ""I'd shut it down and give the money back to the shareholders.""[145] Then, in 2006, Jobs sent an email to all employees when Apple's market capitalization rose above Dell's. It read:"
,,,"Team, it turned out that Michael Dell wasn't perfect at predicting the future. Based on today's stock market close, Apple is worth more than Dell. Stocks go up and down, and things may be different tomorrow, but I thought it was worth a moment of reflection today. Steve.[146]"
,,,"Jobs was both admired and criticized for his consummate skill at persuasion and salesmanship, which has been dubbed the ""reality distortion field"" and was particularly evident during his keynote speeches (colloquially known as ""Stevenotes"") at Macworld Expos and at Apple Worldwide Developers Conferences.[147]"
,,,"Jobs usually went to work wearing a black long-sleeved mock turtleneck made by Issey Miyake, Levi's 501 blue jeans, and New Balance 991 sneakers.[148][149] Jobs told his biographer Walter Isaacson ""...he came to like the idea of having a uniform for himself, both because of its daily convenience (the rationale he claimed) and its ability to convey a signature style.""[148]"
,,,Jobs was a board member at Gap Inc. from 1999 to 2002.[150]
,,,"In 2001, Jobs was granted stock options in the amount of 7.5 million shares of Apple with an exercise price of $18.30. It was alleged that the options had been backdated, and that the exercise price should have been $21.10. It was further alleged that Jobs had thereby incurred taxable income of $20,000,000 that he did not report, and that Apple overstated its earnings by that same amount. As a result, Jobs potentially faced a number of criminal charges and civil penalties. The case was the subject of active criminal and civil government investigations,[151] though an independent internal Apple investigation completed on December 29, 2006 found that Jobs was unaware of these issues and that the options granted to him were returned without being exercised in 2003.[152]"
,,,"In 2005, Jobs responded to criticism of Apple's poor recycling programs for e-waste in the US by lashing out at environmental and other advocates at Apple's annual meeting in Cupertino in April. A few weeks later, Apple announced it would take back iPods for free at its retail stores. The Computer TakeBack Campaign responded by flying a banner from a plane over the Stanford University graduation at which Jobs was the commencement speaker. The banner read ""Steve, don't be a mini-player—recycle all e-waste."""
,,,"In 2006, he further expanded Apple's recycling programs to any US customer who buys a new Mac. This program includes shipping and ""environmentally friendly disposal"" of their old systems.[153] The success of Apple's unique products and services provided several years of stable financial returns, propelling Apple to become the world's most valuable publicly traded company in 2011.[154]"
,,,"Jobs was perceived as a demanding perfectionist[155][156] who always aspired to position his businesses and their products at the forefront of the information technology industry by foreseeing and setting innovation and style trends. He summed up this self-concept at the end of his keynote speech at the Macworld Conference and Expo in January 2007, by quoting ice hockey player Wayne Gretzky:"
,,,"There's an old Wayne Gretzky quote that I love. ""I skate to where the puck is going to be, not where it has been."" And we've always tried to do that at Apple. Since the very, very beginning. And we always will.[157]"
,,,"On July 1, 2008, a US$7 billion class action suit was filed against several members of the Apple board of directors for revenue lost because of alleged securities fraud.[158][159]"
,,,"In a 2011 interview with biographer Walter Isaacson, Jobs revealed that he had met with US President Barack Obama, complained about the nation's shortage of software engineers, and told Obama that he was ""headed for a one-term presidency"".[160] Jobs proposed that any foreign student who got an engineering degree at a US university should automatically be offered a green card. After the meeting, Jobs commented, ""The president is very smart, but he kept explaining to us reasons why things can't get done . . . . It infuriates me.""[160]"
,,,"In October 2003, Jobs was diagnosed with cancer. In mid 2004, he announced to his employees that he had a cancerous tumor in his pancreas.[161] The prognosis for pancreatic cancer is usually very poor;[162] Jobs stated that he had a rare, much less aggressive type, known as islet cell neuroendocrine tumor.[161]"
,,,"Despite his diagnosis, Jobs resisted his doctors' recommendations for medical intervention for nine months,[163] instead relying on alternative medicine to thwart the disease. According to Harvard researcher Ramzi Amri, his choice of alternative treatment ""led to an unnecessarily early death"". Other doctors agree that Jobs's diet was insufficient to address his disease. However, cancer researcher and alternative medicine critic David Gorski wrote that ""it's impossible to know whether and by how much he might have decreased his chances of surviving his cancer through his flirtation with woo. My best guess was that Jobs probably only modestly decreased his chances of survival, if that.""[164][165] Barrie R. Cassileth, the chief of Memorial Sloan Kettering Cancer Center's integrative medicine department,[166] on the other hand, said, ""Jobs's faith in alternative medicine likely cost him his life ... He had the only kind of pancreatic cancer that is treatable and curable ... He essentially committed suicide.""[167] According to Jobs's biographer, Walter Isaacson, ""for nine months he refused to undergo surgery for his pancreatic cancer – a decision he later regretted as his health declined"".[168] ""Instead, he tried a vegan diet, acupuncture, herbal remedies, and other treatments he found online, and even consulted a psychic. He was also influenced by a doctor who ran a clinic that advised juice fasts, bowel cleansings and other unproven approaches, before finally having surgery in July 2004.""[169] He underwent a pancreaticoduodenectomy (or ""Whipple procedure"") that appeared to remove the tumor successfully.[170][171] Jobs did not receive chemotherapy or radiation therapy.[161][172] During Jobs's absence, Tim Cook, head of worldwide sales and operations at Apple, ran the company.[161]"
,,,"As of January 2006[update], only Jobs's wife, his doctors, and Iger and his wife knew that his cancer had returned. Jobs told Iger privately that he hoped to live to see his son Reed's high school graduation in 2010.[130] In early August 2006, Jobs delivered the keynote for Apple's annual Worldwide Developers Conference. His ""thin, almost gaunt"" appearance and unusually ""listless"" delivery,[173][174] together with his choice to delegate significant portions of his keynote to other presenters, inspired a flurry of media and internet speculation about the state of his health.[175] In contrast, according to an Ars Technica journal report, Worldwide Developers Conference (WWDC) attendees who saw Jobs in person said he ""looked fine"".[176] Following the keynote, an Apple spokesperson said that ""Steve's health is robust.""[177]"
,,,"Two years later, similar concerns followed Jobs's 2008 WWDC keynote address.[178] Apple officials stated that Jobs was victim to a ""common bug"" and was taking antibiotics,[179] while others surmised his cachectic appearance was due to the Whipple procedure.[172] During a July conference call discussing Apple earnings, participants responded to repeated questions about Jobs's health by insisting that it was a ""private matter"". Others said that shareholders had a right to know more, given Jobs's hands-on approach to running his company.[180][181] Based on an off-the-record phone conversation with Jobs, The New York Times reported, ""While his health problems amounted to a good deal more than 'a common bug', they weren't life-threatening and he doesn't have a recurrence of cancer.""[182]"
,,,"On August 28, 2008, Bloomberg mistakenly published a 2500-word obituary of Jobs in its corporate news service, containing blank spaces for his age and cause of death. News carriers customarily stockpile up-to-date obituaries to facilitate news delivery in the event of a well-known figure's death. Although the error was promptly rectified, many news carriers and blogs reported on it,[183] intensifying rumors concerning Jobs's health.[184] Jobs responded at Apple's September 2008 Let's Rock keynote by paraphrasing Mark Twain: ""Reports of my death are greatly exaggerated.""[185][186] At a subsequent media event, Jobs concluded his presentation with a slide reading ""110/70"", referring to his blood pressure, stating he would not address further questions about his health.[187]"
,,,"On December 16, 2008, Apple announced that marketing vice-president Phil Schiller would deliver the company's final keynote address at the Macworld Conference and Expo 2009, again reviving questions about Jobs's health.[188][189] In a statement given on January 5, 2009, on Apple.com, Jobs said that he had been suffering from a ""hormone imbalance"" for several months.[190][191]"
,,,"On January 14, 2009, Jobs wrote in an internal Apple memo that in the previous week he had ""learned that my health-related issues are more complex than I originally thought"".[192] He announced a six-month leave of absence until the end of June 2009, to allow him to better focus on his health. Tim Cook, who previously acted as CEO in Jobs's 2004 absence, became acting CEO of Apple, with Jobs still involved with ""major strategic decisions"".[192]"
,,,"In 2009, Tim Cook offered a portion of his liver to Jobs, since both share a rare blood type and the donor liver can regenerate tissue after such an operation. Jobs yelled, ""I'll never let you do that. I'll never do that.""[193]"
,,,"In April 2009, Jobs underwent a liver transplant at Methodist University Hospital Transplant Institute in Memphis, Tennessee.[194][195][196] Jobs's prognosis was described as ""excellent"".[194]"
,,,"On January 17, 2011, a year and a half after Jobs returned to work following the liver transplant, Apple announced that he had been granted a medical leave of absence. Jobs announced his leave in a letter to employees, stating his decision was made ""so he could focus on his health"". As it did at the time of his 2009 medical leave, Apple announced that Tim Cook would run day-to-day operations and that Jobs would continue to be involved in major strategic decisions at the company.[197][198] While on leave, Jobs appeared at the iPad 2 launch event on March 2, the WWDC keynote introducing iCloud on June 6, and before the Cupertino City Council on June 7.[199]"
,,,"On August 24, 2011, Jobs announced his resignation as Apple's CEO, writing to the board, ""I have always said if there ever came a day when I could no longer meet my duties and expectations as Apple's CEO, I would be the first to let you know. Unfortunately, that day has come.""[200] Jobs became chairman of the board and named Tim Cook as his successor as CEO.[201][202] Jobs continued to work for Apple until the day before his death six weeks later.[203][204][205]"
,,,"Jobs died at his Palo Alto, California, home around 3 p.m. (PDT) on October 5, 2011, due to complications from a relapse of his previously treated islet-cell pancreatic neuroendocrine tumor,[14][206][207] which resulted in respiratory arrest.[208] He had lost consciousness the day before and died with his wife, children, and sisters at his side.[209] His sister, Mona Simpson, described his death thus: ""Steve's final words, hours earlier, were monosyllables, repeated three times. Before embarking, he'd looked at his sister Patty, then for a long time at his children, then at his life's partner, Laurene, and then over their shoulders past them. Steve's final words were: 'Oh wow. Oh wow. Oh wow.'"" He then lost consciousness and died several hours later.[209] A small private funeral was held on October 7, 2011, the details of which, out of respect for Jobs's family, were not made public.[210]"
,,,"Apple[211] and Pixar each issued announcements of his death.[212] Apple announced on the same day that they had no plans for a public service, but were encouraging ""well-wishers"" to send their remembrance messages to an email address created to receive such messages.[213] Apple and Microsoft both flew their flags at half-staff throughout their respective headquarters and campuses.[214][215]"
,,,"Bob Iger ordered all Disney properties, including Walt Disney World and Disneyland, to fly their flags at half-staff from October 6 to 12, 2011.[216] For two weeks  following his death, Apple displayed on its corporate Web site a simple page that showed Jobs's name and lifespan next to his grayscale portrait.[217][218][219] On October 19, 2011, Apple employees held a private memorial service for Jobs on the Apple campus in Cupertino. Jobs's widow, Laurene, was in attendance, as well as Cook, Bill Campbell, Norah Jones, Al Gore, and Coldplay.[220] Some of Apple's retail stores closed briefly so employees could attend the memorial. A video of the service was uploaded to Apple's website.[220]"
,,,"California Governor Jerry Brown declared Sunday, October 16, 2011, to be ""Steve Jobs Day"".[221] On that day, an invitation-only memorial was held at Stanford University. Those in attendance included Apple and other tech company executives, members of the media, celebrities, close friends of Jobs, and politicians, along with Jobs's family. Bono, Yo-Yo Ma, and Joan Baez performed at the service, which lasted longer than an hour. The service was highly secured, with guards at all of the university's gates, and a helicopter flying overhead from an area news station.[222][223] Each attendee was given a small brown box as a ""farewell gift"" from Jobs. The box contained a copy of the Autobiography of a Yogi by Paramahansa Yogananda.[224]"
,,,"Childhood friend and fellow Apple co-founder Steve Wozniak,[225] former owner of what would become Pixar, George Lucas,[226] former rival, Microsoft co-founder Bill Gates,[227] and President Barack Obama[228] all offered statements in response to his death."
,,,"Per his request, Jobs is buried in an unmarked grave at Alta Mesa Memorial Park, the only nonsectarian cemetery in Palo Alto.[229][230]"
,,,"On October 7, 2021, Apple released a commemorative YouTube video on the tenth anniversary of Jobs's passing.[231]"
,,,"Jobs's design aesthetic was influenced by philosophies of Zen and Buddhism. In India, he experienced Buddhism while on his seven-month spiritual journey,[232] and his sense of intuition was influenced by the spiritual people with whom he studied.[232] He also learned from many references and sources, such as modernist architectural style of Joseph Eichler,[citation needed] and the industrial designs of Richard Sapper[233] and Dieter Rams.[citation needed]"
,,,"According to Apple co-founder Steve Wozniak, ""Steve didn't ever code. He wasn't an engineer and he didn't do any original design...""[234][235] Daniel Kottke, one of Apple's earliest employees and a college friend of Jobs's, stated: ""Between Woz and Jobs, Woz was the innovator, the inventor. Steve Jobs was the marketing person.""[236]"
,,,"He is listed as either primary inventor or co-inventor in 346 United States patents or patent applications related to a range of technologies from actual computer and portable devices to user interfaces (including touch-based), speakers, keyboards, power adapters, staircases, clasps, sleeves, lanyards and packages. Jobs's contributions to most of his patents were to ""the look and feel of the product"". His industrial design chief Jonathan Ive had his name along with Jobs's name for 200 of the patents.[237] Most of these are design patents (specific product designs; for example, Jobs listed as primary inventor in patents for both original and lamp-style iMacs, as well as PowerBook G4 Titanium) as opposed to utility patents (inventions).[238][239] He has 43 issued US patents on inventions.[238] The patent on the Mac OS X Dock user interface with ""magnification"" feature was issued the day before he died.[240] Although Jobs had little involvement in the engineering and technical side of the original Apple computers,[235] Jobs later used his CEO position to directly involve himself with product design.[241]"
,,,"Involved in many projects throughout his career was his long-time marketing executive and confidant Joanna Hoffman, known as one of the few employees at Apple and NeXT who could successfully stand up to Jobs while also engaging with him.[242]"
,,,"Even while terminally ill in the hospital, Jobs sketched new devices that would hold the iPad in a hospital bed.[209] He also despised the oxygen monitor on his finger, and suggested ways to revise the design for simplicity.[243]"
,,,"Since his death, the former Apple CEO has won 141 patents, more than most inventors win during their lifetimes. Currently, Jobs holds over 450 patents.[244]"
,,,"Although entirely designed by Steve Wozniak, Jobs had the idea of selling the desktop computer, which led to the formation of Apple Computer in 1976. Both Jobs and Wozniak constructed several of the first Apple I prototypes by hand, and sold some of their belongings in order to do so. Eventually, 200 units were produced.[75]"
,,,"The Apple II is an 8-bit home computer, one of the world's first highly successful mass-produced microcomputer products,[83] designed primarily by Wozniak (though Jobs oversaw the development of the Apple II's unusual case[245] and Rod Holt developed the unique power supply[81]). It was introduced in 1977 at the West Coast Computer Faire by Jobs and Wozniak and was the first consumer product sold by Apple."
,,,"The Lisa is a personal computer designed by Apple during the early 1980s. It was the first personal computer to offer a graphical user interface in a machine aimed at individual business users. Development of the Lisa began in 1978.[246] The Lisa sold poorly, with only 100,000 units sold.[247]"
,,,"In 1982, after Jobs was forced out of the Lisa project,[248] he joined the Macintosh project. The Macintosh is not a direct descendant of Lisa, although there are obvious similarities between the systems. The final revision, the Lisa 2/10, was modified and sold as the Macintosh XL.[249]"
,,,"Once he joined the original Macintosh team, Jobs took over the project after Wozniak had experienced a traumatic airplane accident and temporarily left the company.[95] Jobs introduced the Macintosh computer on January 24, 1984. This was the first mass-market personal computer featuring an integral graphical user interface and mouse.[250] This first model was later renamed to ""Macintosh 128k"" for uniqueness amongst a populous family of subsequently updated models which are also based on Apple's same proprietary architecture. Since 1998, Apple has largely phased out the Macintosh name in favor of ""Mac"", though the product family has been nicknamed ""Mac"" or ""the Mac"" since the development of the first model. The Macintosh was introduced by a US$1.5 million Ridley Scott television commercial, ""1984"".[251] It most notably aired during the third quarter of Super Bowl XVIII on January 22, 1984, and some people consider the ad a ""watershed event""[252] and a ""masterpiece"".[253] Regis McKenna called the ad ""more successful than the Mac itself"".[254] ""1984"" uses an unnamed heroine to represent the coming of the Macintosh (indicated by a Picasso-style picture of the computer on her white tank top) as a means of saving humanity from the conformity of IBM's attempts to dominate the computer industry. The ad alludes to George Orwell's novel, Nineteen Eighty-Four, which describes a dystopian future ruled by a televised ""Big Brother.""[255][256]"
,,,"The Macintosh, however, was expensive, which hindered its ability to be competitive in a market already dominated by the Commodore 64 for consumers, as well as the IBM Personal Computer and its accompanying clone market for businesses.[257] Macintosh systems still found success in education and desktop publishing and kept Apple as the second-largest PC manufacturer for the next decade."
,,,"After Jobs was forced out of Apple in 1985, he started NeXT, a workstation computer company. The NeXT Computer was introduced in 1988 at a lavish launch event. Using the NeXT Computer, Tim Berners-Lee created the world's first web browser, the WorldWideWeb. The NeXT Computer's operating system, named NeXTSTEP, begat Darwin, which is now the foundation of most of Apple's products such as Macintosh's macOS and iPhone's iOS.[258][259]"
,,,"Apple iMac G3 was introduced in 1998 and its innovative design was directly the result of Jobs's return to Apple. Apple boasted ""the back of our computer looks better than the front of anyone else's.""[260] Described as ""cartoonlike"", the first iMac, clad in Bondi Blue plastic, was unlike any personal computer that came before. In 1999, Apple introduced the Graphite gray Apple iMac and since has varied the shape, color and size considerably while maintaining the all-in-one design. Design ideas were intended to create a connection with the user such as the handle and a ""breathing"" light effect when the computer went to sleep.[261] The Apple iMac sold for $1,299 at that time. The iMac also featured forward-thinking changes, such as eschewing the floppy disk drive and moving exclusively to USB for connecting peripherals. This latter change resulted, through the iMac's success, in the interface being popularized among third-party peripheral makers—as evidenced by the fact that many early USB peripherals were made of translucent plastic (to match the iMac design).[262]"
,,,"iTunes is a media player, media library, online radio broadcaster, and mobile device management application developed by Apple. It is used to play, download, and organize digital audio and video (as well as other types of media available on the iTunes Store) on personal computers running the macOS and Microsoft Windows operating systems. The iTunes Store is also available on the iPod Touch, iPhone, and iPad."
,,,"Through the iTunes Store, users can purchase and download music, music videos, television shows, audiobooks, podcasts, movies, and movie rentals in some countries, and ringtones, available on the iPhone and iPod Touch (fourth generation onward). Application software for the iPhone, iPad and iPod Touch can be downloaded from the App Store."
,,,"The first generation of iPod was released October 23, 2001. The major innovation of the iPod was its small size achieved by using a 1.8"" hard drive compared to the 2.5"" drives common to players at that time. The capacity of the first generation iPod ranged from 5 GB to 10 GB.[263] The iPod sold for US$399 and more than 100,000 iPods were sold before the end of 2001. The introduction of the iPod resulted in Apple becoming a major player in the music industry.[264] Also, the iPod's success prepared the way for the iTunes music store and the iPhone.[265] After the first few generations of iPod, Apple released the touchscreen iPod Touch, the reduced-size iPod Mini and iPod Nano, and the screenless iPod Shuffle in the following years.[264]"
,,,"Apple began work on the first iPhone in 2005 and the first iPhone was released on June 29, 2007. The iPhone created such a sensation that a survey indicated six out of ten Americans were aware of its release. Time declared it ""Invention of the Year"" for 2007 and included it in the All-TIME 100 Gadgets list in 2010, in the category of Communication[266].[267] The completed iPhone had multimedia capabilities and functioned as a quad-band touch screen smartphone.[268] A year later, the iPhone 3G was released in July 2008 with three key features: support for GPS, 3G data and tri-band UMTS/HSDPA. In June 2009, the iPhone 3GS, whose improvements included voice control, a better camera, and a faster processor, was introduced by Phil Schiller.[269] The iPhone 4 was thinner than previous models, had a five megapixel camera capable of recording video in 720p HD, and added a secondary front-facing camera for video calls.[270] A major feature of the iPhone 4S, introduced in October 2011, was Siri, a virtual assistant capable of voice recognition.[267]"
,,,"The iPad is an iOS-based line of tablet computers designed and marketed by Apple. The first iPad was released on April 3, 2010. The user interface is built around the device's multi-touch screen, including a virtual keyboard. The iPad includes built-in Wi-Fi and cellular connectivity on select models. As of April 2015[update], more than 250 million iPads have been sold.[271]"
,,,"In 1989, Jobs first met his future wife, Laurene Powell, when he gave a lecture at the Stanford Graduate School of Business, where she was a student. Soon after the event, he stated that Laurene ""was right there in the front row in the lecture hall, and I couldn't take my eyes off of her ... kept losing my train of thought, and started feeling a little giddy.""[19][page needed] After the lecture, Jobs met up with her in the parking lot and invited her out to dinner. From that point forward, they were together, with a few minor exceptions, for the rest of his life.[19][page needed]"
,,,"Jobs proposed on New Year's Day 1990 with ""a fistful of freshly picked wildflowers"".[19][page needed] They married on March 18, 1991, in a Buddhist ceremony at the Ahwahnee Hotel in Yosemite National Park.[19][page needed] Fifty people, including Jobs's father, Paul, and his sister Mona, attended. The ceremony was conducted by Jobs's guru, Kobun Chino Otogawa. The vegan wedding cake was in the shape of Yosemite's Half Dome, and the wedding ended with a hike (during which Laurene's brothers had a snowball fight). Jobs is reported to have said to Mona: ""You see, Mona [...], Laurene is descended from Joe Namath, and we're descended from John Muir.""[272]"
,,,"Jobs's and Powell's first child, Reed, was born in September 1991.[273] Jobs's father, Paul, died a year and a half later, on March 5, 1993. Jobs's childhood home remains a tourist attraction and is currently owned by his stepmother (Paul's second wife), Marilyn Jobs.[274]"
,,,"Jobs and Powell had two more children, Erin, born in August 1995, and Eve, born in May 1998.[273] The family lived in Palo Alto, California.[275] A journalist who grew up locally remembered him as owning the house with ""the scariest [Halloween] decorations in Palo Alto ... I don't remember seeing him. I was busy being terrified.""[276]"
,,,"Although a billionaire, Jobs made it known that, like Bill Gates, he had stipulated that most of his monetary fortune would not be left to his children.[277][278] These technology leaders also had in common another family-related area: both men limited their children's access, age appropriate, to social media, computer games and the Internet.[279][280]"
,,,"Chrisann Brennan notes that after Jobs was forced out of Apple, ""he apologized many times over for his behavior"" towards her and Lisa. She also states that Jobs ""said that he never took responsibility when he should have, and that he was sorry"".[281] By this time, Jobs had developed a strong relationship with Lisa and when she was nine, Jobs had her name on her birth certificate changed from ""Lisa Brennan"" to ""Lisa Brennan-Jobs"".[13][page needed] In addition, Jobs and Brennan developed a working relationship to co-parent Lisa, a change Brennan credits to the influence of his newly found biological sister, Mona Simpson (who worked to repair the relationship between Lisa and Jobs).[13][page needed] Jobs found Mona after first finding his birth mother, Joanne Schieble Simpson, shortly after he left Apple.[282]"
,,,"Jobs did not contact his birth family during his adoptive mother Clara's lifetime, however. He would later tell his official biographer Walter Isaacson: ""I never wanted [Paul and Clara] to feel like I didn't consider them my parents, because they were totally my parents [...] I loved them so much that I never wanted them to know of my search, and I even had reporters keep it quiet when any of them found out.""[282] However, in 1986, when Jobs was 31, Clara was diagnosed with lung cancer. He began to spend a great deal of time with her and learned more details about her background and his adoption, information that motivated him to find his biological mother. Jobs found on his birth certificate the name of the San Francisco doctor to whom Schieble had turned when she was pregnant. Although the doctor did not help Jobs while he was alive, he left a letter for Jobs to be opened upon his death. As he died soon afterwards, Jobs was given the letter which stated that ""his mother had been an unmarried graduate student from Wisconsin named Joanne Schieble.""[282]"
,,,"Jobs only contacted Schieble after Clara died in early 1986 and after he received permission from his father, Paul. In addition, out of respect for Paul, he asked the media not to report on his search.[283] Jobs stated that he was motivated to find his birth mother out of both curiosity and a need ""to see if she was okay and to thank her, because I'm glad I didn't end up as an abortion. She was twenty-three and she went through a lot to have me.""[284] Schieble was emotional during their first meeting (though she wasn't familiar with the history of Apple or Jobs's role in it) and told him that she had been pressured into signing the adoption papers. She said that she regretted giving him up and repeatedly apologized to him for it. Jobs and Schieble would develop a friendly relationship throughout the rest of his life and would spend Christmas together.[285]"
,,,"During this first visit, Schieble told Jobs that he had a sister, Mona, who was not aware that she had a brother.[284] Schieble then arranged for them to meet in New York where Mona worked. Her first impression of Jobs was that ""he was totally straightforward and lovely, just a normal and sweet guy.""[286] Simpson and Jobs then went for a long walk to get to know each other.[286] Jobs later told his biographer that ""Mona was not completely thrilled at first to have me in her life and have her mother so emotionally affectionate toward me ... As we got to know each other, we became really good friends, and she is my family. I don't know what I'd do without her. I can't imagine a better sister. My adopted sister, Patty, and I were never close.""[286]"
,,,"""I grew up as an only child, with a single mother. Because we were poor and because I knew my father had emigrated from Syria, I imagined he looked like Omar Sharif. I hoped he would be rich and kind and would come into our lives (and our not-yet-furnished apartment) and help us. Later, after I'd met my father, I tried to believe he'd changed his number and left no forwarding address because he was an idealistic revolutionary, plotting a new world for the Arab people. Even as a feminist, my whole life I'd been waiting for a man to love, who could love me. For decades, I'd thought that man would be my father. When I was 25, I met that man, and he was my brother."""
,,,—Mona Simpson[209]
,,,"Jobs then learned his family history. Six months after he was given up for adoption, Schieble's father died, she wed Jandali, and they had a daughter, Mona.[3][287] Jandali states that after finishing his PhD he returned to Syria to work and that it was during this period that Schieble left him[3] (they divorced in 1962).[20] He also states that after the divorce he lost contact with Mona for a period of time: "
,,,"I also bear the responsibility for being away from my daughter when she was four years old, as her mother divorced me when I went to Syria, but we got back in touch after 10 years. We lost touch again when her mother moved and I didn't know where she was, but since 10 years ago we've been in constant contact, and I see her three times a year. I organized a trip for her last year to visit Syria and Lebanon and she went with a relative from Florida.[3]"
,,," A few years later, Schieble married an ice skating teacher, George Simpson.[287] Mona Jandali took her stepfather's last name and thus became Mona Simpson. In 1970, after divorcing her second husband, Schieble took Mona to Los Angeles and raised her on her own.[287]"
,,,"When Simpson found that their father, Abdulfattah Jandali, was living in Sacramento, California, Jobs had no interest in meeting him as he believed Jandali didn't treat his children well.[288] Simpson went to Sacramento alone and met Jandali, who worked in a small restaurant.[289] Jandali and Simpson spoke for several hours, during which time he told her that he had left teaching for the restaurant business.[289] He also said that he and Schieble had given another child away for adoption but that ""we'll never see that baby again. That baby's gone.""[289] At the request of Jobs, Simpson did not tell Jandali that she had met his son.[289] Jandali further told Simpson that he once managed a Mediterranean restaurant near San Jose and that ""all of the successful technology people used to come there. Even Steve Jobs ... oh yeah, he used to come in, and he was a sweet guy and a big tipper.""[289]"
,,,"After hearing about the visit, Jobs recalled that ""it was amazing ... I had been to that restaurant a few times, and I remember meeting the owner. He was Syrian. Balding. We shook hands.""[289] However, Jobs still did not want to meet Jandali because ""I was a wealthy man by then, and I didn't trust him not to try to blackmail me or go to the press about it ... I asked Mona not to tell him about me.""[289] Jandali later discovered his relationship to Jobs through an online blog. He then contacted Simpson and asked ""what is this thing about Steve Jobs?"" Simpson told him that it was true and later commented, ""My father is thoughtful and a beautiful storyteller, but he is very, very passive ... He never contacted Steve.""[285] Because Simpson herself researched her Syrian roots and began to meet members of the family, she assumed that Jobs would eventually want to meet their father, but he never did.[285] Jobs also never showed an interest in his Syrian heritage or the Middle East.[285] Simpson fictionalized the search for their father in her 1992 novel The Lost Father.[285] Malek Jandali is their cousin.[290]"
,,,Jobs kept his philanthropic and charitable efforts private; he donated $50 million to Stanford hospital and also contributed to efforts to cure AIDS.[291] He also formed his own charitable foundation called the Steven P. Jobs foundation in 1985.[292]
,,,Sources:
